{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For most of these problems\n",
    "  - MDP 모델을 모르거나 MDP 모델을 알더라도 sampling 방법 이외에 사용할 것이 없을 때\n",
    "  - 결국 sampling으로 문제를 풀때 Model-free Control을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Free Prediction에서는 estimate하는 것을 배웠었고, 이번 시간에는 Optimise하는 것을 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy / Off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on-policy, 내가 학습하고자 하는 policy와 실제 환경에서 경험을 쌓는 policy가 같다.\n",
    "- off-policy, 다른 agent가 행동한 것을 바탕으로 학습을 하는 것이 off-policy, 즉 학습하는 policy와 경험을 쌓는 policy가 다른 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy Iteration은 Control방법론이다. 최적의 value function을 찾는 방법론이다.\n",
    "- policy evaluation\n",
    "  - policy가 고정되어 있을 때 그 policy를 평가하고(\\\\(v_{\\pi}\\\\)를 측정하고)\n",
    "- policy improvement\n",
    "  - 평가된 v를 바탕으로 greedy policy improvement를 했다.\n",
    "- 그래서 새로운 policy가 나오면 또 평가하고 평가를 바탕으로 새 policy를 만들고, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dynamic programming에서 어떻게 control문제를 푸는지에 대해서 이야기했었음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised Policy Iteration with Monte-Carlo Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy evaluation, Monte-Carlo policy evaluation을 넣고,\n",
    "- Policy improvement, 이 부분은 앞에서 했던 policy iteration 방법에서 착안하면 되지 않는가?\n",
    "  - 안된다. 왜냐하면, Monte-carlo로 V를 학습하고 V를 바탕으로 greedy policy를 한다는 것은 현재 state가 있고 내 다음 state가 뭔지 안다. 알때, 다음 state의 V값 중에 V값이 제일 큰 곳으로 가는 policy를 만드는 것인데, 그렇다면 다음 state를 안다는 것은 MDP를 안다는 것이고 MDP를 모를때는 가보지 않고는 모른다. 그래서 model-free이기 때문에 greedy policy를 만들수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free Policy Iteration Using Action-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결론적으로 위 이야기는 MDP, model을 알아야 greedy improvement를 할 수 있다.\n",
    "- 그렇다면, state-value function(V)이 아니라 action-value function(Q)을 policy evaluation단계에서 평가했다고 하면 Q에 대해서 greedy policy improvement는 가능한가??\n",
    "  - 가능하다. 일단, value function 자체는 state-value function이든, action-value function이든 다 구할 수 있다. 왜냐하면, state s에서 action a를 해보고 나온 return을 평균 취하면 Q가 되기 때문. 그리고 greedy policy improvement 단계에서 V(state-value function)을 사용하면 몇 개의 state가 될지 모르기 때문에 MDP를 알아야하는 문제가 있었는데 여기서 action-value function을 사용하게 되면 하나의 state에서 action개수는 정해져 있기 때문에 가능하다. 따라서 greedy policy improvement에서는 할 수 있는 action중에 Q값이 제일 큰 것을 새로운 policy로 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 여기서 문제가 있는데, greedy하게만 움직인다면 충분히 많은 곳을 가볼수가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e-greedy exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래서 greedy한 것만 찾아가면 탐험을 할 수 없기 때문에 나온 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e-greedy를 policy improvement에 사용가능하다는 것을 증명하는 부분이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복잡한 상황에서 연속적으로 어떤 결정을 내린다 했을 때 현재 상태에서만 고려해서 greedy하게 선택을 한다면 너무 단순한 모델이 되어버린다. 따라서 작은 확률로 greedy하지 않은 action을 취하게 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 action explore 가능\n",
    "- policy가 계속 발전하는 것을 보장가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"401\" alt=\"2018-11-24 8 13 29\" src=\"https://user-images.githubusercontent.com/22078438/48967536-77d98380-f025-11e8-9138-587adbe4b2cc.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시작점이 맨 위에서 시작하지 않는다. 그 이유는?\n",
    "  - Monte carlo의 경우 episode단위로 평가를 진행하는데(하나의 episode가 쌓이면 그 episode로 value-function을 업데이트 하는데), 그 이후에 바로 e-greedy improvement 단계로 진행한다. 그 이유는, 왜 더 평가 할 때 까지 기다리는가? 기다릴 이유가 없다는 것이다. 바로 improvement 하자!\n",
    "  - 위 방식을 적용하므로써 빠르게 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앞선 방식을 잘 수렴하게 하기 위한 2가지 제약(GLIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 state-action pairs가 무한히 반복되어야 한다.\n",
    "  - 우리는 e-greedy를 사용하니까 모든 state-action pairs를 갖는다.\n",
    "- 결국은 greedy policy로 수렴해야한다.\n",
    "  - 만약 e-greedy 값을 0.05를 주게 되면 마지막에 수렴되는 policy를 찾더라도 결국은 5%의 확률로 랜덤 행동을 하게 된다. 따라서 결국 policy는 수렴해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어, 1/k로 한다.\n",
    "- 따라서, GLIE Monte-carlo control을 하면 수렴되는 것이 증명되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC, TD control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 TD를 사용하면 어떻게 되나?\n",
    "  - variance가 작고,\n",
    "  - online 학습이 가능하고\n",
    "  - 끝나지 않는 에피소드에도 적용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Action-Value functions with Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy Control With Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy evaluation, 이 부분은 Sarsa를 사용하고\n",
    "- Policy improvement, e-greedy를 그대로 사용한다.\n",
    "- 한 step가고 update하고, improvement한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sarsa는 다음 조건이 만족될 때 수렴한다.\n",
    "  - GLIE 해야한다. 모든 state action pair를 방문해야하고, 결국은 greedy policy로 수렴해야한다.\n",
    "  - Robbins-Monro sequence of step-sizes \\\\(\\alpha_t\\\\), \\\\(\\alpha\\\\)를 1에서 \\\\(\\infty\\\\)까지 더하면 \\\\(\\infty\\\\)가 되어야한다.\n",
    "    - step_size는 Q를 굉장히 먼 곳으로 데려갈 수 있어야한다.\n",
    "  - \\\\(\\alpha^2\\\\)을 무한히 더하면 무한대보다 작다.\n",
    "    - Q value를 수정하는게 점점 작아진다. 결국은 수렴한다.\n",
    "- 실제 사용할 때는 위 조건을 신경쓰지 않아도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-Step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-Step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"483\" alt=\"2018-11-24 8 58 55\" src=\"https://user-images.githubusercontent.com/22078438/48967984-10730200-f02c-11e8-8c5b-222ae5e51ec1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward-view Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"599\" alt=\"2018-11-24 8 59 10\" src=\"https://user-images.githubusercontent.com/22078438/48967986-1bc62d80-f02c-11e8-9520-25f79a9885fa.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backward-view Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"504\" alt=\"2018-11-24 8 59 19\" src=\"https://user-images.githubusercontent.com/22078438/48967992-2a144980-f02c-11e8-866f-95e65a23fab0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD(0)\n",
    "- TD n-step\n",
    "- TD lambda forward-view, 게임이 끝나야 가능했다.\n",
    "- TD lambda backward-view, eligibility traces 사용\n",
    "  - backward-view에 eligibility trace를 사용하면 forward-view랑 똑같더라!\n",
    "- 모두 사용가능, n-step TD를 써서 n까지는 reward를 보고 거기서 bootstrapping을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-step Sarsa, 바로 전 state만 reward를 받게 되는데,\n",
    "- Sarsa(lambda), eligibility traces때문에 각각의 state의 책임을 통해서 전부다 update 된다. Sarsa lambda는 지나온 경로에 대해서 모두 update해준다. 직전 칸은 많이 update된다.\n",
    "- 아래 그림 참고!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"347\" alt=\"2018-11-24 8 59 28\" src=\"https://user-images.githubusercontent.com/22078438/48967979-fb966e80-f02b-11e8-848a-ae06cf368d80.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행동하는 policy \\\\(\\mu\\\\)가 있고, target policy \\\\(\\pi\\\\)가 있다.\n",
    "- 행동 policy, 실제 action을 sampling하는 policy이다. action을 sampling한다는 것은 굉장히 정확한 표현이다. 확률 분포가 있고 이 policy에 확률에 비례한 어떤 action이 나오는 것이 sampling한다. 이다.\n",
    "- target policy, \\\\(\\pi\\\\)를 따랐을 때의 state value function, action value function을 구하고 싶은데 \\\\(\\pi\\\\)를 따랐을 때가 아니라 \\\\(\\mu\\\\)를 따라서 움직여야하는 상황이다. 이때도 학습 가능\n",
    "  - 다른 agent의 행동을 관찰한 것으로 학습 가능하다. 예를 들어) 사람이 어떤 행동을 했을 때 그 행동을 똑같이 배우는 것이 아니라 그 행동이 좋다면 배우고, 아니면 다른 행동을 해야지를 배우게 된다.\n",
    "  - 한 번 경험을 update하고 나면 on-policy 방식에서는 경험을 버렸다. 왜냐하면, 그 경험으로 부터 policy를 update하고 나면 policy가 조금 바뀌기 때문에 이 policy로 새롭게 action을 해야 의미 있는 경험이 된다. 하지만 off-policy는 옛날에 했던 경험, 사람이 했던 경험을 재사용할 수 있다.\n",
    "  - 탐험적인 행동을 하면서 optimal policy를 배울 수 있다.\n",
    "  - 하나의 policy를 따르면서 여러 policy를 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Samling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 X가 있다.(X는 state)\n",
    "- X가 확률분포 P상에서 sampling되는 것이다. P는 확률분포이다.(P의 정의)\n",
    "- Q는 비뚤어진 주사위라서, 6이 나올 확률이 60%이고, 나머지 1, 2, 3, 4, 5가 나올 확률이 40%인 확률분포를 가진다.(Q의 정의)\n",
    "- f는 주사위 숫자 3을 넣었을 때 함수이다.(함수 f의 정의)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P를 이용해서 주사위를 던졌을 때 f(x)의 기대값을 구하고 싶다.\n",
    "$$ \\mathbb{E}_{X~P}[f(X)] = \\sum P(X)f(X) $$\n",
    "- (문제) **여기서, P를 이용해서 던져 놓고, 다른 주사위 Q를 이용했을 때 기대값을 구하고싶다.**\n",
    "$$ \\mathbb{E}_{X~P}[f(X)] = \\sum Q(X) P(X) / Q(X) f(X) $$\n",
    "$$ \\mathbb{E}_{X~P}[f(X)] = \\mathbb{E}_{X~Q}[P(X)/Q(X)f(X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결론, 내가 다른 분포로 부터 구하고 싶을 때는 첫 번째 분포의 확률이 있고 두 번째 분포의 확률이 있을 때 그 비율을 곱해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling for off-policy Monte-Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\mu\\\\)를 이용해서 게임이 끝날 때까지 환경과 상호작용한 후에 return \\\\(G_t\\\\)를 받는다. 그렇다면, \\\\(G_t\\\\)를 그냥 쓰는 것이 아니고 \\\\(G_t\\\\)를 얻기 위해서 했던 모든 action의 확률 비율이 있다. 그것을 계속 곱해주면 correction이 된다는 것이다. 다른 policy \\\\(\\mu\\\\)를 따랐지만, policy \\\\(\\pi\\\\)를 따랐을 때로 구할 수 있다. 교정해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"504\" alt=\"2018-11-24 9 56 17\" src=\"https://user-images.githubusercontent.com/22078438/48968499-cc83fb00-f033-11e8-8848-07f2280bd521.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만, 이 방법론은 사용할 수 없다. 왜냐하면 variance가 너무 커서 동작하지 않음.\n",
    "- 또한, \\\\(\\mu\\\\)가 0이면 사용 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD는 1-step마다 update하기 때문에 importance sampling ratio가 하나밖에 없다. 그 action하나 한것에 대해서 target policy 확률과 behaviour policy 확률 나눈 것을 곱해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- importance sampling을 안쓰고 하고 싶다.\n",
    "- behavior policy를 이용해서 어떤 action을 하나 뽑고 실제로 action을 하면 \\\\(S_{t+1}\\\\)에 도달하게 되는데 여기서 TD target은 \\\\(S_{t+1}\\\\)을 사용하게 된다.(다음 step에서의 Q가 쓰인다.) 그래서 그자리에 behavior policy 대신에 target policy를 써준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매우 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다시 말하면,\n",
    "- 현재 상태에서 한 step을 갔는데 방금 한칸을 가는 action-value를 학습하고 싶다. 이때 behavior policy는 왼쪽으로 가는 policy이고, target policy는 오른쪽으로 가는 policy라고 한다면 TD target 자리에 target policy를 넣어주면 그 방향으로 학습을 하게 된다.\n",
    "- 결론적으로, 우리는 behavior policy로 target policy를 학습하게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD learning에서는 움직으"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
