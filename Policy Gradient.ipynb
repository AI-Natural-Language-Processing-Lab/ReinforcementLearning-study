{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이전 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value based reinforcement learning을 했고, Q라는 action value function을 이용해서 Q function을 구하고 Q function을 통해서 policy를 찾았다.\n",
    "- DQN도 value based RL이고 DNN으로 Q function을 approximate하고 그것으로 policy를 찾았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy gradient 장/단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 장점\n",
    "  - 수렴이 더 잘된다.\n",
    "  - action이 연속적인 경우에 효과적이다. value based function은 discrete한 output에 대해서 다루었다.\n",
    "  - 확률적인 policy를 배울 수 있다. ex) 가위바위보 게임\n",
    "- 단점\n",
    "  - local minima에 빠지기 쉽다.\n",
    "  - policy의 evaluation이 비효율적이다.\n",
    "  - variance가 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value based RL의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function을 바탕으로 policy를 계산하기 때문에 value function이 조금만 달라져도 policy에 크게 영향을 받는다. 이 현상은 알고리즘ㅈ 수렴에 불안정을 더한다.\n",
    "- stochastic policy\n",
    "  - 때로는 stochastic policy가 최적의 policy가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q function을 사용하는 action value based 방법에서는 DNN을 update 하기위해서 target과 현재 Q function값의 MSE를 구했다.\n",
    "  - Q function -> Q function이 나오는 DNN\n",
    "  - policy -> policy가 나오는 DNN\n",
    "<br>\n",
    "- Objective function\n",
    "  1. state value, 게임에서 보통 똑같은 state에서 시작하기 때문에 처음 시작 state의 value function이 최대로 되게 학습한다.\n",
    "  2. average value\n",
    "  3. average reward per time-step, 각 time-step 마다 받는 reward를 각 state에서 머무르는 비율(stationary distribution)을 곱한 expectation 사용\n",
    "<br>\n",
    "- 따라서, policy gradient의 목표는 이 objective function을 최대화 시키는 theta라는 policy의 parameter vector를 찾는것이다.\n",
    "  - how? gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function 구하는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finite difference policy gradient\n",
    "  - parameter vector가 5개의 차원으로 이루어져 있으면 하나씩 흔들어 보면서 그 gradient를 구한다. policy가 미분가능하지 않아도 사용할 수 있는 방법\n",
    "- Monte carlo policy gradient\n",
    "  - objective function에 직접 gradient를 취해준다. policy는 미분 가능하다고 본다.\n",
    "- actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a) &= \\pi_{\\theta}(s,a) \\frac{\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a)}{\\pi_{\\theta}(s,a)} \\\\\n",
    "&= \\pi_{\\theta}(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a) \\end{split} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\(\\begin{split} J(\\theta)\n",
    "&= \\mathbb{E}_{\\pi_{\\theta}}[r] \\\\\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a)R_{s,a}\n",
    "\\end{split}\\\\)\n",
    "<br><br>\n",
    "\\\\(\\begin{split} \\bigtriangledown_{\\theta} J(\\theta)\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)R_{s,a} \\\\\n",
    "&= \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)r]\n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 마지막 식에서 정의된 \\\\(r\\\\)은 immediate reward이다.\n",
    "- \\\\(\\bigtriangledown_{\\theta} J(\\theta)\\\\)를 이용해서 policy DNN update한다.\n",
    "- 그러나 강화학습은 stochastic policy gradient로써 policy 자체가 stochastic하다.\n",
    "  - policy 2가지 표현, softmax policy, gaussian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte carlo policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- episode가 끝나면 해당 policy에 대한 Q function을 구할 수 있다. 하지만 episode가 너무 길면?\n",
    "- 높은 variance를 가진다.\n",
    "  - 위 문제를 해결하기 위해서 Neural network를 2개 만들어서 Q function도 approximate해서 gradient를 구하자 -> actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor, policy approximate하고,\n",
    "- critic, Q function approximate한다. -> 현재 policy를 평가한다.\n",
    "  - 따라서 w와 \\\\(\\theta\\\\) 두개를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy evaluation\n",
    "  - Monte carlo policy evaluation\n",
    "  - TD learning\n",
    "  - TD(\\\\(\\lambda \\\\))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor critic에 baseline을 추가하여 variance 문제를 해결한다.\n",
    "- 예를 들어) action 1를 했을 때 reward를 100만을 받고, action 2를 했을 때 reward 99만을 받는다면 결국 수렴하게 되겠지만 variance가 너무 크다 따라서 action value function에서 state value function을 빼주어 advantage를 만들어 variance문제를 해결한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
