{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이전 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value based reinforcement learning을 했고, Q라는 action value function을 이용해서 Q function을 구하고 Q function을 통해서 policy를 찾았다.\n",
    "- DQN도 value based RL이고 DNN으로 Q function을 approximate하고 그것으로 policy를 찾았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state value function, \\\\(V^\\pi(s)\\\\)\n",
    "- action value function, \\\\(Q^\\pi(s,a)\\\\)\n",
    "- 또한, 앞서서 Monte-carlo나 TD로 문제푸는 것을 배웠다. 그때는 policy에 대한 언급은 없었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- table lookup 방법은 더이상 불가능하다. 따라서 value function approximation 방법이 제안되었고, 함수로 value function을 학습하고 policy를 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Policy를 parameter로 표현할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model free 방법론 이고,\n",
    "- value based method는 value function에 기반한 방법론으로 value function을 함수로 표현했고 함수의 parameter를 update해서 그 함수가 정확한 value를 return 하도록 학습했다. 그때 policy는 implicit policy(value function을 이용해서 policy를 만듦)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy based, value function을 학습하지 않고, 바로 policy를 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지금 부터는 policy를 output하는 network를 만들 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value based\n",
    "  - Learn value function\n",
    "  - implicit policy(e-greedy)\n",
    "- policy based\n",
    "  - no value function\n",
    "  - learn policy\n",
    "- actor-critic\n",
    "  - learn value function\n",
    "  - learn policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy gradient 장/단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 장점\n",
    "  - 수렴이 더 잘된다.\n",
    "  - action이 연속적인 경우에 효과적이다. value based function은 discrete한 output에 대해서 다루었다. action의 차원이 많기 때문에 앞서 value function 방식으로 모든 action을 넣어보고 결정할 수 없다.\n",
    "  - 확률적인 policy를 배울 수 있다. ex) 가위바위보 게임\n",
    "- 단점\n",
    "  - local minima에 빠지기 쉽다.\n",
    "  - policy의 evaluation이 비효율적이다.\n",
    "  - variance가 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를들어)\n",
    "![default](https://user-images.githubusercontent.com/22078438/49021272-97171300-f1d5-11e8-8c9a-d7d553747b1f.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 예제에서 검은칸은 feature가 불안정하여 하나의 방향으로만 action이 정해지면 절대 보상에 도달할 수 없다.\n",
    "- 그 이유는, 검은색에서 왼쪽으로만 가는 action이 반복된다면 첫번째 검은칸에 있을 때는 절대 보상으로 갈 수 없는 경우가 발생한다.\n",
    "- 따라서 value based 방법을 사용하면 문제해결을 할 수 없다. 하지만 stochastic policy를 사용하면 (왼쪽 0.5, 오른쪽 0.5)의 확률로 보상에 도달할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서, Dynamic programming에서 markov property가 성립할 때(fully observable할 때) 최적의 optimal descrete policy가 존재 하지만 partially observable하면 성립하지 않는다.(그리고 위와 같은 경우는 다른 state임에도 불구하고 검은칸이 두개가 있어서 구분도 안됨.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameter \\\\(\\theta\\\\)에 대해서 \\\\(\\pi\\\\)는 action a를 할 확률을 뱉는다.\n",
    "- 어떤 policy가 더 좋은 \\\\(\\pi\\\\)인가?\n",
    "  - 그래서 objective function이 나왔다. 우리가 maximize하고자 하는 목적함수는 무엇인가?를 정의해야한다.\n",
    "  - 쉽게 말해서 이 policy를 따랐을 때 총 reward를 더 많이 받으면 좋은 policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value based RL의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function을 바탕으로 policy를 계산하기 때문에 value function이 조금만 달라져도 policy에 크게 영향을 받는다. 이 현상은 알고리즘ㅈ 수렴에 불안정을 더한다.\n",
    "- stochastic policy\n",
    "  - 때로는 stochastic policy가 최적의 policy가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떤 \\\\(\\pi\\\\)가 더 좋은 \\\\(\\pi\\\\)인지 정의 되어야한다. \\\\(\\pi\\\\)가 정의되어야 그 목적을 향해서 update가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q function을 사용하는 action value based 방법에서는 DNN을 update 하기위해서 target과 현재 Q function값의 MSE를 구했다.\n",
    "  - Q function -> Q function이 나오는 DNN\n",
    "  - policy -> policy가 나오는 DNN\n",
    "<br>\n",
    "- Objective function\n",
    "  1. state value, 게임에서 보통 똑같은 state에서 시작하기 때문에 처음 시작 state의 value function이 최대로 되게 학습한다.\n",
    "   - episodic한 환경, 처음 시작 state가 항상 정해져 있고 policy \\\\(\\pi\\\\)로 게임을 끝까지 했을 때 value를 최대화 하는 문제이다.\n",
    "  2. average value\n",
    "   - continuous 환경, Markov chain에서 policy \\\\(\\pi\\\\)를 따라 움직이다 보면 각 state마다 머무르는 확률을 구할 수 있다.(각 state에 있을 확률 * 그 state에서 value의 모든 총합)\n",
    "  3. average reward per time-step, 각 time-step 마다 받는 reward를 각 state에서 머무르는 비율(stationary distribution)을 곱한 expectation 사용\n",
    "   - 한 step 보고 그 state에서 한 step 얻은 기대값\n",
    "<br>\n",
    "- 따라서, policy gradient의 목표는 이 objective function을 최대화 시키는 theta라는 policy의 parameter vector를 찾는것이다.\n",
    "  - how? gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- episodic 환경\n",
    "  - 처음 시작 state가 항상 정해져있음.\n",
    "  - 이때 policy \\\\(\\pi\\\\)로 게임을 끝까지 했을 때 얼마의 reward를 받을지 (\\\\(\\pi\\\\)를 따랐을 때 value의 기대값)\n",
    "- continuous 환경\n",
    "  - \\\\(d^{\\pi_{\\theta}(s)}\\\\)가 나오는데, stationary distribution(\\\\(d^{\\pi_{\\theta}(s)}\\\\)), markov chain에서 policy \\\\(\\pi\\\\)를 따라서 계속 움직인다 보면,\n",
    "  - 각 state 별로 머무를 확률을 구할 수 있고\n",
    "  - 따라서, 각 state에 있을 확률 (\\\\(d^{\\pi_{\\theta}(s)}\\\\)) * 그 state에서 value의 총합이 된다.\n",
    "- average reward per time step(1-step)\n",
    "  - \\\\( J_avR(\\theta) = \\sum_s d^{\\pi_{\\theta}(s)} \\sum_a \\pi_\\theta(s,a) \\mathcal{R}^a_s\\\\)\n",
    "  - \\\\(\\pi_\\theta(s,a)\\\\), action들의 확률, 그 state에서 1-step 얻을 기대값\n",
    "  - \\\\(\\mathcal{R}^a_s\\\\), 각 action을 했을 때 reward값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 우리는 objective function \\\\(J(\\theta)\\\\)를 정의했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimization, 어떤 임의의 함수가 있을 때 주어진 정의역안에서 걔를 최대화 하는 input x를 찾는 문제\n",
    "- \\\\(J(\\theta)\\\\)를 maximize하고 input \\\\(\\theta\\\\)를 찾는 것이 목적이다.\n",
    "  - \\\\(\\theta\\\\)를 바꿔주면 policy가 바뀌고, policy가 바뀌면 게임을 하는 동안 reward가 바뀐다.<br>\n",
    "**우리는 \\\\(\\theta\\\\)를 조정해서 J를 Maximize하고 싶다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞선 value function approximation에서는 최소화 하는 방향으로 학습을 했지만,\n",
    "- 이번 문제는 \\\\(J(\\theta)\\\\)가 reward이므로 최대화 하는 방향으로 학습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function 구하는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finite difference policy gradient\n",
    "  - parameter vector가 5개의 차원으로 이루어져 있으면 하나씩 흔들어 보면서 그 gradient를 구한다. policy가 미분가능하지 않아도 사용할 수 있는 방법\n",
    "- Monte carlo policy gradient\n",
    "  - objective function에 직접 gradient를 취해준다. policy는 미분 가능하다고 본다.\n",
    "- actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\pi\\\\)는 항상 미분 가능하다고 가정한다.\n",
    "- policy가 항상 미분가능하다.\n",
    "- Likelihood ratios를 사용해서 계산을 편하게 한다.\n",
    "- Score function\n",
    "  - \\\\(\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a) &= \\pi_{\\theta}(s,a) \\frac{\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a)}{\\pi_{\\theta}(s,a)} \\\\\n",
    "&= \\pi_{\\theta}(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a) \\end{split} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 분포를 따르는 initial state(초기 state가 분포가 있음) S에서 시작해서 한 step가고 reward가 끝난다.\n",
    "- ex) 벤딩머신, 한번 당기면 reward를 받고 끝난다. 1-step MDP의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\(\\begin{split} J(\\theta)\n",
    "&= \\mathbb{E}_{\\pi_{\\theta}}[r] \\\\\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a)R_{s,a}\n",
    "\\end{split}\\\\)\n",
    "<br><br>\n",
    "\\\\(\\begin{split} \\bigtriangledown_{\\theta} J(\\theta)\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)R_{s,a} \\\\\n",
    "&= \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)r]\n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식에서 \\\\(\\bigtriangledown_{\\theta} J(\\theta)\\\\)(J에 대한 gradient)가 \\\\(\\sum\\\\)이 빠지면서 \\\\(\\mathbb{E}\\\\)로 표현된다.\n",
    "- \\\\(\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)\\\\)\n",
    "  - 엄밀히 말하자면 J gradient에 대한 sample을 얻는 것이다. 이 sample은 unbiased sample이라서 이것을 여러번 update하다 보면 대수의 법칙에 의해 gradient J에 수렴한다.\n",
    "  - 각각의 수행은 다를 수 있지만 기대값이기 때문에 결국은 위 식과 같아진다 그래서 이 식으로 update하면 된다.\n",
    "- r\n",
    "  - 실제 받은 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 마지막 식에서 정의된 \\\\(r\\\\)은 immediate reward이다.\n",
    "- \\\\(\\bigtriangledown_{\\theta} J(\\theta)\\\\)를 이용해서 policy DNN update한다.\n",
    "- 그러나 강화학습은 stochastic policy gradient로써 policy 자체가 stochastic하다.\n",
    "  - policy 2가지 표현, softmax policy, gaussian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax policy\n",
    "  - \\\\(\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)\\\\), 이거는 쉽게 구할 수 있다. 왜냐하면 \\\\(\\pi_\\theta(s,a)\\\\)는 우리가 정할 수 있기 때문이다. 하지만 앞서서 나온 \\\\(\\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)R_{s,a}\\\\) 이 식을 모두 구하기에는 어려움이 있다.\n",
    "- Gaussian policy\n",
    "  - 어떤 평균이 있어서 어떤 state에서 대체로 이 action을 할것인데 약간의 variance를 줘서 다른 action도 할 수 있는 policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 one-step MDP에 대해서 이야기를 했었는데 그렇다면 multi step MDP는?\n",
    "- Multi step MDP도 likelihood ratio를 일반화 가능하게 하는 것이 policy gradient theorem이다.\n",
    "- 위의 식 \\\\(\\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)r]\\\\)에서 r -> Q로 대체<br><br>\n",
    "  \\\\(\\bigtriangledown_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)Q^{\\pi_\\theta}(s,a)]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(Q^{\\pi_\\theta}(s,a)\\\\)\n",
    "  - Multi step은 그 step에서 a를 했을 때 얼마를 받을지 다 더해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\theta)\\\\)는 위에서 정의한 3개 다 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte carlo policy gradient(REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- episode가 끝나면 해당 policy에 대한 Q function을 구할 수 있다. 하지만 episode가 너무 길면?\n",
    "- 높은 variance를 가진다.\n",
    "  - 위 문제를 해결하기 위해서 Neural network를 2개 만들어서 Q function도 approximate해서 gradient를 구하자 -> actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return \\\\(v_t\\\\), 축적된 discounted reward, \\\\( Q^{\\pi_\\theta}(s_t,a_t)\\\\)\n",
    "- \\\\(\\bigtriangleup \\theta_t = \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)v_t\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드\n",
    "  - \\\\(\\theta\\\\)를 초기화 하고,\n",
    "  - \\\\(\\pi\\\\)에 따라 state, action, reward를 반복하고,\n",
    "  - \\\\(\\theta \\leftarrow \\theta + \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)v_t\\\\)\n",
    "- 결과적으로, 점점 좋은 policy \\\\(\\pi\\\\)가 만들어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte carlo 방법은 variance가 너무 크기 때문에 critic을 넣는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split} \\bigtriangledown_{\\theta} J(\\theta) &\\approx \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)Q_{w}(s,a)] \\\\\n",
    "\\bigtriangleup \\theta &= \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)Q_w(s,a) \\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 대상이 \\\\(\\theta, w \\\\)로 두개이다.\n",
    "- \\\\(\\pi\\\\)로 \\\\(\\theta\\\\)를 학습하고, \\\\(Q\\\\)로 \\\\(w\\\\)를 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor, policy approximate하고,\n",
    "- critic, Q function approximate한다. -> 현재 policy를 평가한다.\n",
    "  - 따라서 w와 \\\\(\\theta\\\\) 두개를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q를 학습하는 것은 policy evaluation 문제\n",
    "- policy evaluation\n",
    "  - Monte carlo policy evaluation\n",
    "  - TD learning\n",
    "  - TD(\\\\(\\lambda \\\\))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49024168-3b9c5380-f1dc-11e8-9693-4b5b3daa79dd.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state s와 \\\\(\\theta\\\\)를 초기화한다.\n",
    "- \\\\(\\theta\\\\)에서 action을 하나 찾으면 policy \\\\(\\pi\\\\)를 이용해서 sample \\\\(a ~ \\pi_\\theta \\\\)\n",
    "- 매 step 마다 reward알고, 다음 state가 뭔지 안다. 그리고 그 state에서 무슨 action할지 a'를 찾고,<br>\n",
    "  \\\\(\\delta = r + \\gamma Q_w(s', a') - Q_w(s, a)\\\\), \\\\(\\delta\\\\)(TD error, w를 update한다), Q(s', a')는 한 step 더 가서 Q, Q(s, a)는 현재 추측<br><br>\n",
    "- \\\\(w \\leftarrow w + \\beta \\delta \\varnothing(s,a)\\\\), \\\\(\\beta\\\\)(learning_rate)<br><br>\n",
    "- \\\\(\\theta \\leftarrow \\theta + \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)Q_w(s,a)\\\\)\n",
    "  - \\\\(\\log \\pi_{\\theta}(s_t,a_t)\\\\), 학습 방향을 조정하고,\n",
    "  - \\\\(Q_w(s,a)\\\\), 학습의 조정 크기를 결정한다.\n",
    "- \\\\(a \\leftarrow a', s \\leftarrow s'\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing variance using Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor critic에 baseline을 추가하여 variance 문제를 해결한다.\n",
    "- 예를 들어) action 1를 했을 때 reward를 100만을 받고, action 2를 했을 때 reward 99만을 받는다면 결국 수렴하게 되겠지만 variance가 너무 크다 따라서 action value function에서 state value function을 빼주어 advantage를 만들어 variance문제를 해결한다.\n",
    "- baseline을 빼준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split} \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)B(s)] &= \\sum_{s \\in \\mathcal{S}} d^{\\pi_\\theta}(s) \\sum_a \\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)B(s) \\;\\;\\text{- B는 action과 연관이 없기 때문에} \\sum \\text{ 밖으로 나올 수 있다.} \\\\\n",
    "&= \\sum_{s \\in \\mathcal{S}} d^{\\pi_\\theta}B(s)\\bigtriangledown_{\\theta} \\sum_a \\log \\pi_{\\theta} (s,a) \\;\\;\\text{-}\\sum\\text{을 다 더하면 1이되고, 미분하면 0이된다.} \\\\\n",
    "&= 0\n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 첫 번째 식에서 두 번째 식으로 넘어가면서 Expectation이 사라지고 \\\\(\\pi\\\\)가 붙는데 \\\\(\\pi log\\pi\\\\)가 gradient \\\\(\\pi\\\\)이다.\n",
    " - B(s)\n",
    "   - B(s)는 Q - B를 해도 된다. 왜냐하면 앞선 식에서 B(s)를 식에 더하면 0이 되기 때문에 기대값이 바뀌지 않는다. 따라서 사용가능하고 B(s)는 s에 대한 일반적인 함수이기 때문에 s로 표현되는 모든 식이 가능하다 그래서 V(s)(state value function)로 대체가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(A^{\\pi_\\theta}_\\theta(s,a) = Q^{\\pi_\\theta}_w(s,a) - V^{\\pi_\\theta}_v(s)\\\\)\n",
    "- \\\\(\\theta, w, v\\\\)총 3개의 parameter가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 TD error가 advantage function의 smaple이기 때문에<br>\n",
    "  \\\\(\\delta^{\\pi_\\theta} = r + \\gamma V^{\\pi_\\theta}(s') - V^{\\pi_\\theta}(s)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\mathbb{E}_{\\pi_\\theta}[\\delta^{\\pi_\\theta}|s, a] &= \\mathbb{E}_{\\pi_\\theta}[r + \\gamma V^{\\pi_\\theta}(s')|s,a] - V^{\\pi_\\theta}(s) \\\\\n",
    "&= Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s)\\\\\n",
    "&= A^{\\pi_\\theta}(s,a) \n",
    "\\end{split}\\\\)\n",
    "- \\\\(\\delta\\\\) 1개는 A와 같지 않지만 그들의 평균은 A와 같다.\n",
    "- 결과적으로,<br>\n",
    "\\\\(\\bigtriangledown_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)\\delta^{\\pi_\\theta}] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(Q^{\\pi_{\\theta}}(s,a) - V^{\\pi_{\\theta}}(s)\\\\)\n",
    "  - state s에 있는 것 보다 a를 하면 얼마나 더 좋은지를 학습한다.\n",
    "- 하지만, V을 학습할려고 paramter하나가 더 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD error(advantage function의 sample)는 advantage에 unbiased estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\mathbb{E}_{\\pi_\\theta}[\\delta^{\\pi_\\theta}|s, a] &= \\mathbb{E}_{\\pi_\\theta}[r + \\gamma V^{\\pi_\\theta}(s')|s,a] - V^{\\pi_\\theta}(s) \\\\\n",
    "&= Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s)\\\\\n",
    "&= A^{\\pi_\\theta}(s,a) \n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\mathbb{E}_{\\pi_\\theta}[r + \\gamma V^{\\pi_\\theta}(s')|s,a]\\\\),\n",
    "  - state s에서 action a를 했을 때 그때 받은 reward와 한 step 더 갔을 때 value의 기대값 -> \\\\(Q^{\\pi_\\theta}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로,\n",
    "  - \\\\(\\delta\\\\)의 기대값이 A이기 때문에 \\\\(\\delta\\\\)라는 sample들은 A의 unbiased sample이라는 뜻.\n",
    "  - \\\\(\\delta\\\\) 1개는 A와 같지 않지만 계속 \\\\(\\delta\\\\)를 취하다 보면 그것의 평균은 A와 같을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49025168-65ef1080-f1de-11e8-82cc-a99e68baa57a.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
