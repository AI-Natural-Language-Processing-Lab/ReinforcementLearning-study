{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이전 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value based reinforcement learning을 했고, Q라는 action value function을 이용해서 Q function을 구하고 Q function을 통해서 policy를 찾았다.\n",
    "- DQN도 value based RL이고 DNN으로 Q function을 approximate하고 그것으로 policy를 찾았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state value function, \\\\(V^\\pi(s)\\\\)\n",
    "- action value function, \\\\(Q^\\pi(s,a)\\\\)\n",
    "- 또한, 앞서서 Monte-carlo나 TD로 문제푸는 것을 배웠다. 그때는 policy에 대한 언급은 없었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- table lookup 방법은 더이상 불가능하다. 따라서 value function approximation 방법이 제안되었고, 함수로 value function을 학습하고 policy를 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Policy를 parameter로 표현할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model free 방법론 이고,\n",
    "- value based method는 value function에 기반한 방법론으로 value function을 함수로 표현했고 함수의 parameter를 update해서 그 함수가 정확한 value를 return 하도록 학습했다. 그때 policy는 implicit policy(value function을 이용해서 policy를 만듦)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy based, value function을 학습하지 않고, 바로 policy를 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지금 부터는 policy를 output하는 network를 만들 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value based\n",
    "  - Learn value function\n",
    "  - implicit policy(e-greedy)\n",
    "- policy based\n",
    "  - no value function\n",
    "  - learn policy\n",
    "- actor-critic\n",
    "  - learn value function\n",
    "  - learn policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy gradient 장/단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 장점\n",
    "  - 수렴이 더 잘된다.\n",
    "  - action이 연속적인 경우에 효과적이다. value based function은 discrete한 output에 대해서 다루었다. action의 차원이 많기 때문에 앞서 value function 방식으로 모든 action을 넣어보고 결정할 수 없다.\n",
    "  - 확률적인 policy를 배울 수 있다. ex) 가위바위보 게임\n",
    "- 단점\n",
    "  - local minima에 빠지기 쉽다.\n",
    "  - policy의 evaluation이 비효율적이다.\n",
    "  - variance가 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value based RL의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function을 바탕으로 policy를 계산하기 때문에 value function이 조금만 달라져도 policy에 크게 영향을 받는다. 이 현상은 알고리즘ㅈ 수렴에 불안정을 더한다.\n",
    "- stochastic policy\n",
    "  - 때로는 stochastic policy가 최적의 policy가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떤 \\\\(\\pi\\\\)가 더 좋은 \\\\(\\pi\\\\)인지 정의 되어야한다. \\\\(\\pi\\\\)가 정의되어야 그 목적을 향해서 update가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q function을 사용하는 action value based 방법에서는 DNN을 update 하기위해서 target과 현재 Q function값의 MSE를 구했다.\n",
    "  - Q function -> Q function이 나오는 DNN\n",
    "  - policy -> policy가 나오는 DNN\n",
    "<br>\n",
    "- Objective function\n",
    "  1. state value, 게임에서 보통 똑같은 state에서 시작하기 때문에 처음 시작 state의 value function이 최대로 되게 학습한다.\n",
    "   - episodic한 환경, 처음 시작 state가 항상 정해져 있고 policy \\\\(\\pi\\\\)로 게임을 끝까지 했을 때 value를 최대화 하는 문제이다.\n",
    "  2. average value\n",
    "   - continuous 환경, Markov chain에서 policy \\\\(\\pi\\\\)를 따라 움직이다 보면 각 state마다 머무르는 확률을 구할 수 있다.(각 state에 있을 확률 * 그 state에서 value의 모든 총합)\n",
    "  3. average reward per time-step, 각 time-step 마다 받는 reward를 각 state에서 머무르는 비율(stationary distribution)을 곱한 expectation 사용\n",
    "   - 한 step 보고 그 state에서 한 step 얻은 기대값\n",
    "<br>\n",
    "- 따라서, policy gradient의 목표는 이 objective function을 최대화 시키는 theta라는 policy의 parameter vector를 찾는것이다.\n",
    "  - how? gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 우리는 objective function \\\\(J(\\theta)\\\\)를 정의했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function 구하는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finite difference policy gradient\n",
    "  - parameter vector가 5개의 차원으로 이루어져 있으면 하나씩 흔들어 보면서 그 gradient를 구한다. policy가 미분가능하지 않아도 사용할 수 있는 방법\n",
    "- Monte carlo policy gradient\n",
    "  - objective function에 직접 gradient를 취해준다. policy는 미분 가능하다고 본다.\n",
    "- actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\pi\\\\)는 항상 미분 가능하다고 가정한다.\n",
    "- Likelihood ratios를 사용해서 계산을 편하게 한다.\n",
    "- Score function\n",
    "  - \\\\(\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a) &= \\pi_{\\theta}(s,a) \\frac{\\bigtriangledown_{\\theta} \\pi_{\\theta}(s,a)}{\\pi_{\\theta}(s,a)} \\\\\n",
    "&= \\pi_{\\theta}(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a) \\end{split} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 분포를 따르는 initial state S에서 시작해서 한 step가고 reward가 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\(\\begin{split} J(\\theta)\n",
    "&= \\mathbb{E}_{\\pi_{\\theta}}[r] \\\\\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a)R_{s,a}\n",
    "\\end{split}\\\\)\n",
    "<br><br>\n",
    "\\\\(\\begin{split} \\bigtriangledown_{\\theta} J(\\theta)\n",
    "&= \\sum_{s \\in \\mathcal{S}}d(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s,a) \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)R_{s,a} \\\\\n",
    "&= \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)r]\n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 마지막 식에서 정의된 \\\\(r\\\\)은 immediate reward이다.\n",
    "- \\\\(\\bigtriangledown_{\\theta} J(\\theta)\\\\)를 이용해서 policy DNN update한다.\n",
    "- 그러나 강화학습은 stochastic policy gradient로써 policy 자체가 stochastic하다.\n",
    "  - policy 2가지 표현, softmax policy, gaussian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 one-step MDP에 대해서 이야기를 했었는데 그렇다면 multi step MDP는?\n",
    "- Multi step MDP도 likelihood ratio를 일반화 가능하게 하는 것이 policy gradient theorem이다.\n",
    "- 위의 식 \\\\(\\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)r]\\\\)에서 r -> Q로 대체<br><br>\n",
    "  \\\\(\\bigtriangledown_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s,a)Q^{\\pi_\\theta}(s,a)]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\theta)\\\\)는 위에서 정의한 3개 다 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte carlo policy gradient(REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- episode가 끝나면 해당 policy에 대한 Q function을 구할 수 있다. 하지만 episode가 너무 길면?\n",
    "- 높은 variance를 가진다.\n",
    "  - 위 문제를 해결하기 위해서 Neural network를 2개 만들어서 Q function도 approximate해서 gradient를 구하자 -> actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return \\\\(v_t\\\\), 축적된 discounted reward, \\\\( Q^{\\pi_\\theta}(s_t,a_t)\\\\)\n",
    "- \\\\(\\bigtriangleup \\theta_t = \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)v_t\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드\n",
    "  - \\\\(\\theta\\\\)를 초기화 하고,\n",
    "  - \\\\(\\pi\\\\)에 따라 state, action, reward를 반복하고,\n",
    "  - \\\\(\\theta \\leftarrow \\theta + \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)v_t\\\\)\n",
    "- 결과적으로, 점점 좋은 policy \\\\(\\pi\\\\)가 만들어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor critic policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte carlo 방법은 variance가 너무 크기 때문에 critic을 넣는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split} \\bigtriangledown_{\\theta} J(\\theta) &\\approx \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)Q_{w}(s,a)] \\\\\n",
    "\\bigtriangleup \\theta &= \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)Q_w(s,a) \\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 대상이 \\\\(\\theta, w \\\\)로 두개이다.\n",
    "- \\\\(\\pi\\\\)로 \\\\(\\theta\\\\)를 학습하고, \\\\(Q\\\\)로 \\\\(w\\\\)를 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor, policy approximate하고,\n",
    "- critic, Q function approximate한다. -> 현재 policy를 평가한다.\n",
    "  - 따라서 w와 \\\\(\\theta\\\\) 두개를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q를 학습하는 것은 policy evaluation 문제\n",
    "- policy evaluation\n",
    "  - Monte carlo policy evaluation\n",
    "  - TD learning\n",
    "  - TD(\\\\(\\lambda \\\\))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state s와 \\\\(\\theta\\\\)를 초기화한다.\n",
    "- \\\\(\\theta\\\\)에서 action을 하나 찾으면 policy \\\\(\\pi\\\\)를 이용해서 sample \\\\(a ~ \\pi_\\theta \\\\)\n",
    "- 매 step 마다 reward알고, 다음 state가 뭔지 안다. 그리고 그 state에서 무슨 action할지 a'를 찾고,<br>\n",
    "  \\\\(\\delta = r + \\gamma Q_w(s', a') - Q_w(s, a)\\\\), \\\\(\\delta\\\\)(TD error, w를 update한다), Q(s', a')는 한 step 더 가서 Q, Q(s, a)는 현재 추측<br><br>\n",
    "- \\\\(w \\leftarrow w + \\beta \\delta \\varnothing(s,a)\\\\), \\\\(\\beta\\\\)(learning_rate)<br><br>\n",
    "- \\\\(\\theta \\leftarrow \\theta + \\alpha \\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)Q_w(s,a)\\\\)\n",
    "  - \\\\(\\log \\pi_{\\theta}(s_t,a_t)\\\\), 학습 방향을 조정하고,\n",
    "  - \\\\(Q_w(s,a)\\\\), 학습의 조정 크기를 결정한다.\n",
    "- \\\\(a \\leftarrow a', s \\leftarrow s'\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing variance using Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor critic에 baseline을 추가하여 variance 문제를 해결한다.\n",
    "- 예를 들어) action 1를 했을 때 reward를 100만을 받고, action 2를 했을 때 reward 99만을 받는다면 결국 수렴하게 되겠지만 variance가 너무 크다 따라서 action value function에서 state value function을 빼주어 advantage를 만들어 variance문제를 해결한다.\n",
    "- baseline을 빼준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split} \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)B(s)] &= \\sum_{s \\in \\mathcal{S}} d^{\\pi_\\theta}(s) \\sum_a \\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)B(s) \\;\\;\\text{- B는 action과 연관이 없기 때문에} \\sum \\text{ 밖으로 나올 수 있다.} \\\\\n",
    "&= \\sum_{s \\in \\mathcal{S}} d^{\\pi_\\theta}B(s)\\bigtriangledown_{\\theta} \\sum_a \\log \\pi_{\\theta} (s,a) \\;\\;\\text{-}\\sum\\text{을 다 더하면 1이되고, 미분하면 0이된다.} \\\\\n",
    "&= 0\n",
    "\\end{split}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(A^{\\pi_\\theta}_\\theta(s,a) = Q^{\\pi_\\theta}_w(s,a) - V^{\\pi_\\theta}_v(s)\\\\)\n",
    "- \\\\(\\theta, w, v\\\\)총 3개의 parameter가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 TD error가 advantage function의 smaple이기 때문에<br>\n",
    "  \\\\(\\delta^{\\pi_\\theta} = r + \\gamma V^{\\pi_\\theta}(s') - V^{\\pi_\\theta}(s)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{split}\n",
    "\\mathbb{E}_{\\pi_\\theta}[\\delta^{\\pi_\\theta}|s, a] &= \\mathbb{E}_{\\pi_\\theta}[r + \\gamma V^{\\pi_\\theta}(s')|s,a] - V^{\\pi_\\theta}(s) \\\\\n",
    "&= Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s)\\\\\n",
    "&= A^{\\pi_\\theta}(s,a) \n",
    "\\end{split}\\\\)\n",
    "- \\\\(\\delta\\\\) 1개는 A와 같지 않지만 그들의 평균은 A와 같다.\n",
    "- 결과적으로,<br>\n",
    "\\\\(\\bigtriangledown_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\bigtriangledown_{\\theta} \\log \\pi_{\\theta} (s,a)\\delta^{\\pi_\\theta}] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
