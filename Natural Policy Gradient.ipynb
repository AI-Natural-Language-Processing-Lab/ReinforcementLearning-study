{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 policy gradient의 문제는 steepest descent direction이 아닐수 있다.\n",
    "  - steepest descent direction, 가장 가파른 방향을 따라 optimization해야하는데 그렇지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 따라서, natural gradient method를 policy gradient에 적용해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떻게? -> Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manifold, 간단히 말해서 점들을 아우르는 subspace이다.\n",
    "- Riemann manifold, manifold가 각지지 않고, 미분 가능하게 곡률을 가진 면이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network의 parameter가 직선으로 뻗어있는 유클리디안 공간이 아니다. 좀 더 일반적으로 구의 표면과 같이 휘어진 공간, 즉 Riemann 공간으로 표현 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떤 확률 분포가 있다고 했을 때,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 공간안에 있는 확률 분포가 있고, 그 확률 분포 안에 한 점은 다른 확률분포에서의 한점이 된다.\n",
    "- 따라서, 유클리디안 공간의 확률분포에서의 한점이 Riemann 공간의 확률분포의 한점이 된다.\n",
    "- 곡률의 1차 근사\n",
    "  - 유클리디안 공간, 이차 근사\n",
    "  - Riemann 공간, 일차 근사(원래 휘어진 공간 이므로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결국 Gradient대신 natural gradient를 사용해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞선 gradient는 non-covariant(1차 근사)해서 생기는 문제이다.\n",
    "- 따라서, policy가 parameterized 된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다.\n",
    "- 이때, steepest direction은 위의 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 따라서, gradient에서 느린 학습이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- direct policy gradient method는 futrue reward의 gradient를 따라 policy를 update한다. 하지만 gradient descent는 non-covariant하다. 따라서 covariant gradient를 제안한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
