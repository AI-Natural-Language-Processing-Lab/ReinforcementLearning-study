{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 직접적인 경험을 하면서 학습하는 알고리즘\n",
    "- Dynamic programming에서 사용하는 bootstrapping을 사용하고, Model free 방식의 장점을 갖추고 있다.\n",
    "- imcomplete 에피소드로 부터 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function을 배우는 것이다.\n",
    "- every visit Monte-Carlo에서 에피소드가 끝나고 받게 되는 보상을 이용해서 value function을 update했다.\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t - V(S_t)) $$\n",
    "- 하지만 Temporal difference는 실제 보상과 다음 step에 대한 미래에 받을 reward를 이용해서 학습한다.\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) $$\n",
    "- 이때 사용하는 보상과 value function의 합을 TD target이라 한다.\n",
    "$$ R_{t+1} + \\gamma V(S_{t+1}) $$\n",
    "- TD target과 실제 \\\\(V(S)\\\\)와 차이를 TD error라 한다.\n",
    "$$ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/48182048-35e5e780-e36d-11e8-9c73-55ea0c9089e4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 왼쪽 그림은 episode가 끝나야 보상에 대한 단계별 update를 하는 반면 오른쪽 그림은 각 단계별로 update가 된다. 따라서 update되는 값의 차이가 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에피소드 중간에도 학습을 할 수 있다는 것이다.\n",
    "- 그래서 종료가 없는 환경에서도 학습을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Return \\\\(G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1}R^T\\\\) is unbiased estimate of \\\\(v_{\\pi}(S_t)\\\\)<br>\n",
    "    v policy가 실제 \\\\(G_t\\\\)에 대해서 unbias라고 할때<br>\n",
    "- True TD target \\\\(R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\\\)is unbiased estimate of \\\\(v_{\\pi}(S_t)\\\\)<br>\n",
    "    TD target도 v policy를 추종하기 때문에 unbias이다.\n",
    "- 하지만  TD target에 V policy를 추정하는 \\\\(V(S_{t+1})\\\\)을 사용하기 때문에 실제값이 아니라 실제값을 추정하는 값이기 때문에 bias가 발생한다.\n",
    "$$ R_{t+1} + \\gamma V(S_{t+1}) $$\n",
    "- 그리고 TD target은 하나의 step에 대한 계산을 하기 때문에 noise가 작게 되고 상대적으로 낮은 variance를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/48182788-e359fa80-e36f-11e8-9834-137b8ba9a8a2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC와 TD를 비교한 것, TD방식이 MC보다 더 빨리 수렴한다. 그리고 bootstrapping 방식이 학습하는데 더 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/48182921-5fecd900-e370-11e8-963d-2fc1b0f50503.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 경우, 첫 번째 episode에서 A가 reward 0을 받았고, B도 0을 받았다. 두 번째 episode에서 B가 1을 받았다. ...\n",
    "- 이때 A는 100%의 확률로 B로 가게 되고, 이때 보상은 0이다.\n",
    "- MC 방식에서는 A가 0의 value를 가지게 된다. 왜냐하면 A를 거쳐 간 episode가 하나인데 최종 보상이 0이기 때문이다. MC는 episode가 끝난 후 보상을 이용해서 value를 update한다.\n",
    "- TD 방식은 다른 episode가 진행 되면서 B의 value가 6으로 update 될 것이기 때문에 A의 value도 같이 update된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD는 MDP 특성을 가지는 알고리즘이다. MDP 환경에서 더 유용하게 동작한다.\n",
    "- MC는 MDP 특성을 가지지 않는다. 그 이유는 bootstrapping을 사용한 연속되는 states에 대한 value를 추정하여 사용거나 update하는것이 아니기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mc](https://user-images.githubusercontent.com/22078438/48183381-caeadf80-e371-11e8-8db8-ffb066f1aa5a.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 episode가 끝이 나야 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![td](https://user-images.githubusercontent.com/22078438/48183384-cc1c0c80-e371-11e8-838b-5325e3d42900.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하나의 step에서 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dp](https://user-images.githubusercontent.com/22078438/48183387-cd4d3980-e371-11e8-8fe9-75159e68ac4b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미 모든 MDP를 안다는 가정을 하는 DP는 value와 reward를 통해서 학습하기 때문에 모든 것을 고려하여 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
