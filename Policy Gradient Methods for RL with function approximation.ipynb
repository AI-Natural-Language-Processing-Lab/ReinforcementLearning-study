{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Approch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State value function\n",
    "  - agent가 특정 state에 있을 때, 특정 state부터 action을 취하고 이동하면서 받는 reward의 평균\n",
    "$$ G_t = R_{t + 1} + \\gamma R_{t + 2} ... = \\sum^{\\infty}_{k = 0} \\gamma^k R_{t + k + 1} $$\n",
    "- 따라서, state value function \\\\(v(s)\\\\)는 어떤 상태 s의 가치이다. 따라서 agent는 다음으로 갈 수 있는 state들 중 높은 가치를 선택한다.\n",
    "<br><br>\n",
    "$$ v(s) = \\mathbb{E}[G_t \\;|\\; S_t = s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 방법을 사용하려면 state에 대한 정보를 모두 알고 있어야한다. 하지만 이것을 몰라도 학습한다는 것이 강화학습의 개념이기 때문에 어느쪽으로 가는 것이 더 나은지에 대한 정보만 있으면 행동할 수 있다. Action value function\n",
    "  - 특정 state s에서 action a를 취했을 때 받는 return의 기대값. 어떤 행동이 얼마나 좋은지에 대한 판단.\n",
    "  <br><br>\n",
    "  $$ q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\;|\\; S_t = s, A_t = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- greedy action-selection policy는 deterministic한 policy를 찾는 것으로 수렴하게 되지만, 종종 optimal policy는 stochastic한 성질을 갖게 됨으로 최적의 policy를 찾을 수 없다고 한다.\n",
    "  - 위 문제는 \\\\(\\epsilon-greedy\\\\) action-selection method로 개선 가능하다.\n",
    "- value function의 수시로 변하는 값들 때문에 action이 크게 변할 수 있다.\n",
    "  - 위 문제 때문에 수렴을 안할 수도 있다. 따라서 policy search라는 기법이 제안되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient based optimization\n",
    "  - policy의 변화를 제어하는 parameter를 통해서 policy를 변화시킨다. policy 변화를 모델링하기 위해서 parameter \\\\(\\theta\\\\)를 사용하여 policy를 \\\\(\\pi_{\\theta}\\\\)로 표현한다.\n",
    "  - 최적의 policy를 찾기 위해서 expected return \\\\(E[R|\\theta]\\\\)를 최대화하도록 parameter를 optimization한다.<br><br>\n",
    "  $$ \\pi^* = arg \\max_{\\pi} \\mathbb{E}[R\\;|\\;\\pi] $$\n",
    "  <br>\n",
    "  $$ \\pi_{\\theta}: \\Theta \\to \\Pi $$\n",
    "  <br>\n",
    "  $$ \\pi^* = arg \\max_{\\theta} \\mathbb{E}[R \\;|\\; \\theta] $$\n",
    "- gradient free optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deterministic approximation\n",
    "  - Markov decision process의 dynamics를 모델링한 후 수식을 통해서 구한다.\n",
    "- monte carlo estimation\n",
    "  - 많은 sample을 얻은 후 empirical하게 expected return을 계산한다.\n",
    "  - likelihood-ratio estimate 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 monte carlo estimation이 좋을 수 있다. 하지만 gradient estimation 방법을 고안하는 것도 무척 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Gradient Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameter \\\\(\\theta\\\\)를 가지는 random variable \\\\(X\\\\)가 있음. \\\\(X: \\Omega \\mapsto \\mathcal{X} \\\\)\n",
    "- 이 \\\\(x\\\\)에 대한 함수 \\\\(f\\\\)가 있다. \\\\(f: \\mathcal{X} \\mapsto \\mathbb{R} \\\\)\n",
    "- expected return과 같이 \\\\(E[f(x)]\\\\)를 최대화 하고자 한다. 따라서, \\\\(\\nabla_{\\theta}E[f(x)]\\\\)를 구해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\( \\begin{align} \\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] &= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx} \\\\ &= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \\\\ &= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \\\\ &= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \\\\ &= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})] \\end{align} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expectation은 많은 sample을 모아서 평균취하는 것으로 근사화 할 수 있음. 따라서,<br><br>\n",
    "$$ {\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식으로 바꿀 수 있다. 하지만 monte carlo estimation의 경우 하나의 epsiode에 대한 모든 reward를 가지고 계산하기 때문에 variance가 높을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\rho\\\\)는 \\\\(\\theta\\\\)에 대해서 미분 가능하고,\n",
    "- 아래 식으로 gradient가 update되며 policy는 수렴한다.<br><br>\n",
    "$$ \\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, reward를 표현하는 \\\\(\\rho\\\\)에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Reward Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시간의 흐름에 따른 reward 표현이 아니라, 모든 시간의 reward를 평균내서 표현하는 방법이다.\n",
    "$$ \\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start-State Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start-state formulation은 시간의 흐름에 따라 감소하는 reward를 표현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RLKorea 글 참조](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
