{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 공부 했던 Dynamic programming은 이미 알고있는 Markov decision process를 bellman equation으로 풀었다.\n",
    "- 하지만, Model free는 MDP를 모르는 상황에서 환경과 상호작용을 하면서 경험을 통해서 학습하게 되는 방식이다.\n",
    "- Prediction, value를 estimation하는 것, Model free에서는 MDP를 모르기 때문에 환경과 상호작용하여 value-function을 찾는 것이다.\n",
    "- Control, 위에서 찾은 value-function을 통해서 policy를 찾는 것이다.\n",
    "- 방법: Monte-Carlo Learning, Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte carlo 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo는 에피소드에서 경험하면서 직접 환경에 대해서 학습하는 방법이다.\n",
    "- transitions과 rewards에 대한 MDP를 전혀 모르고 시작한다.\n",
    "- 에피소드가 종료된 후에 받는 reward의 평균값이 value로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목표, policy \\\\(\\pi\\\\)를 따르고 에피소드 경험을 바탕으로 \\\\(v_{\\pi}\\\\) 배우는 것\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t\\;|\\;S_t=s] $$\n",
    "- MC policy의 evaluation은 empirical mean을 사용한다. expected 대신에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state를 evaluate하기 위한 2가지 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first visit MC, 에피소드에서 하나의 state를 여러번 지나갈 수 있는데 이때 해당 state에 첫번째 방문한 value만 사용한다.\n",
    "- 에피소드가 여러번 반복되므로 각 에피소드에 대한 평균으로 value를 추정한다.\n",
    "- 에피소드가 충분히 진행되면 평균값으로 추정한 value가 최적화된 실제 value와 같게 되고 이것을 통해서 policy를 update하면 최적의 policy를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- every visit MC, 하나의 state를 두 번이상 지나갔다면 이때 모든 value를 각각 사용하여 평균내서 추정하는 방식이다.\n",
    "- 보통 first visit MC를 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평균 구하는 식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean \\\\(\\mu_1, \\mu_2, ... \\\\)of a sequence \\\\(x_1, x_2, ... \\\\)can be computed incrementally\n",
    "$$ \\begin{split} \\mu_k &= \\frac{1}{k} \\sum^k_{j=1}x_j \\\\ &= \\frac{1}{k} \\left( x_k + \\sum^{k-1}_{j=1}x_j \\right) \\\\ &= \\frac{1}{k}(x_k + (k-1)\\mu_{k-1}) \\\\ &= \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1}) \\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연속되는 x의 값을 모두 더해서 k로 나눠주는 것이 평균인데, 위 식은 현재와 그 외의 것을 분리하여 공식으로 나타내었다.\n",
    "- \\\\(\\frac{1}{k}\\\\) 이후 부분이 error가 되며 이것을 통해서 incremental update하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update \\\\(V(s)\\\\) incrementally after episode \\\\(S_1, A_1, R_2, ..., S_T\\\\)\n",
    "- For each state \\\\(S_t\\\\) with return \\\\(G_t\\\\)\n",
    "$$ \\begin{split} N(S_t) &\\leftarrow N(S_t) + 1 \\\\ V(S_t) &\\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t)) \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하나의 에피소드가 완료되면 점진적으로 value function을 update한다.\n",
    "- 에피소드 수행 횟수 \\\\(N(S)\\\\)와 각 보상을 평균해서 \\\\(V(S)\\\\)를 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In non-stationary problems, it can be useful to track a running mean, forget old episodes\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- non-stationary한 환경에서는 이전의 횟수를 사용하지 않고 \\\\(\\alpha\\\\)를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
