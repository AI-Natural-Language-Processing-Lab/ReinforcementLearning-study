{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration & Exploitation Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploitation, 현재 내가 가진 정보를 바탕으로 제일 좋은 선택을 한다.\n",
    "- Exploration, 결정을 위한 정보를 모은다. 정보를 모으기 위해서 최적의 선택이 아니지만 그 행동을 취할 수도 있다.\n",
    "- 위 두 관계는 Trade-off 관계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent는 exploration과 exploitation을 적절히 섞어가며 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떻게 Trade-off를 조절할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\epsilon\\\\)-greedy 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizm in the Face of Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불확실성을 좀 더 긍정적으로 본다.(무슨말??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내가 어느 정보를 얼마만큼 알고있다. 라는 것을 State에 포함시켜서 접근하는 방법론."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 왜?, MDP보다 훨씬 간단한 문제이다. 그래서 이것을 통해서 MDP에 대해서 이해를 한다.\n",
    "- 무엇?, MDP에서 다 빼고 \\\\( \\left\\langle \\mathcal{A}, \\mathcal{R} \\right\\rangle\\\\)만 남긴다. 예를 들어, 카지노에 밴딩머신을 생각할 수 있다. action을 하면 reward를 받는다. bandit마다 자신만의 확률분포가 있어서 sampling된 것으로 결과가 나옴. state가 없다. 따라서 간단함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- m개의 actions이 있고(팔을 당기는 행동), 행동을 했을 때 확률 분포는 모른다. 따라서 그것을 찾는 것이 목표이다. 각 step마다 하나의 machine을 당긴다. 우리의 목표는 cumulative reward를 최대화 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret(후회를 어떻게 정의하는가?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action-value, the mean reward for action a, 특정 model의 reward의 기대값.\n",
    "  - \\\\(Q(a) = \\mathbb{E}[r|a]\\\\), a를 취했을 때 r만 받고 끝남. one-step이라서 그렇다.\n",
    "- Optimal value \\\\(V^*\\\\)\n",
    "  - 내가 당길수 있는 machine의 각각의 기대값 중에 제일 기대값이 높은 것. 제일 좋은 슬롯 머신\n",
    "  - \\\\(V^* = Q(a^*) = max_{a \\in \\mathcal{A} Q(a)} \\\\)\n",
    "- 가장 좋은 optimal value와 특정 model의 value값의 차이가 Regret이다.\n",
    "  - 각각의 slot machine을 알 때 위의 Regret을 구할 수 있다.\n",
    "  - \\\\(l_t = \\mathbb{E}[V^* - Q(a_t)]\\\\)\n",
    "  - Regret은 action 별로 정의된다.\n",
    "- Total Regret\n",
    "  - 내가 t번 슬롯 머신을 당겼을 때 계산한 regret을 다 더한다.\n",
    "  - \\\\(L_t = \\mathbb{E}[\\sum^t_{\\tau=1} V^* - Q(a_{\\tau})] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Maximize cumulative reward = minimize total regret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- counts, action 몇 번 당겼나? count는 각 action별로 정의 되는 개념이다.\n",
    "- gap, 슬롯 머신 별로 정의됨. 젤 좋은 슬롯 머신에서 그 슬롯 머신의 value 뺀 것, 이 머신은 제일 좋은 머신에 비해서 얼마나 부족한가? 에 대한 의미를 담고 있음.<br><br>\n",
    "\n",
    "$$ \\begin{split} L_t &= \\mathbb{E}[\\sum^t_{\\tau=1} V^* - Q(a_{\\tau})] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)] (V^* - Q(a)) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(A)] \\vartriangle_a \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화를 낙관적으로 한다. 모든 value를 maximum으로 초기화 한다. 예를 들어서, machine을 당기는데 모든 machine이 1억을 가지고 있다고 생각하고 시작을 한다.\n",
    "- 간단한 아이디어지만 Q를 높은 값으로 초기화 하고, Monte-carlo 방식(계속해보고 평균내는 것)으로 Q를 update한다.\n",
    "- 처음에 높은 값으로 초기화를 했기 때문에 안해본 머신을 주로 사용하게 된다. 어느정도 자리를 잡게 되면 하나만 하게 된다.\n",
    "- 초기 단계에 exploration을 권장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 좋지만, 수렴한다는 보장은 없다. 또한, 초기값을 잡아주는 방법에 대한 이론이기 때문에 결국은 linear하게 된다.\n",
    "- 하지만, 실무에서 좋은 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decaying \\\\(\\epsilon_t\\\\)-Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\epsilon\\\\)을 줄여나가는 방법\n",
    "- 처음에는 높은 \\\\(\\epsilon\\\\)을 가지고 exploration을 하고 점점 정보가 많이 쌓일 수록 \\\\(\\epsilon\\\\)을 줄여나감.<br><br>\n",
    "$$ \\begin{split} c &> 0 \\\\\n",
    "d &= min_{a|\\vartriangle_a > 0} \\vartriangle_i \\\\ \n",
    "\\epsilon_t &= min \\left\\{1, \\frac{c|\\mathcal{A}|}{d^2 t} \\right\\} \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d, 2등 머신이 1등머신에 비해 얼마나 부족한가를 나타냄.\n",
    "- d가 작아지면 epsilon을 많이 하게 되고, d가 커지면 epsilon이 작아짐.\n",
    "- 어쨌든 t가 분모에 있기 때문에 step이 지날수록 값은 작아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- total regret이 log형태가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- log보다 더 아래로 갈수는 없다.\n",
    "- 제일 좋은 머신과 다른 머신 사이에 유사도에 의해서 난이도가 결정된다.\n",
    "- 아무리 잘해도 더 잘할 수 없는 performance가 존재한다. -> lower bound\n",
    "- arm이 비슷한 확률분포를 가질 수록 문제가 어려워진다.<br><br>\n",
    "\n",
    "$$ lim_{t->\\infty} L_t \\ge log t \\sum_{a|\\vartriangle_a > 0} \\frac{\\vartriangle_a}{KL(\\mathcal{R}^a||\\mathcal{R}^{a^*})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gap \\\\(\\vartriangle_a\\\\), 모든 action에 대해서 합을 하는데 확률 분포가 비슷할 수록 KL-divergence값이 작아지는데 그래서 결국 값은 커진다. 따라서 문제가 어려워짐. gap이 클수록 문제가 어려워짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize in the Face of Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3개의 machine이 있는데 각각 machine의 기대값을 추정할 때 그렇다면 어떤 machine을 당겨야 하나?\n",
    "  - 아래 그림에서 초록색 머신을 당겨야 이치에 맞다.\n",
    "- Uncertainty를 긍정적으로 보자.\n",
    "  - 파란색 머신은 uncertainty가 높다. uncertainty가 높을 수록 explore해야 하는 머신이다. 그래서 잘 모르니까 best action으로 바뀔 수 있다는 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/50445065-6995c480-094f-11e9-8f08-173a2dcf3af5.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 action value에 대해서 upper confidence가 있다 -> \\\\(\\hat{U}_t(a)\\\\)\n",
    "- 예를 들어, 95% 신뢰 구간을 생각하면, 실제 \\\\(Q(a)\\\\)가 내가 생각하는 추정치보다 작을 확률이 95%인 것이다.\n",
    "- Upper Confidence Bound\n",
    "  - \\\\(Q\\\\)는 측정할 수록 점점 정확해 질 것이고, \\\\(U\\\\)는 action을 당긴 횟수가 많아질수록\n",
    "  - \\\\(N_t(a)\\\\)가 작아질수록 -> \\\\(\\hat{U}_t(a)\\\\)는 커지고, 신뢰구간이 넓어지는 것이다. Uncertainty\n",
    "  - \\\\(N_t(a)\\\\)가 커지면 -> 신뢰구간이 좁아짐.\n",
    "$$ a_t = arg max_{a \\in \\mathcal{A}} \\hat{Q}_t(a) + \\hat{U}_t(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q + U를 최대화하는 방향으로 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoeffding's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 언급했던 Upper Confidence Bounds와 섞어 사용할 수 있는 부등식은 매우 많은데 그중에 하나이다.\n",
    "- 분포와 관계없이 항상 사용할 수 있는 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(X_1, ... , X_t\\\\)가 \\\\([0,1]\\\\)사이 random variable이라 하면, 그것의 sample mean이 \\\\(\\bar{X}_t = \\frac{1}{\\tau} \\sum^t_{\\tau = 1} X_{\\tau} \\\\) X는 실제 모평균, u는 틀릴 확률.\n",
    "- 실제 값이 sample 평균보다 u보다 더 틀릴 확률이 좌변이다.<br><br>\n",
    "\n",
    "$$ \\mathbb{P}[\\mathbb{E}[X] > \\bar{X}_t + u] \\le e^{-2tu^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 UCB를 적용하면,\n",
    "- \\\\(Q(a)\\\\)가 기대값이고, \\\\(\\hat{Q}_t(a)\\\\)가 sample 평균이고, \\\\(U_t(a)\\\\)가 UCB가 된다.<br><br>\n",
    "$$ \\mathbb{P}[Q(a) > \\hat{Q}_t(a) + U_t(a)] \\ge e^{-2N_t(a)U_t(a)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper confidence bounds 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률 \\\\(p\\\\)를 정해서 풀면 됨.<br><br>\n",
    "$$ \\begin{split} e^{-2N_t(a)U_t(a)^2} &= p \\\\\n",
    "U_t(a) &= \\sqrt{\\frac{-log p}{2 N_t(a)}} \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UCB1 알고리즘을 최대화하는 action을 고르면 된다.<br><br>\n",
    "\n",
    "$$ a_t = argmax_{a \\in \\mathcal{A}} Q(a) + \\sqrt{\\frac{2 log t}{N_t(a)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upper confidence bound를 계산하려고 하니, 모분포를 모르기 때문에 우리는 Hoeffding's inequality를 사용해야한다. 따라서, \\\\(Q(a) + U\\\\)를 최대화 하는 action을 고르자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 total regret 함수는 log가 된다.<br><br>\n",
    "$$ lim_{t->\\infty} L_t \\ge 8 log t \\sum_{a|\\vartriangle_a > 0} \\vartriangle_a  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hoeffding's 부등식은 제약이 많이 없다. random variable을 [0, 1]사이로 맞춰주기만 한다.(그러므로 max값만 알면 된다.) 따라서 성능 향상이 많지는 않고, 다른 부등식들의 경우 제약이 많고 알아야하는 정보가 많다. 따라서 성능 향상이 더 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward distribution에 대한 prior knowledge를 쓴다. 사람이 어떤 분포를 따를 거다라는 것을 잘 가정한다. ex) 베르누이 분포를 따른다. 가우시안 분포를 따른다.\n",
    "- prior knowledge를 보고 posterior 분포를 구한다. posterior를 보고 exploration을 가이드 한다.\n",
    "  - Bayesian UCB\n",
    "  - Probability matching\n",
    "- prior knowledge가 정확할수록 성능이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action이 젤 좋은 확률을 그대로 뽑는다.\n",
    "- Thompson sampling이라는 방법이 있음.\n",
    "  - bayes law에 의해서 reward의 확률분포가 있을 때, 나는 sampling을 할 수 있다.\n",
    "  - 각각의 슬롯머신에서 sampling을 한다.\n",
    "  - 그 값으로 젤 값이 높은 action을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처음 posterior 분포가 있고, 그 분포를 통해서 sampling한 것 중에서 해서 가장 좋은 슬롯머신을 한번 당긴다.\n",
    "- 당긴 후 받는 reward를 통해서 다시 posterior 분포를 구하고 sampling을 해서 sampling한 것 중에서 가장 좋은 슬롯 머신을 1번만 당긴다.\n",
    "- 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- exploration을 하는 이유는 정보를 얻기 위해서 하는 것인데, 정보의 가치를 계량화 할 수 있나??\n",
    "- 정보를 계량화 할 수 있으면 최적의 결정을 할 수 있음.\n",
    "- uncertain한 상황에서 정보 얻는 것이 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state에 정보를 더한다.\n",
    "- one-step MDP에 information state \\\\(\\tilde{s}\\\\)를 넣음.\n",
    "  - 지금까지 나온 정보를 요약해주는 숫자이다.\n",
    "  - 이 숫자를 state로 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- information state를 넣음으로 MDP가 됨.<br><br>\n",
    "$$ \\tilde{\\mathcal{M}} = \\left\\langle \\tilde{\\mathcal{S}}, \\mathcal{A}, \\tilde{\\mathcal{P}}, \\mathcal{R}, \\gamma \\right\\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, MDP는 많이 풀어봤으니까, MDP를 풀면됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bandits 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward가 bernoulli 분포를 통해서 정의됨. 이기거나 지거나 밖에 없음.\n",
    "- Win or lose a game with 확률 \\\\(\\mu_a\\\\)\n",
    "- 이때 \\\\(\\mu_a\\\\)가 가장 높은 머신을 찾고 싶다.\n",
    "- information state \\\\(\\tilde{s} = \\left\\langle \\alpha, \\beta \\right\\langle\\\\)\n",
    "  - \\\\(\\alpha_a\\\\)는 a를 당겼을 때 진경우 count\n",
    "  - \\\\(\\beta_a\\\\)는 a를 당겼을 때 이겼을 경우 count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그러면 MDP가 생겼고, 이것을 통해서 앞서 배운 강화학습으로 풀수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes-Adaptive Bernoulli Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(Beta(\\alpha_a, \\beta_a)\\\\)분포로 prior를 만드는 것이다. 그래서 prior를 정해놓고,\n",
    "- 매번 action을 할 때 마다 posterior를 update한다.\n",
    "- 그러면 그 posterior 분포 자체가 information state가 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/50463507-9b4e7000-09cf-11e9-9b7d-3a6e981154c8.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림을 설명하면, drug1의 분포와 drug2의 분포(prior)를 정하고 성공과 실패에 따라서 posterior분포가 달라지고, (반복) 따라서, posterior 분포가 계속 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi armed bandit 문제가 계속 이어짐.\n",
    "- 예를 들어) 광고에서 어떤 광고를 클릭했을 때 그 광고를 통해서 다른 광고를 추천하게 된다. 클릭 history를 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[팡요랩 유투브 동영상](https://www.youtube.com/watch?v=nm6RwuA_pGE&t=2384s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
