{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration & Exploitation Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploitation, 현재 내가 가진 정보를 바탕으로 제일 좋은 선택을 한다.\n",
    "- Exploration, 결정을 위한 정보를 모은다. 정보를 모으기 위해서 최적의 선택이 아니지만 그 행동을 취할 수도 있다.\n",
    "- 위 두 관계는 Trade-off 관계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent는 exploration과 exploitation을 적절히 섞어가며 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떻게 Trade-off를 조절할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\epsilon\\\\)-greedy 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizm in the Face of Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불확실성을 좀 더 긍정적으로 본다.(무슨말??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information State Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내가 어느 정보를 얼마만큼 알고있다. 라는 것을 State에 포함시켜서 접근하는 방법론."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 왜?, MDP보다 훨씬 간단한 문제이다. 그래서 이것을 통해서 MDP에 대해서 이해를 한다.\n",
    "- 무엇?, MDP에서 다 빼고 \\\\( \\left\\langle \\mathcal{A}, \\mathcal{R} \\right\\rangle\\\\)만 남긴다. 예를 들어, 카지노에 밴딩머신을 생각할 수 있다. action을 하면 reward를 받는다. bandit마다 자신만의 확률분포가 있어서 sampling된 것으로 결과가 나옴. state가 없다. 따라서 간단함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- m개의 actions이 있고(팔을 당기는 행동), 행동을 했을 때 확률 분포는 모른다. 따라서 그것을 찾는 것이 목표이다. 각 step마다 하나의 machine을 당긴다. 우리의 목표는 cumulative reward를 최대화 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret(후회를 어떻게 정의하는가?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action-value, the mean reward for action a, 특정 model의 reward의 기대값.\n",
    "  - \\\\(Q(a) = \\mathbb{E}[r|a]\\\\), a를 취했을 때 r만 받고 끝남. one-step이라서 그렇다.\n",
    "- Optimal value \\\\(V^*\\\\)\n",
    "  - 내가 당길수 있는 machine의 각각의 기대값 중에 제일 기대값이 높은 것. 제일 좋은 슬롯 머신\n",
    "  - \\\\(V^* = Q(a^*) = max_{a \\in \\mathcal{A} Q(a)} \\\\)\n",
    "- 가장 좋은 optimal value와 특정 model의 value값의 차이가 Regret이다.\n",
    "  - 각각의 slot machine을 알 때 위의 Regret을 구할 수 있다.\n",
    "  - \\\\(l_t = \\mathbb{E}[V^* - Q(a_t)]\\\\)\n",
    "  - Regret은 action 별로 정의된다.\n",
    "- Total Regret\n",
    "  - 내가 t번 슬롯 머신을 당겼을 때 계산한 regret을 다 더한다.\n",
    "  - \\\\(L_t = \\mathbb{E}[\\sum^t_{\\tau=1} V^* - Q(a_{\\tau})] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Maximize cumulative reward = minimize total regret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- counts, action 몇 번 당겼나? count는 각 action별로 정의 되는 개념이다.\n",
    "- gap, 슬롯 머신 별로 정의됨. 젤 좋은 슬롯 머신에서 그 슬롯 머신의 value 뺀 것, 이 머신은 제일 좋은 머신에 비해서 얼마나 부족한가? 에 대한 의미를 담고 있음.<br><br>\n",
    "\n",
    "$$ \\begin{split} L_t &= \\mathbb{E}[\\sum^t_{\\tau=1} V^* - Q(a_{\\tau})] \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)] (V^* - Q(a)) \\\\\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(A)] \\vartriangle_a \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화를 낙관적으로 한다. 모든 value를 maximum으로 초기화 한다. 예를 들어서, machine을 당기는데 모든 machine이 1억을 가지고 있다고 생각하고 시작을 한다.\n",
    "- 간단한 아이디어지만 Q를 높은 값으로 초기화 하고, Monte-carlo 방식(계속해보고 평균내는 것)으로 Q를 update한다.\n",
    "- 처음에 높은 값으로 초기화를 했기 때문에 안해본 머신을 주로 사용하게 된다. 어느정도 자리를 잡게 되면 하나만 하게 된다.\n",
    "- 초기 단계에 exploration을 권장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 좋지만, 수렴한다는 보장은 없다. 또한, 초기값을 잡아주는 방법에 대한 이론이기 때문에 결국은 linear하게 된다.\n",
    "- 하지만, 실무에서 좋은 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decaying \\\\(\\epsilon_t\\\\)-Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\epsilon\\\\)을 줄여나가는 방법\n",
    "- 처음에는 높은 \\\\(\\epsilon\\\\)을 가지고 exploration을 하고 점점 정보가 많이 쌓일 수록 \\\\(\\epsilon\\\\)을 줄여나감.<br><br>\n",
    "$$ \\begin{split} c &> 0 \\\\\n",
    "d &= min_{a|\\vartriangle_a > 0} \\vartriangle_i \\\\ \n",
    "\\epsilon_t &= min \\left\\{1, \\frac{c|\\mathcal{A}|}{d^2 t} \\right\\} \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d, 2등 머신이 1등머신에 비해 얼마나 부족한가를 나타냄.\n",
    "- d가 작아지면 epsilon을 많이 하게 되고, d가 커지면 epsilon이 작아짐.\n",
    "- 어쨌든 t가 분모에 있기 때문에 step이 지날수록 값은 작아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- total regret이 log형태가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
