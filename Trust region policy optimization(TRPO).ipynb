{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust region policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 높은 차원의 action space를 가진 환경부터 action은 적지만 control parameter가 많은 환경까지 범용적으로 사용할 수 있는 좋은 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제 정의, expected discounted reward를 최대화하는 policy를 찾자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP: \\\\(\\langle S, A, P, r, \\rho_0, \\gamma \\rangle\\\\)\n",
    "  - \\\\(P \\\\): state transition probability\n",
    "  - \\\\(\\rho_0\\\\): 초기 state \\\\(s_0\\\\)에 대한 distribution\n",
    "  - \\\\(r\\\\): reward function\n",
    "  - \\\\(\\gamma \\\\): discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-function으로 action을 선택한다.\n",
    "- ex) SARSA, Q-Learning, DQN, Dueling network, prioritized experience replay, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy를 가지고 있으며 policy는 parameter로 이루어져 있음.\n",
    "- REINFORCE, Actor-critic, A3C, TRPO, PPO, GAE, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective function(policy parameter \\\\(\\theta\\\\)의 함수)을 최대화하는 policy를 구하는 것\n",
    "- 각각의 iteration마다 objective function의 gradient를 따라 paramter를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample의 효율성이 없다.\n",
    "  - policy gradient는 현재 policy에 대한 estimate을 update하고 있는데, 한번 사용하면 버리게됨 따라서 효율성에서 떨어진다.\n",
    "- parameter space에서 거리와 policy space에서 거리가 다르다.\n",
    "  - parameter에서 update를 했을 때 policy가 받는 영향이 클수도 있고 작을 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 효율적으로 기대값을 추정하기 위해 만들어졌다.\n",
    "- 목적, 기대값을 계산하고자 하는 확률 분포 p(x)의 확률 밀도 함수를 알고 있지만 p에서 샘플을 생성하기 어려울 때 비교적 샘플을 생성하기 쉬운 q(x)에서 샘플을 생성하여 p의 기대값을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E_{x\\sim p}[f(x)] = \\int p(x)f(x)dx = \\int \\frac{p(x)}{q(x)}q(x)f(x)dx = E_{x \\sim q}\\begin{bmatrix} \\frac{p(x)}{q(x)}f(x) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식에서 \\\\(\\frac{p(x)}{q(x)}\\\\)를 likelihood ratio라 하며, \\\\(p\\\\)를 normal distribution, \\\\(q\\\\)를 importance distribution이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, importance sampling에서 \\\\(p\\\\)의 기대값 \\\\(E_{x \\sim p}[f(x)]\\\\)를 아래와 같이 추정한다.\n",
    "$$ E_{x \\sim p}[f(x)] \\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{p(x_n)}{q(x_n)}f(x_n),\\; x_n\\sim q $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy gradient, importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy gradient에서 이전 policy의 sample을 사용하기 위해서 importance sampling 사용할 수 있음.\n",
    "- importance weight(\\\\(\\frac{p(x_n)}{q(x_n)}\\\\))는 unbound(variance의 범위가 커서) -> 학습이 잘 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step in policy space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 문제로 언급되었던, parameter space는 policy space가 아니다.\n",
    "  - 그렇다면, parameter space가 아니라 policy space에서 조금씩 update가 가능한가?\n",
    "    - 두 policy(old policy, new policy)사이에 KL-divergence를 constraint로 (trust region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 얼마나 두 확률분포가 다른지를 설명해주는 지표\n",
    "- \\\\(D_KL(P||Q) = \\sum_i P(i)log \\frac{P(i)}{Q(i)} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주로 아직 확인되지 않은 모델을 특정 확률분포로 근사시키는데 사용한다.\n",
    "- KL-divergence를 가장 낮추는 정규분포를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q(근사 모델)를 최대한 P(실제 데이터 분포)에 가깝게 만들려면, KL-divergence를 최소화하도록 학습을 시킨다.\n",
    "- KL-divergence의 특징\n",
    "  - 항상 0이상의 값을 갖는다.\n",
    "  - 비대칭적 함수이다.\n",
    "    - \\\\(D_KL(p||q)\\\\)와 \\\\(D_KL(q||p)\\\\)는 다른값이다.\n",
    "  - 두 확률 분포가 동일하면 KL-divergence는 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence 분해하기\n",
    "  $$ \\begin{split} D_KL(P||Q) &= \\sum_i P(i) log \\frac{P(i)}{Q(i)} \\\\ &= \\sum_i P(i)logP(i) - \\sum_i P(i)logQ(i) \\\\ &= -H(P) + H(P,Q) \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence를 둘로 나눌 수 있기 때문에 크로스 엔트로피로 표현이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stable한 update는 안되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower bound를 최적화 하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy old와 policy new의 관계를 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy \\\\(\\pi_{old}\\\\)의 objective func, \\\\(J(\\pi_{old}) = E_{s_0, a_0, ... \\sim \\pi_{old}}[\\sum^\\infty_{t=0} \\gamma^t r(s_t)] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy \\\\(\\pi\\\\) objective func, \\\\(J(\\pi) = E_{s_0, a_0, ... \\sim \\pi}[\\sum^\\infty_{t=0} \\gamma^t r(s_t)] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi) = J(\\pi_{old}) + E_{s_0, a_0, ... \\sim \\pi} \\begin{bmatrix} \\sum^\\infty_{t=0} \\gamma A_{\\pi_{old}}(s_t, a_t) \\end{bmatrix} \\\\)\n",
    "<br><br>\n",
    "- \\\\(J(\\pi) = J(\\pi_{old}) + \\sum_s \\rho_\\pi(s) \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy Iteration & objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy improvement -> greedy policy improvement\n",
    "  - 어떤 행동 상태가 positive advantage를 가지면 policy를 improve한다.\n",
    "  - \\\\( \\pi = argmax_a A_{\\pi_{old}}(s,a) \\\\)\n",
    "- approximation error -> \\\\(A_{\\pi_{old}}(s,a) < 0 \\\\)가능, \\\\( \\rho_\\pi(s) \\\\)구하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi)\\\\) -> local approximation\n",
    "$$ L_{\\pi_{old}}(\\pi) = J(\\pi_{old}) + \\sum_s \\rho_{\\pi_{old}}(s) \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi) = J(\\pi_{old}) + \\sum_s {\\color{Red}{\\rho_\\pi(s)}} \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a)\\\\)\n",
    "<br><br>\n",
    "- 위 식에서 아래 식으로 빨간색 부분이 바뀌었는데, 이것은 policy의 변화가 적다면 steady state distribution의 변화도 무시할 수 있을 것이다.\n",
    "- \\\\( L_{\\pi_{old}}(\\pi) = J(\\pi_{old}) + \\sum_s {\\color{Red} {\\rho_{\\pi_{old}}(s)}} \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, Approximation error는 얼마나 되냐? -> \\\\(\\color{Red}{\\text{Lower bound}}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local approximation with parameterized policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식은,\n",
    "- \\\\(L_{\\pi_{\\theta_0}}(\\pi_{\\theta_0}) = J(\\pi_{\\theta_0})\\\\)으로 표현 가능하고,\n",
    "- Kakade & Langford의 증명, 따라서 \\\\(\\pi_{\\theta_0}\\\\)에서 policy를 조금만 변화시켰을 때 local approximation을 improve -> objective function도 improve함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 local approximation을 improve하는 것이 objective function도 improve하는 것을 증명 했지만, 어느 정도 변해야 objective function의 improve를 보장하는지 알 수 없다.\n",
    "- 따라서, explicit한 J의 lower bound를 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. policy improvement(mixture policy 가정)\n",
    "  - 현재 policy: \\\\(\\pi_{old}\\\\), 새로운 policy: \\\\(\\pi_{new}\\\\)\n",
    "  - \\\\(\\pi' = argmax_{\\pi'}L_{\\pi_{old}}(\\pi')\\\\)\n",
    "  - \\\\(\\pi_{new}(a|s) = (1-\\alpha)\\pi_{old}(a|s) + \\alpha \\pi'(a|s)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lower bound of J\n",
    "  - \\\\(J(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - \\frac{2\\epsilon \\gamma}{(1-\\gamma)^2} \\alpha^2 \\\\)\n",
    "  - \\\\(where \\epsilon = max_s|E_{a \\sim \\pi'(a|s)}[A_\\pi(s,a)]| \\\\)\n",
    "  - 따라서 이제는 lower bound를 최적화하면 된다.\n",
    "  - parameterized policy에 대해서 mixture policy를 구할 수 없기 때문."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative policy Iteration의 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mixture policy를 통해 old policy와 new policy사이의 거리를 나타내지 않고, KL-divergence를 사용한다.\n",
    "  - \\\\( \\alpha^2 -> D^{max}_{KL}(\\pi_{old}, \\pi_{new}) \\\\)\n",
    "- 변형된 lower bound 식 사용\n",
    "  - \\\\( \\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - CD^{max}_{KL}(\\pi_{old}, \\pi_{new})\\\\)\n",
    "  - \\\\( where C = \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^2}, \\epsilon=max_s|E_{a \\sim \\pi'(a|s)}[A_\\pi(s,a)]| \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower bound of objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - CD^{max}_{KL}(\\pi_{old}, \\pi_{new}) \\\\)\n",
    "- 현재 \\\\(\\theta\\\\)에서는 값이 모두 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49554137-3bb7f400-f93e-11e8-891a-6cb11a27d9c1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-constraint optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameterized policy를 lower bound식에 적용한다.\n",
    "  - \\\\(J(\\theta) \\geq L_{\\theta_{old}}(\\theta) - CD^{max}_{KL}(\\theta_{old}, \\theta) \\\\)\n",
    "- Lower bound의 optimization: C가 너무 커서 step이 작다.\n",
    "  - \\\\(maximize_{\\theta}[L_{\\theta_{old}}(\\theta) - CD^{max}_{KL}(\\theta_{old}, \\theta)] \\\\)\n",
    "- Large and robust step: KL-penalty -> KL-constraint\n",
    "  - \\\\(maximize_\\theta L_{\\theta_{old}}(\\theta) \\\\)\n",
    "  - \\\\(s. t. D^{max}_{KL}(\\theta_{old}, \\theta) \\leq \\delta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보의 양"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 KL-constraint를 정확하게 알기 위해서 entropy의 개념과 KL-divergence의 개념을 다시 공부해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤하게 나타나는 숫자가 x이고 그 숫자 x가 나타날 확률이 \\\\(p(x)\\\\)라 하면 확률이 낮은 정보일수록 희소성이 있다.\n",
    "- 정보의 양, \\\\(h(x) = -logp(x)\\\\)\n",
    "  - 로그 함수의 성질에 따라 \\\\(p(x)\\\\)가 0에 가까워지면 무한대로 가고,\n",
    "  - 1일때는 0이다. 따라서 \\\\(p(1)\\\\)은 항상 일어나는 일이라는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- entropy\n",
    "  - 여러 사건이 발생할 경우 entropy는 \\\\(h(x)\\\\)의 weighted sum으로 정의한다.\n",
    "  - \\\\(H[x] = -\\sum^N_{i=1} p_i logp_i\\\\)\n",
    "  - 위 식은 사건이 많아도 쉽게 계산이 되는 특징이 있고, \\\\(p(x)\\\\)들의 합은 무조건 1이 되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-constraint optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(maximize_\\theta L_{\\theta_{old}}(\\theta) \\\\)<br><br>\n",
    "- \\\\(s.t. D^{max}_{KL}(\\theta_{old}, \\theta) \\leq \\delta\\\\)\n",
    "  - policy space에서는 작은 step이다.(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49581154-5cab3400-f994-11e8-9a35-1a53ca938fdb.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서 \\\\(\\eta(\\theta)\\\\)는 최대화하고 싶은 expected discounted rewards이다.\n",
    "- \\\\(L(\\theta)-C \\cdot KL\\\\)은 lower bound function이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(L_{\\theta_{old}}(\\theta) = J(\\theta_{old}) + \\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_{\\theta}(a|s) A_{\\pi_{\\theta_{old}}}(s, a)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(maximize_{\\theta} L_{\\theta_{old}}(\\theta) \\rightarrow maximize_{\\theta} \\begin{bmatrix} \\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a) \\end{bmatrix} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, \\\\(\\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a)\\\\) 이부분을 어떻게 구하냐? -> approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a) = E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\( E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\\\)\n",
    "  - importance sampling 사용\n",
    "  - 위 식이 surrogate advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로,\n",
    "  - \\\\(maximize_\\theta ( E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\; s.t. \\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta) \\leq \\delta \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO를 공부하는데 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[가깝고도 먼 Trpo](https://www.slideshare.net/WoongwonLee/trpo-87165690)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
