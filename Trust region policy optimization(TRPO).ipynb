{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust region policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 높은 차원의 action space를 가진 환경부터 action은 적지만 control parameter가 많은 환경까지 범용적으로 사용할 수 있는 좋은 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제 정의, expected discounted reward를 최대화하는 policy를 찾자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP: \\\\(\\langle S, A, P, r, \\rho_0, \\gamma \\rangle\\\\)\n",
    "  - \\\\(P \\\\): state transition probability\n",
    "  - \\\\(\\rho_0\\\\): 초기 state \\\\(s_0\\\\)에 대한 distribution\n",
    "  - \\\\(r\\\\): reward function\n",
    "  - \\\\(\\gamma \\\\): discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-function으로 action을 선택한다.\n",
    "- ex) SARSA, Q-Learning, DQN, Dueling network, prioritized experience replay, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy를 가지고 있으며 policy는 parameter로 이루어져 있음.\n",
    "- REINFORCE, Actor-critic, A3C, TRPO, PPO, GAE, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective function(policy parameter \\\\(\\theta\\\\)의 함수)을 최대화하는 policy를 구하는 것\n",
    "- 각각의 iteration마다 objective function의 gradient를 따라 paramter를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample의 효율성이 없다.\n",
    "  - policy gradient는 현재 policy에 대한 estimate을 update하고 있는데, 한번 사용하면 버리게됨 따라서 효율성에서 떨어진다.\n",
    "- parameter space에서 거리와 policy space에서 거리가 다르다.\n",
    "  - parameter에서 update를 했을 때 policy가 받는 영향이 클수도 있고 작을 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 효율적으로 기대값을 추정하기 위해 만들어졌다.\n",
    "- 목적, 기대값을 계산하고자 하는 확률 분포 p(x)의 확률 밀도 함수를 알고 있지만 p에서 샘플을 생성하기 어려울 때 비교적 샘플을 생성하기 쉬운 q(x)에서 샘플을 생성하여 p의 기대값을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E_{x\\sim p}[f(x)] = \\int p(x)f(x)dx = \\int \\frac{p(x)}{q(x)}q(x)f(x)dx = E_{x \\sim q}\\begin{bmatrix} \\frac{p(x)}{q(x)}f(x) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식에서 \\\\(\\frac{p(x)}{q(x)}\\\\)를 likelihood ratio라 하며, \\\\(p\\\\)를 normal distribution, \\\\(q\\\\)를 importance distribution이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, importance sampling에서 \\\\(p\\\\)의 기대값 \\\\(E_{x \\sim p}[f(x)]\\\\)를 아래와 같이 추정한다.\n",
    "$$ E_{x \\sim p}[f(x)] \\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{p(x_n)}{q(x_n)}f(x_n),\\; x_n\\sim q $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy gradient, importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy gradient에서 이전 policy의 sample을 사용하기 위해서 importance sampling 사용할 수 있음.\n",
    "- importance weight(\\\\(\\frac{p(x_n)}{q(x_n)}\\\\))는 unbound(variance의 범위가 커서) -> 학습이 잘 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step in policy space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 문제로 언급되었던, parameter space는 policy space가 아니다.\n",
    "  - 그렇다면, parameter space가 아니라 policy space에서 조금씩 update가 가능한가?\n",
    "    - 두 policy(old policy, new policy)사이에 KL-divergence를 constraint로 (trust region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 얼마나 두 확률분포가 다른지를 설명해주는 지표\n",
    "- \\\\(D_KL(P||Q) = \\sum_i P(i)log \\frac{P(i)}{Q(i)} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주로 아직 확인되지 않은 모델을 특정 확률분포로 근사시키는데 사용한다.\n",
    "- KL-divergence를 가장 낮추는 정규분포를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q(근사 모델)를 최대한 P(실제 데이터 분포)에 가깝게 만들려면, KL-divergence를 최소화하도록 학습을 시킨다.\n",
    "- KL-divergence의 특징\n",
    "  - 항상 0이상의 값을 갖는다.\n",
    "  - 비대칭적 함수이다.\n",
    "    - \\\\(D_KL(p||q)\\\\)와 \\\\(D_KL(q||p)\\\\)는 다른값이다.\n",
    "  - 두 확률 분포가 동일하면 KL-divergence는 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence 분해하기\n",
    "  $$ \\begin{split} D_KL(P||Q) &= \\sum_i P(i) log \\frac{P(i)}{Q(i)} \\\\ &= \\sum_i P(i)logP(i) - \\sum_i P(i)logQ(i) \\\\ &= -H(P) + H(P,Q) \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence를 둘로 나눌 수 있기 때문에 크로스 엔트로피로 표현이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stable한 update는 안되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower bound를 최적화 하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO를 공부하는데 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[가깝고도 먼 Trpo](https://www.slideshare.net/WoongwonLee/trpo-87165690)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
