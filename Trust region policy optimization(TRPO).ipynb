{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust region policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 높은 차원의 action space를 가진 환경부터 action은 적지만 control parameter가 많은 환경까지 범용적으로 사용할 수 있는 좋은 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제 정의, expected discounted reward를 최대화하는 policy를 찾자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP: \\\\(\\langle S, A, P, r, \\rho_0, \\gamma \\rangle\\\\)\n",
    "  - \\\\(P \\\\): state transition probability\n",
    "  - \\\\(\\rho_0\\\\): 초기 state \\\\(s_0\\\\)에 대한 distribution\n",
    "  - \\\\(r\\\\): reward function\n",
    "  - \\\\(\\gamma \\\\): discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-function으로 action을 선택한다.\n",
    "- ex) SARSA, Q-Learning, DQN, Dueling network, prioritized experience replay, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy based 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy를 가지고 있으며 policy는 parameter로 이루어져 있음.\n",
    "- REINFORCE, Actor-critic, A3C, TRPO, PPO, GAE, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective function(policy parameter \\\\(\\theta\\\\)의 함수)을 최대화하는 policy를 구하는 것\n",
    "- 각각의 iteration마다 objective function의 gradient를 따라 paramter를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample의 효율성이 없다.\n",
    "  - policy gradient는 현재 policy에 대한 estimate을 update하고 있는데, 한번 사용하면 버리게됨 따라서 효율성에서 떨어진다.\n",
    "- parameter space에서 거리와 policy space에서 거리가 다르다.\n",
    "  - parameter에서 update를 했을 때 policy가 받는 영향이 클수도 있고 작을 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 효율적으로 기대값을 추정하기 위해 만들어졌다.\n",
    "- 목적, 기대값을 계산하고자 하는 확률 분포 p(x)의 확률 밀도 함수를 알고 있지만 p에서 샘플을 생성하기 어려울 때 비교적 샘플을 생성하기 쉬운 q(x)에서 샘플을 생성하여 p의 기대값을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E_{x\\sim p}[f(x)] = \\int p(x)f(x)dx = \\int \\frac{p(x)}{q(x)}q(x)f(x)dx = E_{x \\sim q}\\begin{bmatrix} \\frac{p(x)}{q(x)}f(x) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식에서 \\\\(\\frac{p(x)}{q(x)}\\\\)를 likelihood ratio라 하며, \\\\(p\\\\)를 normal distribution, \\\\(q\\\\)를 importance distribution이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, importance sampling에서 \\\\(p\\\\)의 기대값 \\\\(E_{x \\sim p}[f(x)]\\\\)를 아래와 같이 추정한다.\n",
    "$$ E_{x \\sim p}[f(x)] \\approx \\frac{1}{N} \\sum_{n=1}^N \\frac{p(x_n)}{q(x_n)}f(x_n),\\; x_n\\sim q $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy gradient, importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy gradient에서 이전 policy의 sample을 사용하기 위해서 importance sampling 사용할 수 있음.\n",
    "- importance weight(\\\\(\\frac{p(x_n)}{q(x_n)}\\\\))는 unbound(variance의 범위가 커서) -> 학습이 잘 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step in policy space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 문제로 언급되었던, parameter space는 policy space가 아니다.\n",
    "  - 그렇다면, parameter space가 아니라 policy space에서 조금씩 update가 가능한가?\n",
    "    - 두 policy(old policy, new policy)사이에 KL-divergence를 constraint로 (trust region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 얼마나 두 확률분포가 다른지를 설명해주는 지표\n",
    "- \\\\(D_KL(P||Q) = \\sum_i P(i)log \\frac{P(i)}{Q(i)} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주로 아직 확인되지 않은 모델을 특정 확률분포로 근사시키는데 사용한다.\n",
    "- KL-divergence를 가장 낮추는 정규분포를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q(근사 모델)를 최대한 P(실제 데이터 분포)에 가깝게 만들려면, KL-divergence를 최소화하도록 학습을 시킨다.\n",
    "- KL-divergence의 특징\n",
    "  - 항상 0이상의 값을 갖는다.\n",
    "  - 비대칭적 함수이다.\n",
    "    - \\\\(D_KL(p||q)\\\\)와 \\\\(D_KL(q||p)\\\\)는 다른값이다.\n",
    "  - 두 확률 분포가 동일하면 KL-divergence는 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence 분해하기\n",
    "  $$ \\begin{split} D_KL(P||Q) &= \\sum_i P(i) log \\frac{P(i)}{Q(i)} \\\\ &= \\sum_i P(i)logP(i) - \\sum_i P(i)logQ(i) \\\\ &= -H(P) + H(P,Q) \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence를 둘로 나눌 수 있기 때문에 크로스 엔트로피로 표현이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stable한 update는 안되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower bound를 최적화 하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy old와 policy new의 관계를 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy \\\\(\\pi_{old}\\\\)의 objective func, \\\\(J(\\pi_{old}) = E_{s_0, a_0, ... \\sim \\pi_{old}}[\\sum^\\infty_{t=0} \\gamma^t r(s_t)] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy \\\\(\\pi\\\\) objective func, \\\\(J(\\pi) = E_{s_0, a_0, ... \\sim \\pi}[\\sum^\\infty_{t=0} \\gamma^t r(s_t)] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi) = J(\\pi_{old}) + E_{s_0, a_0, ... \\sim \\pi} \\begin{bmatrix} \\sum^\\infty_{t=0} \\gamma A_{\\pi_{old}}(s_t, a_t) \\end{bmatrix} \\\\)\n",
    "<br><br>\n",
    "- \\\\(J(\\pi) = J(\\pi_{old}) + \\sum_s \\rho_\\pi(s) \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy Iteration & objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy improvement -> greedy policy improvement\n",
    "  - 어떤 행동 상태가 positive advantage를 가지면 policy를 improve한다.\n",
    "  - \\\\( \\pi = argmax_a A_{\\pi_{old}}(s,a) \\\\)\n",
    "- approximation error -> \\\\(A_{\\pi_{old}}(s,a) < 0 \\\\)가능, \\\\( \\rho_\\pi(s) \\\\)구하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi)\\\\) -> local approximation\n",
    "$$ L_{\\pi_{old}}(\\pi) = J(\\pi_{old}) + \\sum_s \\rho_{\\pi_{old}}(s) \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi) = J(\\pi_{old}) + \\sum_s {\\color{Red}{\\rho_\\pi(s)}} \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a)\\\\)\n",
    "<br><br>\n",
    "- 위 식에서 아래 식으로 빨간색 부분이 바뀌었는데, 이것은 policy의 변화가 적다면 steady state distribution의 변화도 무시할 수 있을 것이다.\n",
    "- \\\\( L_{\\pi_{old}}(\\pi) = J(\\pi_{old}) + \\sum_s {\\color{Red} {\\rho_{\\pi_{old}}(s)}} \\sum_a \\pi(a|s) A_{\\pi_{old}}(s,a) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, Approximation error는 얼마나 되냐? -> \\\\(\\color{Red}{\\text{Lower bound}}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local approximation with parameterized policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식은,\n",
    "- \\\\(L_{\\pi_{\\theta_0}}(\\pi_{\\theta_0}) = J(\\pi_{\\theta_0})\\\\)으로 표현 가능하고,\n",
    "- Kakade & Langford의 증명, 따라서 \\\\(\\pi_{\\theta_0}\\\\)에서 policy를 조금만 변화시켰을 때 local approximation을 improve -> objective function도 improve함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 local approximation을 improve하는 것이 objective function도 improve하는 것을 증명 했지만, 어느 정도 변해야 objective function의 improve를 보장하는지 알 수 없다.\n",
    "- 따라서, explicit한 J의 lower bound를 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. policy improvement(mixture policy 가정)\n",
    "  - 현재 policy: \\\\(\\pi_{old}\\\\), 새로운 policy: \\\\(\\pi_{new}\\\\)\n",
    "  - \\\\(\\pi' = argmax_{\\pi'}L_{\\pi_{old}}(\\pi')\\\\)\n",
    "  - \\\\(\\pi_{new}(a|s) = (1-\\alpha)\\pi_{old}(a|s) + \\alpha \\pi'(a|s)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lower bound of J\n",
    "  - \\\\(J(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - \\frac{2\\epsilon \\gamma}{(1-\\gamma)^2} \\alpha^2 \\\\)\n",
    "  - \\\\(where \\epsilon = max_s|E_{a \\sim \\pi'(a|s)}[A_\\pi(s,a)]| \\\\)\n",
    "  - 따라서 이제는 lower bound를 최적화하면 된다.\n",
    "  - parameterized policy에 대해서 mixture policy를 구할 수 없기 때문."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative policy Iteration의 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mixture policy를 통해 old policy와 new policy사이의 거리를 나타내지 않고, KL-divergence를 사용한다.\n",
    "  - \\\\( \\alpha^2 -> D^{max}_{KL}(\\pi_{old}, \\pi_{new}) \\\\)\n",
    "- 변형된 lower bound 식 사용\n",
    "  - \\\\( \\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - CD^{max}_{KL}(\\pi_{old}, \\pi_{new})\\\\)\n",
    "  - \\\\( where C = \\frac{4 \\epsilon \\gamma}{(1 - \\gamma)^2}, \\epsilon=max_s|E_{a \\sim \\pi'(a|s)}[A_\\pi(s,a)]| \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower bound of objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(J(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - CD^{max}_{KL}(\\pi_{old}, \\pi_{new}) \\\\)\n",
    "- 현재 \\\\(\\theta\\\\)에서는 값이 모두 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49554137-3bb7f400-f93e-11e8-891a-6cb11a27d9c1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-constraint optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameterized policy를 lower bound식에 적용한다.\n",
    "  - \\\\(J(\\theta) \\geq L_{\\theta_{old}}(\\theta) - CD^{max}_{KL}(\\theta_{old}, \\theta) \\\\)\n",
    "- Lower bound의 optimization: C가 너무 커서 step이 작다.\n",
    "  - \\\\(maximize_{\\theta}[L_{\\theta_{old}}(\\theta) - CD^{max}_{KL}(\\theta_{old}, \\theta)] \\\\)\n",
    "- Large and robust step: KL-penalty -> KL-constraint\n",
    "  - \\\\(maximize_\\theta L_{\\theta_{old}}(\\theta) \\\\)\n",
    "  - \\\\(s. t. D^{max}_{KL}(\\theta_{old}, \\theta) \\leq \\delta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보의 양"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 KL-constraint를 정확하게 알기 위해서 entropy의 개념과 KL-divergence의 개념을 다시 공부해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤하게 나타나는 숫자가 x이고 그 숫자 x가 나타날 확률이 \\\\(p(x)\\\\)라 하면 확률이 낮은 정보일수록 희소성이 있다.\n",
    "- 정보의 양, \\\\(h(x) = -logp(x)\\\\)\n",
    "  - 로그 함수의 성질에 따라 \\\\(p(x)\\\\)가 0에 가까워지면 무한대로 가고,\n",
    "  - 1일때는 0이다. 따라서 \\\\(p(1)\\\\)은 항상 일어나는 일이라는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- entropy\n",
    "  - 여러 사건이 발생할 경우 entropy는 \\\\(h(x)\\\\)의 weighted sum으로 정의한다.\n",
    "  - \\\\(H[x] = -\\sum^N_{i=1} p_i logp_i\\\\)\n",
    "  - 위 식은 사건이 많아도 쉽게 계산이 되는 특징이 있고, \\\\(p(x)\\\\)들의 합은 무조건 1이 되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-constraint optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(maximize_\\theta L_{\\theta_{old}}(\\theta) \\\\)<br><br>\n",
    "- \\\\(s.t. D^{max}_{KL}(\\theta_{old}, \\theta) \\leq \\delta\\\\)\n",
    "  - policy space에서는 작은 step이다.(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/49581154-5cab3400-f994-11e8-9a35-1a53ca938fdb.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서 \\\\(\\eta(\\theta)\\\\)는 최대화하고 싶은 expected discounted rewards이다.\n",
    "- \\\\(L(\\theta)-C \\cdot KL\\\\)은 lower bound function이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(L_{\\theta_{old}}(\\theta) = J(\\theta_{old}) + \\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_{\\theta}(a|s) A_{\\pi_{\\theta_{old}}}(s, a)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(maximize_{\\theta} L_{\\theta_{old}}(\\theta) \\rightarrow maximize_{\\theta} \\begin{bmatrix} \\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a) \\end{bmatrix} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, \\\\(\\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a)\\\\) 이부분을 어떻게 구하냐? -> approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\sum_s \\rho_{\\pi_{\\theta_{old}}}(s) \\sum_a \\pi_\\theta(a|s) A_{\\pi_{\\theta_{old}}}(s, a) = E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\( E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\\\)\n",
    "  - importance sampling 사용\n",
    "  - 위 식이 surrogate advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로,\n",
    "  - \\\\(maximize_\\theta ( E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\; s.t. \\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta) \\leq \\delta \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural policy gradient를 통해서 하고싶은 것은 steepest direction을 찾고 그 방향대로 policy를 update하고 싶다.\n",
    "  - steepest direction, 현재 parameter space상의 point로 부터 일정 거리 만큼 움직였을 때 가장 작은 함수 값을 가지는 지점으로 가는 방향을 의미한다.\n",
    "  - 일정 거리, parameter space위에 정의하기 위해서 좌표 변화에 invariant한 metric을 사용해서 나타내었다.\n",
    "  - \\\\(|d\\theta|^2 = \\sum_{ij} (\\theta) d\\theta_i d\\theta_i = d\\theta^T G(\\theta) d\\theta\\\\)\n",
    "- optimize하고자 하는 함수에 대해서는 1차 미분으로 local approximation을 취하고,\n",
    "- 일정 거리를 표현하기 위해서 distribution의 변화를 의미하는 KL-divergence를 가져온다. KL-divergence는 2차 미분의 결과로 Hessian 항만 남는다.\n",
    "- \\\\(\\mathcal{L}_{\\theta_k}(\\theta) \\approx \\mathcal{L}_{\\theta_k}(\\theta_k) + g^T(\\theta-\\theta_k) \\qquad g \\doteq \\triangledown_\\theta \\mathcal{L}_{\\theta_k}(\\theta) |_{\\theta_k} \\\\)\n",
    "- \\\\(\\bar{D_{KL}}(\\theta||\\theta_k) \\approx \\frac{1}{2}(\\theta-\\theta_k)^T H(\\theta-\\theta_k) \\qquad H \\doteq \\triangledown^2_\\theta \\bar{D_{KL}}(\\theta||\\theta_k) |_{\\theta_k} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 문제는 KL-divergence constraint가 있는 optimization문제이지만 lagrange multiplier를 사용해서 penalty 문제로 전환가능하다.\n",
    "- \\\\(\\theta_{k+1}= arg \\max_\\theta g^T(\\theta-\\theta_k) \\qquad s.t. \\frac{1}{2} (\\theta-\\theta_k)^T H(\\theta-\\theta_k) \\leq \\delta \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update rule\n",
    "  - \\\\(\\theta_{k + 1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^T H^-1 g}} H^-1 g \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher information matrix [Fisher information](https://ko.wikipedia.org/wiki/%ED%94%BC%EC%85%94_%EC%A0%95%EB%B3%B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 확률변수의 관측값으로부터, 확률변수의 분포의 매개변수에 대해 유추할 수 있는 정보의 양.\n",
    "- 확률 변수 X가 매개변수 \\\\(\\theta\\\\)로 주어지는 분포를 따른다고 할 때 관측값 \\\\(X = x\\\\)로 부터 주어지는 \\\\(\\theta\\\\)에 대한 fisher 정보는 아래와 같다.\n",
    " - \\\\(\\mathcal{I}_x(\\theta) = E \\begin{bmatrix} \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta} ln Pr(x|\\theta) \\end{pmatrix}^2 \\end{bmatrix}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian Matrix [Hessian Matrix](http://norman3.github.io/prml/docs/chapter05/4.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backprop 기법을 이용해서 에러 함수에 대한 w 1차 미분값을 계산할 수 있었다.\n",
    "- 2차 미분값 Hessian 행렬 계산 방법\n",
    "- \\\\(\\frac{\\partial^2 E}{\\partial w_{ji} \\partial w_{lk}}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\( i ,j \\in \\{1, ..., W\\} \\\\) 이고 W는 모든 weight와 bias를 포함한다.\n",
    "- 이때 각각의 2차 미분 값을 \\\\(H_ij\\\\)로 표기하고 이것으로 만들어지는 행렬을 Hessian 행렬 H로 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\( L_{\\theta_{old}}(\\theta) = E_{s \\sim \\pi_{\\theta_{old}}, a \\sim \\pi_\\theta} \\begin{bmatrix} \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\pi_{\\theta_{old}}}(s,a) \\end{bmatrix} \\\\)\n",
    "  - 위 식을 풀기 위해서\n",
    "  - \\\\(maximize_\\theta L_{\\theta_{old}}(\\theta) \\qquad s.t. \\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta) \\leq \\delta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차 미분 approximation to surrogate advantage function\n",
    "  - \\\\( L_{\\theta_{old}}(\\theta) \\approx L_{\\theta_{old}}(\\theta_{old}) + g^T(\\theta - \\theta_{old}), \\qquad g = \\triangledown_\\theta L_{\\theta_{old}}(\\theta)|_{\\theta_{old}} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2차 미분 approximation to KL-divergence\n",
    "  - \\\\(\\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_{old})^T H(\\theta - \\theta_{old}), \\qquad H = \\triangledown^2_\\theta \\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta)|_{\\theta_{old}} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- constraint problem이 바뀜.\n",
    "  - \\\\(maximize_\\theta g^T(\\theta - \\theta_{old}) \\qquad s.t. \\frac{1}{2}(\\theta-\\theta_{old})^T H(\\theta - \\theta_{old}) \\leq \\delta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langrange Multiplier method\n",
    "  - 위 식의 해는 \\\\(g^T(\\theta - \\theta_{old}) - \\beta \\frac{1}{2}(\\theta - \\theta_{old})^T H(\\theta - \\theta_{old})\\\\)의 미분이 0이 되는 지점을 찾는 것이다.\n",
    "  - \\\\(\\beta = 1\\\\)일 때, \\\\(s = \\theta-\\theta_{old}\\\\) distance s를 먼저 구한다.\n",
    "    - \\\\(g - Hs = 0\\\\)\n",
    "    - \\\\(s = H^-1 g\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL-divergence constraint를 통해서 step-size를 구한다\n",
    "  - Lagrange multiplier를 통해 구한 direction은 boundary에서의 direction이고\n",
    "    - \\\\(\\theta - \\theta_{old} = \\alpha s \\\\)\n",
    "    - \\\\( \\frac{1}{2}(\\theta - \\theta_{old})^T H(\\theta - \\theta_{old}) = \\delta \\\\)\n",
    "    - \\\\( \\rightarrow \\frac{1}{2}(\\alpha s)^T H(\\alpha s) = \\delta \\\\)\n",
    "    - \\\\( \\rightarrow \\alpha = \\sqrt{ \\frac{2 \\delta}{s^T H s}} = \\sqrt{ \\frac{2 \\delta}{g^T H^-1 g}} (s = H^-1 g) \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제\n",
    "  - \\\\(maximize_\\theta g^T(\\theta - \\theta_{old}) \\qquad s.t. \\frac{1}{2}(\\theta-\\theta_{old})^T H(\\theta - \\theta_{old}) \\leq \\delta\\\\)\n",
    "- 해결\n",
    "  - \\\\(\\theta_{k + 1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^T H^-1 g}} H^-1 g \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated natural policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NPG에서 Neural network와 같이 parameter가 많은 경우 \\\\( H^-1 g \\\\)계산이 어렵다.\n",
    "  - parameter 갯수가 N개 이면, H의 크기는 \\\\(N^2\\\\), \\\\(H^-1\\\\)계산은 \\\\(O(N^3)\\\\)이 된다.\n",
    "- 따라서, Conjugate gradient method를 통해서 \\\\(H^-1\\\\)을 계산하지 않고 \\\\(H^-1 g\\\\)를 구해야한다.\n",
    "  -> Truncated natural policy gradient\n",
    "- Conjugate gradient method는 Ax = b의 선형 시스템 문제를 푸는 방법이다.\n",
    "  - Analytic하게 구하지 않고 iterative하게 x를 구하는 방법\n",
    "  - 임의의 벡터 v에 대해 hessian-vector product Hv로 시작해서 Hv=g가 되도록 v를 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Natural policy gradient의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trust region size가 robust하지 않을 수 있다. 왜냐하면, 몇몇 iteration이 너무 크거거나 performance의 degrade를 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust region policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 문제를 sub-problem으로 나누고 각 sub-problem을 두 step으로 푼다.\n",
    "  - finding search direction\n",
    "  - do line search on that direction inside trust region\n",
    "- Trust region policy optimization\n",
    "  - Search direction, \\\\( \\vartriangle = \\sqrt{ \\frac{2 \\delta}{g^T H^-1 g}}H^-1 g \\\\)\n",
    "  - backtracking line search, \\\\(\\theta_{new} = \\theta_{old} + \\alpha^j \\vartriangle \\qquad (L_{\\theta_{old}}(\\theta) > 0 \\; and \\; \\overline{D^{\\rho_{old}}_{KL}}(\\theta_{old}, \\theta) \\leq \\delta \\\\) 일 때, stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO를 공부하는데 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[가깝고도 먼 Trpo](https://www.slideshare.net/WoongwonLee/trpo-87165690)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
