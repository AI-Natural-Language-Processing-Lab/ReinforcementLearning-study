{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft actor critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최근 발표된 off-policy model-free 알고리즘이다.\n",
    "- 로봇 실험에 쓰이지만, 일단 SuperMario 환경에 적용해 보려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잘 만들어진 환경이 아닌 실제 환경에서 실험을 하는 것은 매우 힘들다.\n",
    "  - data stream의 interruptions\n",
    "  - requirement for a low-latency(input과 output사이 과정에서 지연을 최소화 하는 것) inference\n",
    "  - 기계적 마모 현상을 피하기 위해서 smooth exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요구 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample Efficiency\n",
    "  - 실제 환경에서 학습을 진행하는 것은 매우 오랜시간이 걸린다. 따라서 좋은 sample complexity는 성공적인 학습을 위한 첫 번째 전제조건이다.\n",
    "- No Sensitive Hyperparameters\n",
    "  - 실제 환경에서 학습 시간이 오래 걸리는데 hyperparameter를 튜닝할 때 마다 학습을 다시 해야하기 때문에 많은 시간이 걸린다. 따라서 Maximum entropy reinforcement learning은 robust한 framework를 제공한다.\n",
    "- Off-Policy Learning\n",
    "  - 다른 task에서 진행한 data를 가져와서 학습할 수 있고, 이미 collected data도 학습에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asynchronous Sampling\n",
    "  - 환경과 상호작용하면서 training도 유지되는 것을 원한다. 따라서 data sampling과 training이 독립적인 threads로 돌아가야한다.\n",
    "- Stop / Resume Training\n",
    "  - 실제 환경에서 training을 하다보면 잘못되는 경우가 많다. 따라서 constant interruptions in the data stream이 필요하다.\n",
    "- Action smoothing\n",
    "  - Gaussian exploration은 actuators가 높은 빈도로 jitter(불안정한 신호들의 차이)하게 만듦. 이것은 hardware에 손상을 줄 수 있고, 그래서 correlating the exploration이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 질문, correlating the exploration이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- soft actor critic은 maximum entropy reinforcement learning에 기반이 됨.\n",
    "- \\\\( J(\\pi) = \\mathbb{E}_\\pi \\Biggl[ \\sum_{t} r(s_t, a_t) - \\alpha log(\\pi(a_t|s_t)) \\Biggr]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(s_t\\\\)와 \\\\(a_t\\\\)는 state와 action이다.\n",
    "- optimal policy는 expected return을 최대화 하고, expected entropy를 최대화한다.\n",
    "- 앞서 언급한 두 값들을 컨트롤하기 위해서 non-negative \\\\(\\alpha\\\\)가 있다.\n",
    "  - expected return objective를 최대화 하기 위해서는 \\\\(\\alpha = 0\\\\)을 하면 된다.\n",
    "  - 자동적으로 \\\\(\\alpha\\\\)값은 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- entropy를 uniform하게 생각하고\n",
    "- exploration(maximize entropy) and exploitation(maximize return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- soft actor critic은 parameterizing된 objective를 최대화 한다.(Gaussian policy)\n",
    "- Q-function\n",
    "- 위 두 네트워크를 dynamic programming을 통해서 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과적으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일관적인 학습과 효과적인 sample사용\n",
    "- performance under the conventional, maximum expected return objective(without entropy regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maximum entropy reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- expert sample을 이용해서 return function을 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적인 Inverse Reinforcement Learning(IRL)은 환경과 optimal 행동이 주어질 때 reward function을 찾는 것이다.\n",
    "- reward function에 대한 세 가지 알고리즘\n",
    "  - 유한한 state space에서 tabulated reward function을 찾음.\n",
    "  - potentially infinite state space에 대한 reward function의 linear functional approximation을 다룸.\n",
    "  - 관측된 trajectories의 realistic case에 대한 알고리즘임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(D = {(s_0, a_0), (s_1, a_1) , ... } \\\\) sequence가 주어질 때\n",
    "- inverse RL을 통해서 reward를 찾고,\n",
    "- policy를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- behavior cloning은 결국 그냥 따라하는 것이지만, inverse RL은 reward를 찾고 reward로 policy를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imitation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강화학습은 sparse한 rewards를 가지고 있고,\n",
    "- 학습을 오래해야 결과가 나온다.\n",
    "- 또한, 로봇틱스의 경우 한번 실패하면 하드웨어 손실이 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복잡한 행동도 쉽게 배울수 있고,\n",
    "- 위 문제를 해결하기 위해서 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BAIR reference](https://bair.berkeley.edu/blog/2018/12/14/sac/)<br>\n",
    "[Soft actor critic paper](https://arxiv.org/pdf/1801.01290.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
