{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft actor critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최근 발표된 off-policy model-free 알고리즘이다.\n",
    "- 로봇 실험에 쓰이지만, 일단 SuperMario 환경에 적용해 보려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잘 만들어진 환경이 아닌 실제 환경에서 실험을 하는 것은 매우 힘들다.\n",
    "  - data stream의 interruptions\n",
    "  - requirement for a low-latency(input과 output사이 과정에서 지연을 최소화 하는 것) inference\n",
    "  - 기계적 마모 현상을 피하기 위해서 smooth exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요구 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample Efficiency\n",
    "  - 실제 환경에서 학습을 진행하는 것은 매우 오랜시간이 걸린다. 따라서 좋은 sample complexity는 성공적인 학습을 위한 첫 번째 전제조건이다.\n",
    "- No Sensitive Hyperparameters\n",
    "  - 실제 환경에서 학습 시간이 오래 걸리는데 hyperparameter를 튜닝할 때 마다 학습을 다시 해야하기 때문에 많은 시간이 걸린다. 따라서 Maximum entropy reinforcement learning은 robust한 framework를 제공한다.\n",
    "- Off-Policy Learning\n",
    "  - 다른 task에서 진행한 data를 가져와서 학습할 수 있고, 이미 collected data도 학습에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
