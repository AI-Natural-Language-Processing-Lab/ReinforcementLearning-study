{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimization 문제\n",
    "  - 어떤 목적함수(objective function)의 함수값을 최적화(최대, 최소)시키는 parameter를 찾는 문제이다.\n",
    "  - 만약, 목적함수 변수 개수에 따라\n",
    "    - 하나, 일변수 함수\n",
    "    - 여러개, 다변수 함수\n",
    "  - 목적함수의 차수\n",
    "    - 일차 이하, 선형 optimization 문제\n",
    "    - 그 외, 비선형 optimization 문제\n",
    "  - 목적함수 외 parameter가 만족해야할 별도의 제약조건이 있나?\n",
    "    - 있음, constrained optimization\n",
    "    - 없음, unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 위치에서 함수값이 감소 or 증가하는 방향으로 조금씩 parameter 값을 이동하는 것이다.\n",
    "- 가장 중요한 것은 어느 방향으로 얼마나 내려갈 것인지 결정하는 것이다.\n",
    "- 이동할 방향과 이동할 양을 결정할 때 사용되는 가장 기본적인 수학적 원리는 일차 미분(기울기)와 이차미분(곡률) 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일차미분을 이용한 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(x_{k+1} = x_k - \\lambda f'(x_k)\\\\)\n",
    "- 일차 미분을 이용한 가장 단순한 형태는 위 수식과 같이 임의의 시작점에서 수렴할 때 까지 x를 변화시키는 것이다.\n",
    "- Gradient Descent 방법이다.\n",
    "- 하지만 위 방법은 \\\\(\\lambda\\\\)값에 의해서 발산 할 수도 있고 너무 작으면 학습속도가 많이 느리다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이차미분을 이용한 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일차 미분을 이용한 optimization문제를 해결하기 위해서 일차미분과 이차미분을 함께 이용한다.\n",
    "- \\\\(x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞선 일차미분과 다르게 굉장히 빠른 수렴속도를 보이고, step size도 필요없는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 이차미분 optimization에도 단점이 있다.\n",
    "  - 변곡점 근처에서 매우 불안정한 이동 특성을 보인다. 3차 함수에서 변곡점에서 이차 미분을 하면 0이 되므로 정의가 안되는 문제가 발생한다.\n",
    "  - 또한, 변곡점이 아니더라도 변곡점 근처에서는 이차미분이 0에 가까운 값을 갖기 때문에 step size가 너무 커져서 발산할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론적으로,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일차미분을 사용하면 항상 옳은 방향으로 가지만 step size를 정하는 문제가 있고,\n",
    "- 이차미분을 사용하면 빠르게 수렴하지만 변곡점 근처에 대한 문제가 있고 극대, 극소를 구분하지 못한다는 문제가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이차미분 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 지점에서 구한 \\\\(f''\\\\)값을 다른 모든 구간으로 확대 적용하는 것에 있다.\n",
    "- 예를 들어) \\\\(f(x) = x^4\\\\)의 예시에서 \\\\(f''(2) = 48\\\\)이다. 그런데, \\\\(x = 2\\\\)뿐만 아니라 다른 모든 x에서도 \\\\(f''(x) = 48\\\\)이라 하면 이러한 f는 \\\\(f(x) = 24x^2 + bx + c\\\\)꼴의 이차함수가 된다. 그리고 \\\\(f(2)\\\\)와 \\\\(f'(2)\\\\)도 현재 지점과 같게 일치를 시키면 \\\\(f(x) = 24^2 - 64x + 48\\\\)이 된다.\n",
    "- 위 예시와 같이 현재 지점에서의 함수 변화를 이차 포물선으로 근사시킨 후 원래의 함수가 이 근사 포물선과 같을 것이라고 가정해버리면 함수의 값을 최소화시키는 지점은 포물선의 극점일 것이므로 구해진 포물선의 극점으로 x를 이동시키면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- step 크기를 결정하는 방법에 관한 것으로 앞서 설명한 최적화 기법들의 문제점을 효과적으로 극복가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 일차미분의 문제는 항상 옳은 방향으로 향하고 있지만 step size를 정하는 것이 문제이다.\n",
    "- 또한, 이차 미분을 이용한 경우에도 변곡점 근처에서는 탐색이 불안정해지는 문제가 있다.\n",
    "- 이것의 근본적인 원인은, step size를 현재 지점에서의 국소적인 변화만을 보고 정하는 것에 있다.\n",
    "  - 그래서 line search방법은 이동하고자 하는 방향을 따라서 실제 함수값(f)의 변화를 미리 살펴본 후에 이동할 양을 정하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두가지 방법이 존재한다.\n",
    "  - backtracking line search\n",
    "    - 이동하고자 하는 방향을 따라서 최대한 멀리 가보고 해당 지점의 함수값이 현재의 함수값에 비해서 충분히 작아졌는지 검사한 후 작지 않다면 점차적으로 거리를 줄여가면서 다시 함수 값을 비교한다. 충분히 작다면 해당 지점으로 점프한다. 그리고 그 점프한 지점에서 line search를 다시 한다.\n",
    "  - golden section search\n",
    "    - 탐색구간의 내부를 황금 비율인 1:1.618로 분할해 가면서 구간 내에서 함수값이 최소가 되는 지점을 탐색한후 해당 지점으로 점프한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과적으로,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일차미분을 이용한 최적화 기법은 항상 옳은 방향을 제안하기 떄문에 line search 방법을 일차미분 최적화 방법과 결합하면 **이동 방향은 일차미분으로 판단하고, 이동 크기는 line search로 결정한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- line search 방법이 주로 일차미분을 이용한 최적화 기법의 단점을 극복한다면 trust region은 이차미분을 이용한 최적화 기법의 단점을 극복한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이차미분을 이용한 최적화 기법의 핵심 원리는 모든 구간에서 함수의 이차미분 \\\\(f''\\\\)값이 상수라고 가정하고 함수의 변화를 이차함수로 근사한 후 근사 함수의 극점으로 이동하는데,\n",
    "- **이것의 가장 큰 문제는 실제 함수는 이차함수가 아니라는 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, Trust region 방법은 근사 함수에 대한 신뢰 영역을 정의한 후 이동할 목적지에 대한 탐색 범위를 이 영역 내부로 제한하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과적으로,\n",
    "  - 이차 근사한 함수의 극점을 찾았는데 실제 f(x)는 이차함수가 아니기 때문에 그곳이 극점이 아닐수 있다. 따라서 trust region 방법을 써서 region을 정해두고 region 밖으로 나가게 되면 step size를 줄여서 trust region을 벗어나지 않도록 위치를 수정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damping & Saddle-free 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- damping 방법은 앞서 line search 방법, trust region방법 처럼 수렴의 안정성을 보장하기 위한 방법이다.\n",
    "- 원래 damping 방법은 nonlinear least squares problem에 사용되는데 핵심 원리는 일반적인 최적화 문제에도 동일하게 적용 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(x_{k+1} = x_k - \\frac{f'(x_k)}{\\left\\vert f''(x_k) \\right\\vert + \\mu_k}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 \\\\(\\mu_k\\\\)가 damping factor이며 영어로는 '진폭을 억제하는'이라는 의미를 갖는다. 즉, damping 방법은 변곡점 근처에서 발산하려고 하는 원래의 시스템에 억제를 가하여 모든 경우에 부드럽게 동작하도록 만든 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또한, \\\\(\\mu_k\\\\)가 큰 경우에는 상대적으로 \\\\(f''\\\\)의 영향이 작아지기 때문에 일차미분을 이용한 최적화 기법과 유사해지고, 반대로 \\\\(\\mu_k\\\\)가 작은 경우에는 이차미분을 이용한 최적화 기법과 유사한 특성을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또한, \\\\(\\left\\vert f''(x_k) \\right\\vert\\\\) 절대값이 붙게 되면서 일차미분 부호를 이용해서 극대, 극소를 구분할 수 있게된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참조]http://darkpgmr.tistory.com/149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
