{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- markov decision process는 markov reward process에 decision을 추가한 개념이다.\n",
    "\n",
    "- 모든 state는 markov 환경에서 이루어진다.\n",
    "\n",
    "- 현재 상태 s에서 action a를 취했을 때 다음 상태 s'로 가게 될 확률 \\\\(\\mathcal{P}\\\\)\n",
    "$$ \\mathcal{P}^a_{ss'} = \\mathbb{P}[S_{t+1} = s'|S_t=s,A_t=a] $$\n",
    "\n",
    "- reward 함수도 현재 상태 s에서 action a를 취했을 때 expectation reward\n",
    "$$ \\mathcal{R}^a_s = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy는 agent가 행동하는데 중요한 역할을 하게 된다. 현재 state에 대해서 어떤 action을 취할 확률을 나타낸다.\n",
    "$$ \\pi(a|s) = \\mathbb{P}[A_t = a | S_t = s] $$\n",
    "\n",
    "- markov decision process policy에서는 현재 state만 고려하고 과러는 고려하지 않고 action한다. 이때 확률적으로 action을 결정하게 되고 policy는 time-step에 stationary하다.(time-independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given markov decision process \\\\(\\mathcal{M} = \\langle \\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma \\rangle\\\\) and a policy \\\\(\\pi\\\\)<br>\n",
    "    하나의 MDP가 주어졌을 때, 하나의 policy가 존재한다.\n",
    "\n",
    "- policy 개념을 markov process에 적용하면 \\\\(\\mathcal{P}^{\\pi}\\\\)가 되고, 이것을 policy \\\\(\\pi\\\\)를 따르는 \\\\(\\mathcal{P}\\\\)라 읽는다. 왜냐하면, policy를 벗어나는 state transition은 없기 때문이다.\n",
    "\n",
    "- The state and reward requence \\\\(S_1, R_2, S_2, ... \\\\) is a markov reward process \\\\(\\langle \\mathcal{S},\\mathcal{P}^{\\pi},\\mathcal{R}^{\\pi},\\gamma \\rangle \\\\)\n",
    "$$ \\mathcal{P}^{\\pi}_{s,s'} = \\sum_{a\\in A} \\pi(a|s) \\mathcal{P}^a_{ss'} $$\n",
    "$$ \\mathcal{R}^{\\pi}_s = \\sum_{a \\in A} \\pi(a|s) \\mathcal{R}^a_s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s에서 policy를 따르는 value가 되며 이것은 s에서 policy를 따를 때 expectation reward의 모든 합이 된다. 이것을 state-value function, v라 함.\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\;|\\; S_t=s] $$\n",
    "\n",
    "- 위 식에 action을 추가하게 되면 action-value function, q가 된다. 이것은 state s이면서 action  a인 조건에서 policy를 따르는 expectation reward 모든 합이 된다.<br>\n",
    "    즉, policy를 따라 action을 했을 때 얼마나 좋은지를 나타내는 값이 된다.\n",
    "$$ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\;|\\;S_t =s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v function, q function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function은 bellman equation을 통해서 분리 가능하다.\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\;|\\;S_t=s] $$\n",
    "\n",
    "- action-value function bellman equation 분리\n",
    "$$ q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}\\;|\\;S_t=s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 식으로 나타내면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function, 예를 들어) 현재 상태 s에서 policy를 따르는 v는 두 가지의 action 중에 하나에 대한 action a 를 취했을 때 policy를 따르는 q가 있고, 나머지 다른 action a'를 취했을 때 q'이 있다. 이때 q, q'을 합하면 v를 구성하게 된다.\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) $$\n",
    "\n",
    "- action-value function, 예를 들어) 현재 상태 s에서 action a를 취할 때 policy를 따르는 q는 reward r과 다음 state인 s'으로 이동하게 된다. 그 다음 state s'에서의 policy를 따르는 v가 있다. 하지만 위에서 s'으로 갔지만 다른 상태로 갈수도 있다. 따라서 위의 가능성을 모두 합하면 된다.\n",
    "$$ q_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}^a_{ss'} v_{\\pi}(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic1](https://user-images.githubusercontent.com/22078438/48111664-e41e5e00-e295-11e8-9531-495fa92366dd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식을 이용해서 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman expectation equation 간소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Bellman expectation equation can be expressed concisely using the induced MRP,\n",
    "$$ v_{\\pi} = \\mathcal{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi}v_{\\pi} $$\n",
    "\n",
    "- v에 대한 식으로 나타내면\n",
    "$$ v_{\\pi} (I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\mathcal{R}^{\\pi} $$\n",
    "- \\\\((I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\\\) 이 식이 상수값이 되기 때문에 linear하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function의 값을 최대로 하는 것은 optimal v를 구하는 것이다. 이때의 \\\\(v\\\\)를 \\\\(v_*\\\\)라 하고 optimal state value function\n",
    "$$ v_*(s) = \\max_{\\pi}v_{\\pi}(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://user-images.githubusercontent.com/22078438/48111892-13819a80-e297-11e8-91bf-4edc299e13eb.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action-value function의 값을 최대로 하는 optimal q를 구하는 것이다. 이때의 \\\\(q\\\\)를 \\\\(q_*\\\\)라 하고 optimal action value function이라 함.\n",
    "$$ q_*(s,a)=\\max_{\\pi}q_{\\pi}(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/48112269-1da49880-e299-11e8-987f-8313298aba9f.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy간 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\pi \\ge \\pi' i\\!f v_{\\pi}(s) \\ge v_{\\pi'}(s), \\forall s $$\n",
    "- 모든 상태에 대해서, policy를 따르는 v(s)가 다른 policy'을 따르는 v(s)보다 크거나 같다면 policy가 policy'보다 더 좋거나 같은 결과를 내는 정책이다.\n",
    "- 모든 상태에서 가장 optimal policy는 반드시 하나 혹은 하나 이상 존재한다는 것이다. 그 이유는 optimal value function을 찾거나, optimal action-value function을 찾으면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(q_*\\\\)을 이용하는 optimal policy가 있다. 그때 \\\\(q_*\\\\)가 최대가 되도록 하는 action a에서 \\\\(policy_*\\\\)가 1이 되고, 반대의 경우 0이된다. \\\\(policy_*\\\\)가 1일때 최적의 행동을 취하도록 실행이 되게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman optimal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function, \\\\(v_*\\\\)는 곧 \\\\(q_*\\\\)들이 이루는 값에서 최대값을 갖는 q를 선택하면 된다.\n",
    "$$ v_*(s) = \\max_a q_*(s,a) $$\n",
    "- action-value function, \\\\(q_*\\\\)는 즉시 받게 될 reward + action a가 주어졌을 때 다음 상태 s'으로 갈 확률 * value의 모든 합으로 나타낼 수 있다. 최대값을 갖는 action을 취하더라도 다음 state로 가게되는 것이 확률적으로 결정된다.\n",
    "$$ q_*(s,a)=\\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}v_*(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 state-value function을 v에 대해서 나타내면\n",
    "$$ v_*(s) = \\max_a \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}v_*(s') $$\n",
    "- 위 action-value function을 q에 대해서 나타내면\n",
    "$$ q_*(s,a)=\\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'} \\max_{a'} q_*(s',a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman optimal equation 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-linear하다. 앞서 봤던 \\\\( v_{\\pi} (I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\mathcal{R}^{\\pi} \\\\) 방정식은 linear했다. 하지만 위 수식은 max연산이 있기 때문에 non-linear하다.\n",
    "- 따라서, 연산량이 너무 많기 때문에 모든 연산을 다 할 수 없다. 그러므로,<br>\n",
    "    Value Iteration<br>\n",
    "    Policy Iteration<br>\n",
    "    Q-learning<br>\n",
    "    Sarsa<br>\n",
    "- 또한, 여러 환경이 존재한다.<br>\n",
    "    Infinite and continuous MDPs, 무한하고 지속되는 환경<br>\n",
    "    Partially observable MDPs, 부분적으로 볼 수 있고 전체적인 것은 알 수 없는 환경<br>\n",
    "    Undiscounted, average reward MDPs, 미래 보상을 discount하지 않거나 다르게 처리해야하는 환경<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
