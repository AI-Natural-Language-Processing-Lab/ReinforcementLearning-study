{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- markov decision process는 markov reward process에 decision을 추가한 개념이다.\n",
    "\n",
    "- 모든 state는 markov 환경에서 이루어진다.\n",
    "\n",
    "- 현재 상태 s에서 action a를 취했을 때 다음 상태 s'로 가게 될 확률 \\\\(\\mathcal{P}\\\\)\n",
    "$$ \\mathcal{P}^a_{ss'} = \\mathbb{P}[S_{t+1} = s'|S_t=s,A_t=a] $$\n",
    "\n",
    "- reward 함수도 현재 상태 s에서 action a를 취했을 때 expectation reward\n",
    "$$ \\mathcal{R}^a_s = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy는 agent가 행동하는데 중요한 역할을 하게 된다. 현재 state에 대해서 어떤 action을 취할 확률을 나타낸다.\n",
    "$$ \\pi(a|s) = \\mathbb{P}[A_t = a | S_t = s] $$\n",
    "\n",
    "- markov decision process policy에서는 현재 state만 고려하고 과러는 고려하지 않고 action한다. 이때 확률적으로 action을 결정하게 되고 policy는 time-step에 stationary하다.(time-independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given markov decision process \\\\(\\mathcal{M} = \\langle \\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma \\rangle\\\\) and a policy \\\\(\\pi\\\\)<br>\n",
    "    하나의 MDP가 주어졌을 때, 하나의 policy가 존재한다.\n",
    "\n",
    "- policy 개념을 markov process에 적용하면 \\\\(\\mathcal{P}^{\\pi}\\\\)가 되고, 이것을 policy \\\\(\\pi\\\\)를 따르는 \\\\(\\mathcal{P}\\\\)라 읽는다. 왜냐하면, policy를 벗어나는 state transition은 없기 때문이다.\n",
    "\n",
    "- The state and reward requence \\\\(S_1, R_2, S_2, ... \\\\) is a markov reward process \\\\(\\langle \\mathcal{S},\\mathcal{P}^{\\pi},\\mathcal{R}^{\\pi},\\gamma \\rangle \\\\)\n",
    "$$ \\mathcal{P}^{\\pi}_{s,s'} = \\sum_{a\\in A} \\pi(a|s) \\mathcal{P}^a_{ss'} $$\n",
    "$$ \\mathcal{R}^{\\pi}_s = \\sum_{a \\in A} \\pi(a|s) \\mathcal{R}^a_s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s에서 policy를 따르는 value가 되며 이것은 s에서 policy를 따를 때 expectation reward의 모든 합이 된다. 이것을 state-value function, v라 함.\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\;|\\; S_t=s] $$\n",
    "\n",
    "- 위 식에 action을 추가하게 되면 action-value function, q가 된다. 이것은 state s이면서 action  a인 조건에서 policy를 따르는 expectation reward 모든 합이 된다.<br>\n",
    "    즉, policy를 따라 action을 했을 때 얼마나 좋은지를 나타내는 값이 된다.\n",
    "$$ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\;|\\;S_t =s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v function, q function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function은 bellman equation을 통해서 분리 가능하다.\n",
    "$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\;|\\;S_t=s] $$\n",
    "\n",
    "- action-value function bellman equation 분리\n",
    "$$ q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}\\;|\\;S_t=s,A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 식으로 나타내면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function, 예를 들어) 현재 상태 s에서 policy를 따르는 v는 두 가지의 action 중에 하나에 대한 action a 를 취했을 때 policy를 따르는 q가 있고, 나머지 다른 action a'를 취했을 때 q'이 있다. 이때 q, q'을 합하면 v를 구성하게 된다.\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) $$\n",
    "\n",
    "- action-value function, 예를 들어) 현재 상태 s에서 action a를 취할 때 policy를 따르는 q는 reward r과 다음 state인 s'으로 이동하게 된다. 그 다음 state s'에서의 policy를 따르는 v가 있다. 하지만 위에서 s'으로 갔지만 다른 상태로 갈수도 있다. 따라서 위의 가능성을 모두 합하면 된다.\n",
    "$$ q_{\\pi}(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in S} \\mathcal{P}^a_{ss'} v_{\\pi}(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic1](https://user-images.githubusercontent.com/22078438/48111664-e41e5e00-e295-11e8-9531-495fa92366dd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식을 이용해서 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state value function과 action value function의 관계로 현재 state/action과 다음 state/action의 관계식이 만들어진다. 이것이 Bellman equation이라 한다.\n",
    "- Bellman equation은 expectation과 optimality 두 가지로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov decision process에서 정의한 state-value function은 아래와 같다.<br><br>\n",
    "$$ V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] $$<br>\n",
    "- 그리고, \\\\(G_t = R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + ...\\\\)이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} V_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\ &= \\mathbb{E}_{\\pi} [R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + ...|S_t=s] \\\\ &= \\mathbb{E}_{\\pi}[R_{t + 1} + \\gamma(R_{t + 2} + \\gamma R_{t + 3} + ...) |S_t=s] \\\\ &= \\mathbb{E}_{\\pi}[R_{t + 1} + \\gamma(G_{t + 1})|S_t = s] \\\\ &= \\mathbb{E}_{\\pi}[R_{t + 1} + \\gamma V(S_{t + 1})|S_t = s] \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, state-value function을 위와 같은 식으로 바꿀수 있고, \\\\(R_{t + 1})\\\\)는 \\\\(t + 1\\\\)시점에 받는 reward를 의미하고, \\\\(\\gamma V(S_{t + 1})\\\\)는 미래에 받을 reward에 discounted factor가 곱해진 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **하지만 위의 표현은 구현하기가 힘들다. 따라서, 구현할 수 있는 형태로 바꾸는 것이 중요하다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 시점(t), state에서 가능한 action a1, a2, a3가 있다 가정하자. 그때 agent가 선택한 policy에 따라서 action을 할 확률이 정해진다. 그러면 우린 action-value function을 얻을 수 있다.\n",
    "- 위 설명에 따라서 우리는 state-value function의 expectation을 각 action을 할 확률(policy)와 그 action의 value function을 곱한 것으로 나타낼 수 있다.<br><br>\n",
    "\n",
    "$$ V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) * q_{\\pi}(s, a) \\; ----------- \\; (1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞선 식으로는 next state-value function의 관계가 잘 드러나지 않기 때문에 action-value function에 대해서 조금 더 식을 전개하면 아래와 같이 나타난다.<br><br>\n",
    "$$ q_{\\pi}(s, a) = \\mathcal{R}^a_s + \\gamma * \\sum_{s' \\in S} \\mathcal{P}^a_{ss'} * V_{\\pi}(s') \\; ----------- \\; (2) $$<br>\n",
    "\n",
    "- **state s에서 action a를 했을 때 그 action에 대한 value는 두 가지 요소로 이루어져있다.**\n",
    "  - state s에서 action a를 했을 때 reward\n",
    "  - 그 다음 state의 value function\n",
    "- 여기서 \\\\(V_{\\pi}(s')\\\\)는 \\\\(t + 1\\\\) 시점에서 value function이므로 discount factor를 적용해줘야한다. 또한, deterministic한 환경이 아니라면, 현재 t 시점에서 state s에서 다음 \\\\(t + 1\\\\) state s'으로 전이될 확률도 식에 적용해줘야한다.<br><br>\n",
    "\n",
    "$$ V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\left( R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{\\pi}(s') \\right) \\; ----------- \\; (3) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 (1) 식과 (3) 식을 합쳐서 정리하면,<br><br>\n",
    "$$ q_{\\pi}(s, a) = R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} \\sum_{a' \\in A} \\pi(a'|s') q_{\\pi}(s', a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State, action-value function의 Bellman Expectation Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1), (2), (3)을 적절히 이용하여 아래와 같은 식을 도출할 수 있다.<br><br>\n",
    "\n",
    "$$ V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\left( R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{\\pi}(s') \\right) $$<br>\n",
    "\n",
    "$$ q_{\\pi}(s, a) = R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} \\sum_{a' \\in A} \\pi(a'|s') q_{\\pi}(s', a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일반적인 강화학습 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 초기 상태의 agent가 환경과 상호작용하면서 얻는 state와 action에 대한 값은 모두 의미없는 값들이다.\n",
    "2. 그리고 optimal policy를 모르는 상태로 random하게 action을 선택해서 행동한다.\n",
    "3. random action을 선택하여 환경과 상호작용하면서 얻은 reward와 state에 대한 정보를 이용해서, 특정 state에서 특정 action을 하는 것이 좋다(축적된 reward의 합이 가장 큰 경우)는 것을 배우게 된다.\n",
    "4. 위 에서 '좋다'라는 것을 판단하는 수단이 state-value function과 action-value function이 되고, 이것을 Bellman equation을 이용해서 update하며 더 좋은 reward를 얻을 수 있도록 유도한다.\n",
    "5. 이때, 가장 좋은 reward를 받을 수 있는 action을 선택하도록 optimal policy를 찾아간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**반복적으로 얻은 값이 가장 큰 경우를 Bellman optimality equation이라 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **최대의 reward를 얻는 policy를 찾는 것이 강화학습의 목표이다.**\n",
    "- **optimal policy를 따르는 Bellman equation을 Bellman optimality equation이라 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function의 optimal<br><br>\n",
    "$$ V_{*}(s) = {max}_{\\pi} V_{\\pi}(s) $$<br>\n",
    "\n",
    "- action-value function의 optimal<br><br>\n",
    "$$ q_{*}(s, a) = {max}_{\\pi} q_{\\pi}(s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function<br><br>\n",
    "$$ \\begin{split} V_{*}(s) = {max}_{\\pi} V_{\\pi}(s) &= {max}_a q_{\\pi}(s, a) \\\\ &= {max}_{a} \\left( R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{\\pi}(s') \\right) \\\\ &= R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{*}(s') \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action-value function<br><br>\n",
    "$$ \\begin{split} q_{*}(s, a) = {max}_{\\pi} q_{\\pi}(s, a) &= {max}_{\\pi} \\left( R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{\\pi}(s') \\right) \\\\ &= R^a_s + \\gamma \\sum_{s' \\in S} P^a_{ss'} V_{*}(s') \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bellman equation 참조 블로그](http://sumniya.tistory.com/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman expectation equation 간소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Bellman expectation equation can be expressed concisely using the induced MRP,\n",
    "$$ v_{\\pi} = \\mathcal{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi}v_{\\pi} $$\n",
    "\n",
    "- v에 대한 식으로 나타내면\n",
    "$$ v_{\\pi} (I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\mathcal{R}^{\\pi} $$\n",
    "- \\\\((I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\\\) 이 식이 상수값이 되기 때문에 linear하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function의 값을 최대로 하는 것은 optimal v를 구하는 것이다. 이때의 \\\\(v\\\\)를 \\\\(v_*\\\\)라 하고 optimal state value function\n",
    "$$ v_*(s) = \\max_{\\pi}v_{\\pi}(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://user-images.githubusercontent.com/22078438/48111892-13819a80-e297-11e8-91bf-4edc299e13eb.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action-value function의 값을 최대로 하는 optimal q를 구하는 것이다. 이때의 \\\\(q\\\\)를 \\\\(q_*\\\\)라 하고 optimal action value function이라 함.\n",
    "$$ q_*(s,a)=\\max_{\\pi}q_{\\pi}(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/48112269-1da49880-e299-11e8-987f-8313298aba9f.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy간 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\pi \\ge \\pi' i\\!f v_{\\pi}(s) \\ge v_{\\pi'}(s), \\forall s $$\n",
    "- 모든 상태에 대해서, policy를 따르는 v(s)가 다른 policy'을 따르는 v(s)보다 크거나 같다면 policy가 policy'보다 더 좋거나 같은 결과를 내는 정책이다.\n",
    "- 모든 상태에서 가장 optimal policy는 반드시 하나 혹은 하나 이상 존재한다는 것이다. 그 이유는 optimal value function을 찾거나, optimal action-value function을 찾으면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(q_*\\\\)을 이용하는 optimal policy가 있다. 그때 \\\\(q_*\\\\)가 최대가 되도록 하는 action a에서 \\\\(policy_*\\\\)가 1이 되고, 반대의 경우 0이된다. \\\\(policy_*\\\\)가 1일때 최적의 행동을 취하도록 실행이 되게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman optimal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-value function, \\\\(v_*\\\\)는 곧 \\\\(q_*\\\\)들이 이루는 값에서 최대값을 갖는 q를 선택하면 된다.\n",
    "$$ v_*(s) = \\max_a q_*(s,a) $$\n",
    "- action-value function, \\\\(q_*\\\\)는 즉시 받게 될 reward + action a가 주어졌을 때 다음 상태 s'으로 갈 확률 * value의 모든 합으로 나타낼 수 있다. 최대값을 갖는 action을 취하더라도 다음 state로 가게되는 것이 확률적으로 결정된다.\n",
    "$$ q_*(s,a)=\\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}v_*(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 state-value function을 v에 대해서 나타내면\n",
    "$$ v_*(s) = \\max_a \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}v_*(s') $$\n",
    "- 위 action-value function을 q에 대해서 나타내면\n",
    "$$ q_*(s,a)=\\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'} \\max_{a'} q_*(s',a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman optimal equation 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-linear하다. 앞서 봤던 \\\\( v_{\\pi} (I - \\gamma \\mathcal{P}^{\\pi})^{-1}\\mathcal{R}^{\\pi} \\\\) 방정식은 linear했다. 하지만 위 수식은 max연산이 있기 때문에 non-linear하다.\n",
    "- 따라서, 연산량이 너무 많기 때문에 모든 연산을 다 할 수 없다. 그러므로,<br>\n",
    "    Value Iteration<br>\n",
    "    Policy Iteration<br>\n",
    "    Q-learning<br>\n",
    "    Sarsa<br>\n",
    "- 또한, 여러 환경이 존재한다.<br>\n",
    "    Infinite and continuous MDPs, 무한하고 지속되는 환경<br>\n",
    "    Partially observable MDPs, 부분적으로 볼 수 있고 전체적인 것은 알 수 없는 환경<br>\n",
    "    Undiscounted, average reward MDPs, 미래 보상을 discount하지 않거나 다르게 처리해야하는 환경<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, ... , S_t]\\\\)\n",
    "- \\\\(S_t\\\\)에서 \\\\(S_{t+1}\\\\)로 갈 확률은, \\\\(S_1\\\\) ~ \\\\(S_t\\\\)가 주어져 있을 때 \\\\(S_{t+1}\\\\)로 가는 확률과 같다.\n",
    "- state만 필요할 뿐 history는 필요없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(\\mathcal{S}, \\mathcal{P}\\\\)의 집합으로 이루어져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov reward processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state 마다 reward가 존재한다.\n",
    "- \\\\(\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle\\\\) R과 discount factor \\\\(\\gamma\\\\)사용한다.\n",
    "- \\\\(\\mathcal{P}\\\\)는 state transition matrix(state t에서 t+1까지 가는 확률을 모두 적어둔 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Return을 Maximize한다.\n",
    "  - \\\\(G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum^{\\infty}_{k=0} \\gamma^k R_{t+k+1}\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state value function\n",
    "  - return의 기대값, MRP에서 value function\n",
    "  - \\\\( v(s) = \\mathbb{E}[G_t\\;|\\;S_t=s] \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov reward processes에서 action이 추가됨.\n",
    "- Markov reward precesses에서는 policy가 없었다. 그 이유는 action을 안하기 때문이다. policy는 state \\\\(s_t\\\\)에 있을 때 action a를 할 확률\n",
    "- policy는 state에서 action의 distribution이고, agent의 행동을 완전히 결정한다.\n",
    "- \\\\(\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle\\\\) 다음과 같은 식 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action value function\n",
    "  - state s에서 action a를 했을 때 그 이후에는 \\\\(\\pi\\\\)(policy)를 따라서 게임을 했을 때 \\\\(G_t\\\\)(return)의 최대값\n",
    "  - \\\\(q_{\\pi}(s,a) = \\mathbb{E}[G_t\\;|\\;S_t=s,A_t=a]\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
