{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지도학습\n",
    "- 비지도학습\n",
    "- 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No supervisor, 정답을 가르쳐 줄 supervisor가 없다. 오로지 **reward** 신호만 있다. -> 사람보다 잘할 수 있다. 지도가 있게 되면 지도 이상으로 할 수 가 없다.\n",
    "- Action에 대한 결과값이 즉각적으로 나타나지 않을 수도 있다.<br>\n",
    "    예를 들면) 내가 밥을 먹었을 때, 배가 아플지 아프지 않을 지는 시간이 지나야 확인 가능하다.\n",
    "- **시간**이 매우 중요하다.(sequential, non identically and independent distribution)\n",
    "    i.i.d, identically and independent distribution의 약자이고 '확률변수 X가 i.i.d를 따른다' 결과값이 모두 정규분포를 따르지만, 서로 다른 정규분포를 따른다는 뜻이다. 각 sample이 독립적으로 뽑혀서 한번 뽑힌것이 다음 sample에 영향을 안주는 것이 i.i.d 하지만 강화학습의 경우 sequential data이므로 순서가 있음.\n",
    "- Agent는 선택을 하고 action을 취하는 주체이고 자신의 행동에 따라서 그 다음에 어떤 정보를 받게 될지가 결정되고 이것은 같은 상황에서 다른 행동을 취했을 때 얻게 되는 정보들과 상이하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습 문제란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scalar feedback signal이다.\n",
    "- \\\\(R_t\\\\), t번째 time에 \\\\(R_t\\\\)만큼의 reward가 주어진다.\n",
    "- 그래서 agent의 최종 목적은 cumulative reward를 maximize하는 것이다.\n",
    "- 여기서 reward를 두개 이상 주고 싶을 때는 \\\\(reward = x * 0.1 + y * 0.9\\\\)와 같이 줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward \\\\(R_t\\\\)는 숫자입니다. 음수의 결과가 나타날 수도 있고 양수의 결과가 나타날 수도 있습니다.\n",
    "- 시간 step \\\\(t\\\\)에서 'agent가 어떻게 행동 했는가?'에 대한 reward 입니다.\n",
    "- 각각의 agent는 cumulative reward를 최대화하기 위해서 행동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래서 우리는 sequential하게 결정을 잘 해야한다. 한번 결정을 잘한다고 해서 reward가 max가 되지는 않는다.\n",
    "- 길게 봤을 때 좋은 선택을 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward hypothesis 정의\n",
    "- All goals can be described by the maximisation of expected cumulative reward\n",
    "- 따라서, 눈앞에 큰 보상이 중요한 것이 아니라 마지막에 받는 보상을 최대로 하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histroy & state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- history, agent가 한 것을 기록한다.\n",
    "- state, 다음에 무엇을 할지 결정하기 위해서 사용되는 정보\n",
    "- history의 정보를 가공해서 state를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents, Actions, Observations, Rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- History, history는 연속된 observations, actions, rewards 이다. \\\\( H_t=O_1, R_1, A_1, ... , A_{t-1}, O_t, R_t \\\\)<br>\n",
    "    이러한 history가 다음에 벌어질 일에 대해서 영향을 주게 된다. 과거에 어떤 행동을 했는가에 따라서 history는 바뀌고 그에 따라서 현재 상태, 미래 상태가 달라지게 된다.\n",
    "- Formally, state is a function of the history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- environment state, \\\\(S^e_t\\\\) 환경 입장에서의 상태, 환경이 가지고 있는 다양한 정보들이 있을 것인데 이 환경 속에 있는 agent는 모든 환경에 대한 정보를 볼 수는 없다. 게임할 때 정보(우리가 일반적으로 알 수 없는 정보), ex) 위치 좌표, ...\n",
    "- agent state, \\\\(S^a_t\\\\), 우리 주변의 정보나 상태가 아니라 우리 자신에 대한 상태 정보, 내가 다음 action을 하는데 쓰이는 정보\n",
    "  - 삼성전자 주식을 사는데 몇개의 feature를 볼것인가? 어제 주가만 본다 -> action state 1개, 어제 주가, 재무, ... -> action state가 여러개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### information state (a.k.a. Markov state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보 이론 관점에서의 information state, Markov state라 하는 상태가 있다. 데이터 관점에서 history의 유용한 정보를 담고있는 state이다.<br>\n",
    "    $$ \\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1 , ... , S_t] $$\n",
    "    \n",
    "- 위 식의 해석은, 현재 상태에서 다음 상태가 발생할 확률과 과거의 모든 상태에서 다음 상태가 발생할 확률이 같을 때 \\\\(S_t\\\\)는 Markov property라 한다.\n",
    "- 현재 상태는 과거의 모든 history를 포함하고 있기 때문에 현재 상태가 중요하고 과거의 정보는 의미가 없어진다.\n",
    "- 어떤 state가 markov하다. -> 내가 결정할 때 바로 이전 state만 보면 된다. 과거의 기억은 상관이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경에서 주는 모든 정보를 agent가 알 수 있을 때 Full observablity라 한다. 즉 우리가 세상에서 발생하는 모든 일을 알고있는 것과 같다. 이때 3개의 state는 모두 동일한 정보를 공유하고 있다. $$O_t = S^a_t = S^e_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이것을 Markove decision process(MDP)라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patially observability, 부분적인 환경만 알고있다.\n",
    "  - ex) 포커를 할 때 다른사람의 카드 까지 알수는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent의 3가지 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- agent의 행동을 나타낸다.\n",
    "- state를 넣으면 action이 나온다.\n",
    "- Deterministic policy, \\\\(a = \\pi(s)\\\\), output이 결정되어 나옴.\n",
    "- Stochastic policy, \\\\( \\pi(a|s) = P[A_t=a\\;|\\;S_t=s]\\\\), 여러 action이 가능한데 각 action별 확률을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy, 정책은 어떤 상태에서 이 agent가 어떤 행동을 해야하는가에 대한 내용이다.\n",
    "    규칙이 정해진 것이다.\n",
    "- Policy가 deterministic하다. 어떤 상태에 어떤 행동을 할 지 명확히 정해진 것입니다. $$ a = \\pi(s) $$\n",
    "- Policy가 stochastic하다. 어떤 상태에서 어떤ㄷ 행동을 하는 것이 확률적으로 높은지에 따라서 행동이 결정되는 것이다.\n",
    "$$ \\pi(a|s)= \\mathbb{P}[A_t=a|S_t=s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 게임이 끝날 때 까지 받을 수 있는 미래 reward의 총합\n",
    "- 현재 state가 좋은지 나쁜지를 판단한다.\n",
    "- **내가 이 state로 부터 어떤 policy \\\\(\\pi\\\\)를 따라 갔을 때 게임이 끝날 때 까지 얻는 총 reward의 기대값**\n",
    "- policy가 없으면 value function 정의가 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미래의 보상을 예측하기 위해서 사용한다. expectation(미래의 reward)\n",
    "- 임의의 policy에서 value는 현재 상태 다음에 발생할 reward, 그 다음 reward, ... 을 다 합한 기대값이 됩니다. 하지만 뒤로 갈수록 discounted rate를 적용해야한다.<br>\n",
    "    그 이유는, 현재 받는 reward와 미래에 받는 reward의 가치가 다르기 때문이다.\n",
    "    $$ v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경이 어떻게 될 지 예측한다. 환경이 줄것으로 예상되는 정보\n",
    "- 환경 예측\n",
    "  - reward 예측, state s에서 action a를 했을 때 reward를 뭘 받을지 예측한다.\n",
    "  - state transition을 예측, state s에서 action a를 했을 때 그 다음 state가 뭐가 될지 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태에서 어떤 행동을 취했을 때 환경이 줄 것이라고 예상되는 정보(next state, reward)를 표현한다.\n",
    "- 환경의 행동을 추론하는 model이다.\n",
    "- 다음 상태를 예측하는 \\\\(\\mathcal{P}\\\\), $$ \\mathcal{P}^a_{ss'} = \\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]$$\n",
    "- 다음 reward \\\\(\\mathcal{R}\\\\), $$ \\mathcal{R}^a_s = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$$\n",
    "- 이 정보는 agent 입장에서 보고 있는 환경에 대한 정보이다. 그리고 이 모델은 planning하는데 사용된다.\n",
    "- 이 모델이 transition에 대한 것이라 하면, 이것은 다음 state 정보를 확률적으로 예측하는 모델이고, 모델이 reward에 관한 것이라면 다음에 받게 될 보상의 expectation에 대한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떤 요소로 Agent를 구성하느냐? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value based\n",
    "  - No policy\n",
    "  - Value function\n",
    "- Policy based\n",
    "  - Policy\n",
    "  - No value function\n",
    "- actor critic\n",
    "  - policy\n",
    "  - value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model free\n",
    "  - model을 내부적으로 만들지 않는다.\n",
    "  - Policy and/or value function\n",
    "- Model based\n",
    "  - policy and/or value function\n",
    "  - 내부적으로 모델을 추측해서 만들어서 그것에 근거해서 움직인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy-based agent, value function을 사용하지 않고, policy와 model만으로 구성됨.\n",
    "- Value-based agent, policy를 사용하지 않고, value function과 model만으로 구성됨.\n",
    "- Policy, Value function, Model을 모두 사용하는 agent는 Actor Critic\n",
    "- Model-free agent, model을 고려하지 않고 policy and/or value function을 사용해서 구성됨.\n",
    "- Model-based agent, model과 policy and/or value function을 사용하여 구성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기에 agent는 환경에 대해서 알지 못한다.\n",
    "- 따라서 환경과 상호작용하여 환경에 대해서 알아간다.\n",
    "- 환경을 어느정도 이해하게 되면 policy를 만들고 개선해나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward가 어떻게 되는지 알고, transition이 어떻게 되는지 안다.(환경을 안다.) -> state s에서 action a를 하면 reward 얼마를 받을지 알고, 어떤 state로 갈지 안다.\n",
    "- 환경을 알기 때문에 내부적으로 계산을 통해서 다른 상황에 가볼 수 있다.\n",
    "- 에뮬레이터를 쿼리를 줘서 reward를 계산해서 planning을 해서 최적의 policy를 찾아간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경을 이미 알고있다.\n",
    "- 이미 환경에 대한 공략이 다 있기 때문에 공략을 보면서 한번에 최적의 policy를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning은 시행착오를 반복하면서 학습하게 된다.\n",
    "- 그리고 경험을 통해서 좋은 policy를 발견하게 되는데 그렇다면 보상을 잘 조절해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration / Exploitation dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploration, 초기에 agent는 환경에 대해서 잘 모르기 때문에 환경에 대해서 학습 하기 위해서 exploration(모험)을 해야합니다.\n",
    "    예를 들어) 맨날 가는 음식점에 가는 것 보다 안 가본 음식점을 가봐야 맛집을 찾을 수 있습니다.\n",
    "- Exploitation, 환경에 대한 학습이 어느정도 되면 reward를 최대화하기 위해서 알고 있는 정보를 활용해서 exploitation한다.\n",
    "    예를 들어) 맛있는 음식을 먹고 싶을 때 이미 알고있는 맛집으로 간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서 Exploration과 Exploitation을 적절히 활용해야 최적의 policy를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction / Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction, evaluate the future(given a policy)\n",
    "  - 미래를 평가, value function을 잘 학습시키는 것 state에 따른 value를 찾는 것\n",
    "- Control, optimise the future(find the best policy)\n",
    "  - 미래를 최적화 함. Best한 policy를 찾는다. state에서 어떻게 움직여야 하는지 찾는것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "episodes = 3\n",
    "for episode in range(episodes):\n",
    "    # initial environment를 불러온다(reset)\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "        env.render()\n",
    "        # 랜덤으로 action 생성\n",
    "        action = env.action_space.sample()\n",
    "        # env.step을 통해서 환경에 action을 전달\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 죽기 전까지 얼마나 살아 있었는가?\n",
    "        if done:\n",
    "            print(\"Episode {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
