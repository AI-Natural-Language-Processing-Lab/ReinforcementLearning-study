{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No supervisor, 정답을 가르쳐 줄 supervisor가 없다. 오로지 **reward** 신호만 있다.\n",
    "- Action에 대한 결과값이 즉각적으로 나타나지 않을 수도 있다.<br>\n",
    "    예를 들면) 내가 밥을 먹었을 때, 배가 아플지 아프지 않을 지는 시간이 지나야 확인 가능하다.\n",
    "- **시간**이 매우 중요하다.(sequential, non identically and independent distribution)\n",
    "    i.i.d, identically and independent distribution의 약자이고 '확률변수 X가 i.i.d를 따른다' 결과값이 모두 정규분포를 따르지만, 서로 다른 정규분포를 따른다는 뜻이다.\n",
    "- Agent는 선택을 하고 action을 취하는 주체이고 자신의 행동에 따라서 그 다음에 어떤 정보를 받게 될지가 결정되고 이것은 같은 상황에서 다른 행동을 취했을 때 얻게 되는 정보들과 상이하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward \\\\(R_t\\\\)는 숫자입니다. 음수의 결과가 나타날 수도 있고 양수의 결과가 나타날 수도 있습니다.\n",
    "- 시간 step \\\\(t\\\\)에서 'agent가 어떻게 행동 했는가?'에 대한 reward 입니다.\n",
    "- 각각의 agent는 cumulative reward를 최대화하기 위해서 행동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward hypothesis 정의\n",
    "- All goals can be described by the maximisation of expected cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
