{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No supervisor, 정답을 가르쳐 줄 supervisor가 없다. 오로지 **reward** 신호만 있다.\n",
    "- Action에 대한 결과값이 즉각적으로 나타나지 않을 수도 있다.<br>\n",
    "    예를 들면) 내가 밥을 먹었을 때, 배가 아플지 아프지 않을 지는 시간이 지나야 확인 가능하다.\n",
    "- **시간**이 매우 중요하다.(sequential, non identically and independent distribution)\n",
    "    i.i.d, identically and independent distribution의 약자이고 '확률변수 X가 i.i.d를 따른다' 결과값이 모두 정규분포를 따르지만, 서로 다른 정규분포를 따른다는 뜻이다.\n",
    "- Agent는 선택을 하고 action을 취하는 주체이고 자신의 행동에 따라서 그 다음에 어떤 정보를 받게 될지가 결정되고 이것은 같은 상황에서 다른 행동을 취했을 때 얻게 되는 정보들과 상이하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습의 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward \\\\(R_t\\\\)는 숫자입니다. 음수의 결과가 나타날 수도 있고 양수의 결과가 나타날 수도 있습니다.\n",
    "- 시간 step \\\\(t\\\\)에서 'agent가 어떻게 행동 했는가?'에 대한 reward 입니다.\n",
    "- 각각의 agent는 cumulative reward를 최대화하기 위해서 행동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward hypothesis 정의\n",
    "- All goals can be described by the maximisation of expected cumulative reward\n",
    "- 따라서, 눈앞에 큰 보상이 중요한 것이 아니라 마지막에 받는 보상을 최대로 하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents, Actions, Observations, Rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- History, history는 연속된 observations, actions, rewards 이다. \\\\( H_t=O_1, R_1, A_1, ... , A_{t-1}, O_t, R_t \\\\)<br>\n",
    "    이러한 history가 다음에 벌어질 일에 대해서 영향을 주게 된다. 과거에 어떤 행동을 했는가에 따라서 history는 바뀌고 그에 따라서 현재 상태, 미래 상태가 달라지게 된다.\n",
    "- Formally, state is a function of the history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- environment state, \\\\(S^e_t\\\\) 환경 입장에서의 상태, 환경이 가지고 있는 다양한 정보들이 있을 것인데 이 환경 속에 있는 agent는 모든 환경에 대한 정보를 볼 수는 없다.\n",
    "- agent state, \\\\(S^a_t\\\\), 우리 주변의 정보나 상태가 아니라 우리 자신에 대한 상태 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### information state (a.k.a. Markov state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보 이론 관점에서의 information state, Markov state라 하는 상태가 있다. 데이터 관점에서 history의 유용한 정보를 담고있는 state이다.<br>\n",
    "    $$ \\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1 , ... , S_t] $$\n",
    "    \n",
    "- 위 식의 해석은, 현재 상태에서 다음 상태가 발생할 확률과 과거의 모든 상태에서 다음 상태가 발생할 확률이 같을 때 \\\\(S_t\\\\)는 Markov property라 한다.\n",
    "- 현재 상태는 과거의 모든 history를 포함하고 있기 때문에 현재 상태가 중요하고 과거의 정보는 의미가 없어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경에서 주는 모든 정보를 agent가 알 수 있을 때 Full observablity라 한다. 즉 우리가 세상에서 발생하는 모든 일을 알고있는 것과 같다. 이때 3개의 state는 모두 동일한 정보를 공유하고 있다. $$O_t = S^a_t = S^e_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이것을 Markove decision process(MDP)라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent의 3가지 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy, 정책은 어떤 상태에서 이 agent가 어떤 행동을 해야하는가에 대한 내용이다.\n",
    "    규칙이 정해진 것이다.\n",
    "- Policy가 deterministic하다. 어떤 상태에 어떤 행동을 할 지 명확히 정해진 것입니다. $$ a = \\pi(s) $$\n",
    "- Policy가 stochastic하다. 어떤 상태에서 어떤ㄷ 행동을 하는 것이 확률적으로 높은지에 따라서 행동이 결정되는 것이다.\n",
    "$$ \\pi(a|s)= \\mathbb{P}[A_t=a|S_t=s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미래의 보상을 예측하기 위해서 사용한다. expectation(미래의 reward)\n",
    "- 임의의 policy에서 value는 현재 상태 다음에 발생할 reward, 그 다음 reward, ... 을 다 합한 기대값이 됩니다. 하지만 뒤로 갈수록 discounted rate를 적용해야한다.<br>\n",
    "    그 이유는, 현재 받는 reward와 미래에 받는 reward의 가치가 다르기 때문이다.\n",
    "    $$ v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
