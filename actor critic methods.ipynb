{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE, Baseline algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy와 state value function을 모두 학습하게 되지만, state-value function이 baseline에만 사용되고 critic으로 사용되지 않기 때문이다.\n",
    "- 따라서, bootstrapping에 사용되지 않고 baseline으로만 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function과 policy를 따로 생각하는 구조가 되어야한다.\n",
    "- policy는 action을 선택하는데 사용하고,\n",
    "- critic은 value function을 구하기 위한 value estimator는 action을 평가하는데 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 배운 REINFORCE, Baseline algorithm의 경우,\n",
    "  - bias가 없고 local minimum에 수렴한다. 하지만 학습이 느리다. 왜냐하면 높은 variance를 가지기 때문이다.\n",
    "- 따라서, temporal difference(TD)를 사용하면 multi-step learning을 통해서 bootstrapping정도를 조절할 수 있다. 그래서 bootstrapping critic을 가진 actor-critic을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- critic을 update하는 action value function을 위한 parameter \\\\(w\\\\)와,\n",
    "- actor를 위한 parameter \\\\(\\theta\\\\)가 필요하다.\n",
    "- \\\\(\\theta\\\\)는 critic이 제안하는 방향으로 \\\\(\\theta\\\\)를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- REINFORCE, Baseline 알고리즘 말고, TD를 생각해 볼 수 있고, 그 중 가장 간단한 TD(0)을 사용할 수 있다.\n",
    "  - TD(0)을 사용하게 되면서 앞서 전체의 return을 사용하였던 REINFORCE와 달리 one-step return만을 보게 된다.\n",
    "  - 또한, baseline이 포함된 것을 advantage function이라 부른다.<br><br>\n",
    "\n",
    "$$ \\begin{split} \\theta_{t + 1} &= \\theta_t + \\alpha(G^{(1)}_t - \\hat{v}(S_t,w)) \\frac{\\triangledown_{\\theta} \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t,\\theta)} \\\\ &= \\theta_t + \\alpha(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)) \\frac{\\triangledown_{\\theta} \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t,\\theta)} \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참조 블로그](https://jay.tech.blog/2017/01/04/policy-gradient-methods-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
