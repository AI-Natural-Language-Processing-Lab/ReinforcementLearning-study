{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 왜 entropy는 \\\\(H(X) = - \\sum^n_{i = 1} p(x_i) log p(x_i)\\\\)형태 인가?\n",
    "- KL Divergence는 어떤 의미(왜 수식이 그렇게 되는가?)를 가지고 있는가?\n",
    "- KL Divergence와 Cross-Entropy의 차이는 무엇인가?\n",
    "- Mutual Information은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 철수는 영미에게 해가 동쪽에서 뜬다는 정보를 매일 아침 보낸다.\n",
    "  - 당연한 말이기 때문에 관심이 없다. -> 정보량이 거의 없다.\n",
    "  - 당연하다. -> 그 사건의 확률이 높은 것이다.(확률이 너무 높아서 항상 그런 일이 일어나는 것)\n",
    "- **목표, 어떤 정보를 보냈을 때 그 말의 정보량을 수치로 나타내고 싶다.**\n",
    "- **정보량, 그 사건의 확률과 관계가 있다.**\n",
    "  - 확률이 높을 수록 정보량은 낮고,\n",
    "  - 확률이 낮을 수록 정보량은 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\\\(h\\\\)의 첫 번째 조건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(h\\\\)는 정보량을 나타내는 함수이다.\n",
    "- 앞선 예시와 관련지어서 보면,\n",
    "  - 확률 변수(Random variable) \\\\(X\\\\)는 해가 동쪽에서 뜬다. 해가 서쪽에서 뜬다. 두 가지 값을 가질 수 있다.\n",
    "  - \\\\(X\\\\)의 정보량 \\\\(h(x)\\\\)는 \\\\(p(x)\\\\)에 대한 함수. 즉, \\\\(h = f(p)\\\\)이다. \\\\(p(x)\\\\)가 \\\\(x\\\\)가 벌어질 확률.\n",
    "  - \\\\(p(east) = 0.99999,\\;p(west) = 0.00001\\\\) 이라고 두면 \\\\(h(west) > h(east)\\\\) 이런 식이 도출된다.\n",
    "  - 따라서 \\\\(p(x)\\\\)와 \\\\(h(x)\\\\)는 monotonic한 관계여야 한다. 즉 \\\\(f\\\\)는 단조 감소 함수.\n",
    "    - \\\\(p(x)\\\\)가 커지면 \\\\(h(x)\\\\)는 점점 작아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 가지 사건(해가 뜨는 위치(동쪽, 서쪽), 비가 왔나 안왔나)의 정보를 알려준다고 가정했을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\\\(h\\\\)의 두 번째 조건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률 변수(Random variable) \\\\(X, Y\\\\)\n",
    "  - \\\\(X\\\\)는 East, West 두 가지 값\n",
    "  - \\\\(Y\\\\)는 Rain, not Rain 두 가지 값\n",
    "  - \\\\(X, Y\\\\)는 독립이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(h(x, y) = h(x) + h(y)\\\\), 정보량은 더하기로 각각의 사건에서 얻은 정보량을 더하면 되고,\n",
    "- \\\\(p(x, y) = p(x) * p(y)\\\\), 사건 발생 확률은 곱하기로 나타낼수 있다.\n",
    "- 따라서, \\\\(f(p(x,y)) = f(p(x) * p(y)) = f(p(x)) + f(p(y))\\\\)가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식을 만족시키는 함수는 log 함수이다. 그 이유는 log 값은 곱하기 이면 더하기로 나눌수 있음. 따라서 log 함수가 \\\\(f\\\\)의 함수로 적절하다.\n",
    "- 따라서, **정보량을 나타내는 함수는 확률의 \\\\(log\\\\)값이다.**\n",
    "- \\\\(h(x) = -log_2p(x)\\\\)\n",
    "  - \\\\(log\\\\)식 앞에 \\\\(-\\\\)가 붙는 이유는 단조 감소를 해야하기 때문이다. 밑은 뭐든지 상관없다. 하지만 \\\\(2\\\\)를 사용하면 특별한 의미를 가짐. -> \\\\(h\\\\)의 단위가 bit가 된다.\n",
    "- **정보량은 위와 같이 표현하자고 약속을 한 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h(x) = -log_2p(x) $$\n",
    "- \\\\(h(east) = -log_2 p(east) = -log_2(0.99999999) = 0.000000014\\\\)\n",
    "- \\\\(h(west) = -log_2 p(west) = -log_2(0.00000001) = 26.5754247591\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면, 평균적인 정보량은\n",
    "  - \\\\(p(east) * h(east) + p(west) * h(west) = 0.99999999 * 0.000000014 + 0.00000001 * 26.6\\\\)\n",
    "- 일반적으로,<br><br>\n",
    "$$ H[X] = -\\sum_x p(x) log_2 p(x) = E_p[-log_2p(x)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한번 보내는 정보량의 기대값이 된다.\n",
    "- **Entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuous Variable의 경우\n",
    "  - \\\\(H[x] = \\lim_{\\vartriangle -> 0} \\left\\{ \\sum_i p(x_i) \\vartriangle ln p(x_i) \\right\\} = - \\int p(x) lnp(x)dx\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy는 Average Coding length의 lower bound이다.\n",
    "- Entropyd Maximize?\n",
    "  - Discrete variable: Uniform\n",
    "  - Continuous variable: Gaussian\n",
    "- Entropy Minimize?\n",
    "  - 한 점에 확률이 다 몰려 있는 경우 0이 된다. (예시, 동쪽에서 해가 뜨냐? 서쪽에서 해가 뜨냐 문제에서 entropy를 구해보니 0에 가까운 값이 나왔다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 주사위 4면의 확률분포는\n",
    "  - \\\\(p = (1/4, 1/4, 1/4, 1/4)\\\\)라고 하자.\n",
    "- 하지만 잘못 modeling된 확률분포는\n",
    "  - \\\\(q = (1/2, 1/4, 1/8, 1/8)\\\\)로 잘못 모델링 하였다.\n",
    "- 그렇게 잘못 모델링 한 것을 보고 상태를 각각 0, 10, 110, 111로 코딩하였다.(자주 나오지 않는 확률은 긴 값을 보내고(111), 자주 나오는 확률은 짧은 값을 보낸다.(0))\n",
    "- 실제 최적 코딩은 00, 01, 10, 11이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 잘못 모델링 했을 때 평균 coding length는\n",
    "  - \\\\(\\frac{1}{4} * 1 + \\frac{1}{4} * 2 + \\frac{1}{4} *  3 + \\frac{1}{4} * 3 = 2.25\\\\)\n",
    "- 즉, \\\\(-\\sum_x p(x)log_2 q(x)\\\\) \\\\(p(x)\\\\)는 실제 예측, \\\\(q(x)\\\\)는 내가 모델링 한 것\n",
    "  - \\\\(= -\\frac{1}{4} * log_2(0.5) - \\frac{1}{4} * log_2(0.25) - \\frac{1}{4} * log_2(0.125) - \\frac{1}{4} * log_2(0.125) = 2.25\\\\)\n",
    "- 정확하게 p를 모델링 했을 때,\n",
    "  - \\\\(-\\sum_x p(x) log_2 p(x) = -\\frac{1}{4} * log_2(0.25) - \\frac{1}{4} * log_2(0.25) - \\frac{1}{4} * log_2(0.25) - \\frac{1}{4} * log_2(0.25) = 2\\\\)\n",
    "- 모델링한 q와 p가 다르기 때문에 추가적인 비용이 발생한다.\n",
    "  - 2.25 - 2 = 0.25(KL-Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델링의 오류 때문에 발생한 추가 비용\n",
    "  - \\\\( \\left( -\\sum_x p(x) log_2 q(x) \\right) - \\left( -\\sum_x p(x) log_2 p(x) \\right) = -\\sum_x p(x) log_2 \\frac{q(x)}{p(x)} \\\\)\n",
    "- Continuous variable의 경우\n",
    "  - \\\\( \\begin{split} KL(p \\lVert q) &= - \\int p(x) ln q(x)dx - \\left( -\\int p(x) ln p(x) dx \\right) \\\\ &= - \\int p(x) ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} dx \\end{split} \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(p(x)log_2p(x)\\\\)가 정보량이고, \\\\(p(x)\\\\)를 잘못 \\\\(q(x)\\\\)로 생각해서 구한 정보량을 뺀다.\n",
    "- 질문) 내가 모델링을 잘못한 것은 알겠는데 그렇다면 항상 양수 값일까?\n",
    "  - 아님.\n",
    "  - p와 q가 바뀌면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(KL(p|q) \\neq KL(q|p)\\\\)\n",
    "- \\\\(KL(p|q) \\geq, \\; (p=q,\\\\)일때 0 만족)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p와 q 서로가 반대가 되는 것이 아니라 따라가는 방향성이 있는 것이기 때문에 p와 q를 바꾸면 같지가 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(KL(p|q) = H(p,q) - H(p)\\\\)\n",
    "- Classification 할때 왜 loss 함수는 Cross entropy 인가?\n",
    "  - p(정답)를 근사하기 위해서 q(neural net)을 만들었음.\n",
    "  - 따라서 H(p)는 q와 무관하다. 즉 q의 parameter로 미분하면 사라진다.\n",
    "  - 그래서 H(p,q)를 loss로 사용한다. -> KL divergence 쓰는 것과 똑같음.\n",
    "- KL-divergence는 언제 쓰는가?\n",
    "  - p도 학습하고, q도 학습할 때 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x와 y가 independent이면 \\\\(p(x, y) = p(x) * p(y)\\\\)\n",
    "- 만약 독립이 아니라면 위 식의 차이가 커질 것이다. 따라서 KL-divergence를 이용해서 \\\\(p(x) * p(y)\\\\)가 \\\\(p(x, y)\\\\)에 얼마나 가까운지에 대한 값을 얻을 수 있다.<br><br>\n",
    "$$ \\begin{split} I[x, y] &= KL(p(x, y) \\lVert p(x)p(y)) \\\\ &= -\\int \\int p(x,y) ln \\left( \\frac{p(x)p(y)}{p(x, y)} \\right) dxdy \\end{split} $$<br>\n",
    "- 따라서 \\\\(I(x, y) \\geq 0, x, y\\\\)가 독립일 때 등호가 성립한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
