{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous control with deep reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action space가 discrete space가 아닌 continuous space에 적용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN에서 neural network를 사용하면서 기본적으로 발생하는 parameter의 수가 많아서 학습에 지장이 간다. 그래서 학습을 하는데 사용되는 sample은 i.i.d 특징을 가져야하고 이것을 해결하기 위해서 replay buffer를 사용하였다. 또한, target network을 따로 두는 방법도 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만, DDPG에서는 target network의 parameter update를 soft하게 하는 soft target update방법을 채택하였고,\n",
    "  - \\\\(\\theta' \\leftarrow \\tau \\theta + (1 - \\tau) \\theta'\\\\)\n",
    "- network 학습에서 order of magnitude 문제를 해결하기 위해서 batch norm도 적용하였다.\n",
    "- 마지막으로, exploration을 하기 위해서 noise process(Ornstein-Uhlenbeck)를 도입하였다.\n",
    "  - \\\\(\\mu'(s_t) = \\mu(s_t|\\theta^{\\mu}_t) + N \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 장점\n",
    "  - sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용한다. 이렇게 함으로써 high dimensional observation space문제를 해결한다.\n",
    "- 단점\n",
    "  - discrete & low dimensional action space만 가능하다. Continuous action space를 다루기 위해서 매 step마다 iterative optimization precess를 거쳐야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous space를 해결하기 위해서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model-free, off-policy, actor-critic 알고리즘을 사용한다.\n",
    "- DPG를 기반으로 한다.\n",
    "- DQN에서 좋았던, Replay buffer와 target Q network를 actor-critic에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation: \\\\(x_t\\\\)\n",
    "- Action: \\\\(a_t \\in {IR}^N\\\\)\n",
    "- Reward: \\\\(r_t\\\\)\n",
    "- Discount factor: \\\\(\\gamma\\\\)\n",
    "- Environment: \\\\(E\\\\)\n",
    "- Policy: \\\\(\\pi : S \\rightarrow P(A)\\\\)\n",
    "- Transition dynamics: \\\\(p(s_{t+1}|s_t, a_t)\\\\)\n",
    "- Reward function: \\\\(r(s_t, a_t)\\\\)\n",
    "- Return: \\\\(\\sum^{T}_{i=t} \\gamma^{(i - t)} r(s_i, a_i)\\\\)\n",
    "- Discounted state visitation distribution for a policy: \\\\(\\rho^\\pi\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상태 \\\\(s_t\\\\)에서 행동 \\\\(a_t\\\\)를 취했을 때 Expected return은 아래의 식과 같다.\n",
    "  - \\\\( Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t] \\\\)\n",
    "- Bellman equation을 이용해서 위 식을 변형한다.\n",
    "  - \\\\(Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ]\\\\)\n",
    "- Deterministic policy를 가정한다\n",
    "  - \\\\(Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{split} v(s) &= \\mathbb{E}[G_t | S_t = s] \\\\\n",
    "&= \\mathbb{E}[R_{t + 1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s] \\\\\n",
    "&= \\mathbb{E}[R_{t + 1} + \\gamma(R_{t + 2} + \\gamma R_{t + 3} + ...) | S_t = s] \\\\\n",
    "&= \\mathbb{E}[R_{t + 1} + \\gamma G_{t + 1} | S_t = s] \\\\\n",
    "&= \\mathbb{E}[R_{t + 1} + \\gamma v(S_{t + 1}) | S_t = s] \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinisitc Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic policy gradient는 policy를 state space와 action space의 확률분포를 고려한다.\n",
    "- Deterministic policy는 policy를 생각할 때 action space에서 특정 action을 선택(deterministic)하면서 policy를 발전시킬 수 있다.\n",
    "  - 이때 action 선택은 off-policy 방법을 사용하고,\n",
    "  - stochastic 방법보다 action 부분을 제거할 수 있기 때문에 간단하게 policy를 향상 시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Deterministic policy 개념 참조](https://jay.tech.blog/2018/10/08/deterministic-policy-gradient/)\n",
    "- [DDPG 개념 참조](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
