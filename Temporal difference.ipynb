{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Step Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 step만 가고 update 할 수 있고, 두 step 가고 update 할 수도 있다.\n",
    "- n이 \\\\(\\infty\\\\)이면 Monte-Carlo가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Step Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the following \\\\(n\\\\)-step returns for \\\\(n=1,2,\\infty:\\\\) \n",
    "- \\\\( n=1,\\\\) (TD), \\\\(\\quad G^{(1)}=R_{t+1}+\\gamma V(S_{t+1}) \\\\)\n",
    "- \\\\( n=2,\\\\) \\\\(\\quad G^{(2)}= R_{t+1} + \\gamma R_{t+2}+\\gamma^2 V(S_{t+2}) \\\\)\n",
    "- \\\\( n=\\infty,\\\\) \\\\(\\quad G^{(\\infty)}= R_{t+1} + \\gamma R_{t+2}+ ... +\\gamma^{T-1} R_T \\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD에 대한 성능 비교, n을 바꿨을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-step, 5-step이 가장 좋다. 적당히 n을 정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging n-Step Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(TD(1) + TD(2) + ... + TD(\\infty)\\\\) 다 더해서 사용해도 된다.\n",
    "$$ G_t^{\\lambda} =(1-\\lambda)\\sum^{\\infty}_{n=1} \\infty^{n-1}G^{(n)}_t $$\n",
    "- geometric mean을 사용한다. 다 더하면 1이 되도록 하기 위해서 TD(1)의 가중치가 많이 있다.\n",
    "- Forward-view \\\\(TD(\\lambda)\\\\)\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha(G^{\\lambda}_t- V(S_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 이것은 게임이 끝나야 학습을 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD backward-view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online으로 update가능하고, incomplete sequences에서도 update가능\n",
    "- Eligibility Traces<br>\n",
    "    어느 사건이 일어나면 사건의 책임을 준다. 책임이 큰 것에 update를 많이 해주는 방식<br>\n",
    "    예시, 종이 3번 울리고 전구가 켜지고 전기충격을 받는다. 이때 전기 충격은 전구한테 책임을 줄수 있다. 왜냐하면 가장 최근에 일어난 일이기 때문이다. 종에게 책임을 물을수도 있다. 왜냐하면 가장 빈번하게 일어났기 때문이다.<br>\n",
    "    책임을 나누는 기준, Frequency heuristic(많이 일어나는 애한테 많이 물어야한다.), Recency heuristic(최근에 일어난 애한테 많이 물어야한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 방문하면 1을 올려주고 방문하지 않으면 0.9로 줄여준다. 방문하지 않을 때 계속 줄여줌.\n",
    "<img width=\"385\" alt=\"2018-11-11 10 11 26\" src=\"https://user-images.githubusercontent.com/22078438/48313369-cab34400-e5fe-11e8-85d6-c2dc2d1869b7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림은 방문할수록 커질것이고 최근에 방문할수록 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward View TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 state의 책임이 얼마나 큰지를 기록해두는 값\\\\(E_t(s)\\\\)이 있어서 그것을 곱해서 업데이트를 해주면 TD(\\\\(\\lambda \\\\))와 동일하다.\n",
    "$$ \\delta_t = R_{t+1} + \\gamma V(S_{t+1} - V(S_t) $$\n",
    "$$ V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t(s) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
