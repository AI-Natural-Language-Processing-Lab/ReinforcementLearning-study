{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dynamic, 연속적으로 발생하거나 일시적으로 발생하는 요소에 대한 문제를 푸는 것이다.\n",
    "- Programming, optimising a \"program\"\n",
    "- 따라서, Dynamic programming은 연속적으로 발생되는 문제를 수학적으로 optimising 해서 푸는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 푸는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimal substructure, 하나의 문제를 쪼개고 쪼개진 각각의 문제를 optimising하면 원래 문제도 해결된다는 방식\n",
    "- Overlapping subproblems, 하나의 서브 문제를 해결한 뒤 저장해두고 불러서 다시 사용하는 방식\n",
    "- Dynamic programming은 bellman equation과 value function이 대표적인 특성을 가진다.\n",
    "- Dynamic programming은 MDP의 모든 상황을 다 알고있다고 가정을 한다. 따라서 planning을 할 수 있다. 무엇을 했을 때 어떻게 될지를 이미 알고 있기 때문에 우리는 계획을 미리 세울수 있는 것과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP와 policy를 이용해서 value function을 찾는 것이 prediction\n",
    "- MDP를 이용해서 기존 value function을 최적화 하는 것이 control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제, 주어진 policy \\\\(\\pi \\\\)를 평가하는 것\n",
    "- 해결, Bellman expectation equation을 반복적 연산으로 policy 평가를 한다.\n",
    "- synchronous backup, 이전에 사용했던 \\\\(v_k\\\\)를 이용해서 다음 \\\\(v_{k+1}\\\\)을 update한다.\n",
    "\n",
    "$$ v_{k+1}(s) = \\sum_{a \\in A}\\pi(a|s) \\left( \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}v_k(s')\\right) $$\n",
    "\n",
    "$$ v_{k+1}=\\mathcal{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi}v^k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value 연산이 아래에서 위로 올라가기 때문에 backup이라 표현함. bottom-up 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy를 더 좋게 update하기 위해서 2가지 접근 방식을 도입함.\n",
    "- 현재 policy를 이용해서 value function을 찾는것 -> evaluate\n",
    "- 그리고 이 value 값과 action에 대한 value값을 비교하여 더 좋은 policy를 찾아가는 것을 improve라 한다.\n",
    "- 위 두가지 과정을 반복하면서 policy와 value가 수렴하게 되고, 그때가 optimal policy, value라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 state에서 value function은 해당 state에서 취할 수 있는 action에 대한 평균이기 때문에 value function의 값보다 더 큰값을 갖는 action을 찾아서 취하면 좀 더 좋은 policy로 update 가능하다.\n",
    "\n",
    "$$ q_{\\pi}(s, \\pi'(s)) = \\max_{a \\in \\mathcal{A}} q_{\\pi}(s, a) \\ge q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s) $$\n",
    "\n",
    "- 만약 \\\\(v_{\\pi'}(s) \\ge v_{\\pi}(s)\\\\) 이면\n",
    "$$ \\begin{split}\n",
    "v_{\\pi}(s) &\\le q_{\\pi}(s, \\pi'(s)) = \\mathbb{E}_{\\pi'}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})\\;|\\;S_t=s] \\\\ &\\le \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1})\\;|\\;S_t=s] \\\\ &\\le \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_{\\pi}(S_{t+2}, \\pi'(S_{t+2})\\;|\\;S_t=s] \\\\ &\\le \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2}\\;|\\;S_t=s] = v_{\\pi'}(s) \\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy imporve 과정이 반복되면서 어느정도 수렴하게 되면, q값과 v값이 같아지는 지점이 온다.이때 v와 policy는 최적화된 상태라 할 수 있고 이때의 policy를 optimal policy라 한다.\n",
    "$$ q_{\\pi}(s, \\pi'(s)) = \\max_{a \\in \\mathcal{A}} q_{\\pi}(s, a) = q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 언제까지 반복해야 하나? 수렴 할 때 까지 반복해도 되지만 너무 오래 걸릴수 있기 때문에 value function 변화량이 특정 값 이해가 되면 stop하거나 k가 몇번 반복된 후 한번 imporve하도록 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
