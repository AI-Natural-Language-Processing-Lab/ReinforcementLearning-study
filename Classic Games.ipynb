{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Games(보드 게임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 classic games을 공부하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 규칙은 간단하지만, 개념은 복잡하다.\n",
    "  - Supermario 게임에서 볼 수 있듯이 마지막 goal을 취하면 되고, 몬스터에 닿이면 죽는다. 아주 간단한 rule을 가지고 있지만 해결하는 과정이 매우 복잡하다.\n",
    "- 오랜 시간 사람들이 game을 play해왔다.\n",
    "- 현실 세계를 포함하는 작은 세계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality in Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward를 가장 많이 받는 것이 게임을 가장 잘하는 것이다.\n",
    "- 하지만, two player game으로 넘어오게 되었고, 그때 optimality가 무엇이냐?\n",
    "  - 게임에서 만나는 각각의 상대마다 optimality가 다르다. 예를 들면, 가위바위보에서 항상 가위만 내는 사람에게 optimality는 바위일 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimal을 정의하기 위해서 Game Theory가 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- player가 여러명이 있고, \\\\(i\\\\)번째 player 입장에서 봤을 때 optimal은 무엇인가?\n",
    "  - \\\\(i\\\\)번째 player이외에 다른 player의 policy가 고정되어 있다 했을 때 그 떄의 최적의 policy가 Best response이다.\n",
    "  - 나머지 player에 대한 best response는 정해져있음. policy가 고정되어 있다고 가정했기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정의: **상대 전략을 전제로 자신의 이익을 최대화하는 전략을 선택해 형성된 균형**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 player가 best response를 취할 때 Nash 균형이라 함.\n",
    "- 서로 서로 최적의 response를 가지고 있기 때문에 policy를 바꾸지 않음. 그래서 균형이 맞춰지고, Nash 균형이라 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Agent and Self-Play Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Agent Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best response는 single-agent RL의 해답이 된다.\n",
    "- 상대방이 고정되어 있으면(나를 제외한 모든 것이 환경이라고 생각할 수 있음.) 그 상대방을 환경이라고 두고 진행할 수 있다.\n",
    "- 상대방의 policy가 환경으로 생각되면서 수학적으로 MDP로 표현할 수 있다. 따라서 MDP로 된다.\n",
    "- 따라서, **Best response는 이 MDP의 optimal policy가 된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash equilibrium is fixed-point of self-play RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- self-play RL을 하는데(나와 나 자신이 붙으면서 강화학습을 진행하는데), 어느순간 policy가 바뀌지 않는 부동점(fixed-point)이 Nash 균형\n",
    "  - agent간 game으로 부터 experience가 만들어지는데,<br><br>\n",
    "  $$ a_1 \\sim \\pi^1, a_2 \\sim \\pi^2, ... $$\n",
    "  - action 1은 policy 1을 따르고, action 2는 policy 2를 따른다고 가정한다.\n",
    "  - 각 agent는 상대방에 대해서 best response를 학습해나가고, 한 명의 policy가 다른 player의 환경이 되니까, 환경도 동적으로 바뀌면서 fixed-point를 찾아가는 과정이 된다.\n",
    "  - 모든 player가 상대방에 적응하면서 Nash 균형을 찾아간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nash 균형은 1개 인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 개는 아니다. 하지만 특정 경우에 한 개가 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Player Zero-Sum Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3가지 조건\n",
    "  - two-player game이고, 동시에 게임을 하는 것이 아니라 서로의 turn이 있다.\n",
    "  - '흑'에게 이득이 되는 것은 '백'에게 손해이고, '백'에게 이득이 되는 것은 '흑'에게 손해이다.\n",
    "    - 서로 공격을 한다.\n",
    "    - \\\\(R^1 + R^2 = 0\\\\)\n",
    "  - 상대가 어떤 상태인지 알 수 있어야한다.(perfect information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **위 조건을 만족하면, Nash 균형이 1개만 생긴다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어,\n",
    "  - 가위바위보를 할 때 내가 가위만 내게 되면 이길 확률이 1/3이던 게임은 바위를 내게 되면 계속 이기는 것이 되기 때문에 policy를 바위만 내도록 바꾸게 된다. 이렇게 되면 Nash 균형이 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nash 균형을 찾는 것을 목적으로 한다.\n",
    "  - Game tree search\n",
    "  - self-play reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect and Imperfect information games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perfect information\n",
    "  - Markov game이 모두 observed가능한 것\n",
    "- imperfect information\n",
    "  - 부분적으로 관측 가능한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Search 기반 방법론\n",
    "- 지금 까지 배운 value function은 환경이 고정되어 있고, 우리가 어떤 policy를 취했을 때 return의 기대값 하지만, two-player 게임에는 player가 두명이기 때문에 value function을 다르게 정의한다.\n",
    "  - 첫 번째 player와 두 번째 player의 policy가 정해졌을 때 return의 기대값.\n",
    "- 기존 value function: \\\\(v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\\\\)\n",
    "- minimax value function: \\\\(v_{*}(s) = {\\max}_{\\pi^1} {\\min}_{\\pi^2} v_{\\pi}(s)\\\\)\n",
    "  - \\\\(\\pi^1\\\\)입장에서는 value function을 maximize하는 것이 목표이고,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 player가 서로 경쟁하면서 잘하는 것을 찾는 policy가 minimax policy이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/50765762-47d6fe80-12ba-11e9-8c8a-c0e26c87fa78.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 흰색 원에서는 max값을 선택해서 올리고, 검은색 원에서는 min값을 선택해서 위로 올린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function in Minimax Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tree가 기하급수적으로 커진다.\n",
    "  - 따라서, 게임 끝까지 모두 search하는 것은 불가능하다.\n",
    "- 그러므로, value function approximator를 써서 tree 끝까지 가지말고 중간에 잘라서 사용하자.\n",
    "- depth를 fixed해서(앞선 3수) value function값을 넣어주자.\n",
    "- 가장 간단한 tree search가 minimax search이고, 대표적으로 \\\\(\\alpha, \\beta\\\\) search가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary-Linear Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 binary feature vector를 만들고, feature vector에 weight를 곱한 것이 value라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를들어,\n",
    "![default](https://user-images.githubusercontent.com/22078438/50766110-67225b80-12bb-11e9-882b-5505049e0303.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature는 내가 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8000개의 feature를 만들어서 사용했고,\n",
    "- binary-linear value function을 사용했다.\n",
    "- 전문가가 weights를 tuning 했음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alpha-beta search를 하였고, 16 ~ 40수 까지 보고 사용했음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-play Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금 까지 했던 RL 알고리즘을 사용하는 것이다.\n",
    "- 실제 value와 내가 예측하는 value 차이를 제곱해서 최소화한다.\n",
    "- Monte-carlo <br><br>\n",
    "$$ \\vartriangle w = \\alpha(G_t - v(S_t, w)) \\triangledown_w v(S_t, w) $$<br>\n",
    "\n",
    "- TD(0)<br><br>\n",
    "$$ \\vartriangle w = \\alpha(v(S_{t + 1}, w) - v(S_t, w)) \\triangledown_w v(S_t, w) $$<br>\n",
    "\n",
    "- TD(\\\\(\\lambda\\\\))<br><br>\n",
    "$$ \\vartriangle w = \\alpha(G_t^{\\lambda} - v(S_t, w)) \\triangledown_w v(S_t, w) $$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- minimax value function으로 바뀜.\n",
    "- 중간 중간에 reward가 발생할 수 없기 때문에 \\\\(r, \\gamma\\\\)가 없어졌음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy improvement with Afterstates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- game의 rule을 안다고 가정했을 때, 이전에 model-free와 model-based가 있었는데, 모델을 모를 때는 v만 가지고는 action을 할 수 없음. 왜냐하면 어느 action을 해야 그 state로 가는지 모르기 때문이다.\n",
    "- 하지만, game의 rule을 완전히 알고 있다고 가정하면, 어느 action을 하면 어떤 state로 가는지 알고 있기 때문에 v만 알고있어도 policy improvement를 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ q_*(s, a) = v_*(succ(s,a)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- white<br><br>\n",
    "$$ A_t = argmax_{a} v_*(succ(S_t,a)) $$<br>\n",
    "- black<br><br>\n",
    "$$ A_t = argmin_{a} v_*(succ(S_t,a)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Gammon: Non-Linear Value function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- random weights로 초기화 하고,\n",
    "- self-play로 training 했다.\n",
    "  - 그때, non-linear temporal-difference를 사용했다.\n",
    "  - \\\\(\\delta_t = v(S_{t + 1}, w) - v(S_t, w) \\\\)\n",
    "  - \\\\(\\vartriangle w = \\alpha \\delta_t \\triangledown_w v(S_t, w)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- no exploration이었는데 어떻게 항상 수렴했을까?, Back gammon에는 주사위가 있는데 그것이 stochastic한 역할을 해줬기 때문에 exploration 없이 수렴이 가능했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning과 minimax 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 value를 다음 state value로 update하는 것이다.\n",
    "- TD learning으로 value function 학습하고,\n",
    "  - \\\\(v(S_t, w) \\leftarrow v(S_{t + 1}, w) \\\\)\n",
    "- 그 다음 value function을 이용해서 minimax search에 썼었음.\n",
    "  - \\\\(v_{+}(S_t,w) = {minimax}_{s \\in leaves(S_t)} v(s_t, w)\\\\)\n",
    "    - \\\\(+\\\\)는 \\\\(S_t\\\\)에서 minimax value를 한 결과, search 실행 시킨 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://user-images.githubusercontent.com/22078438/50769090-267c0f80-12c6-11e9-95cd-a80c2fa39dcb.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 \\\\(S_t\\\\)에 있고, 다음 state가 \\\\(S_{t + 1}\\\\)인데, \\\\(S_{t + 1}\\\\)에서 minimax search를 한다. 그 때 minimax value가 초록색 원일 때 \\\\(S_t\\\\)를 초록색 원이 되도록 value를 학습한다.\n",
    "- \\\\(S_{t + 1}\\\\)에서 search를 하고, search한 value로 \\\\(S_t\\\\)를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 배운 simple TD는 \\\\(V(S_{t + 1})\\\\)이 \\\\(S_t\\\\)로 왔는데, minimax value를 찾은 것이 더 정확하니까 그것을 \\\\(S_t\\\\)에 넣자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(v_{+}(S_t, w) = minimax_{s \\in leaves(S_t)} v(s, w)\\\\)\n",
    "  - search 돌린 값\n",
    "- \\\\(v(S_t, w) \\leftarrow v_{+}(S_{t + 1}, w) = v(I_{+}(S_{t + 1}),w)\\\\)\n",
    "  - \\\\(S_{t + 1}\\\\)에서 search 돌린 값을 넣자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Where \\\\(I_{+}(s)\\\\) is the leaf node achieving minimax value from s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래는 TD update할 때 다음 state value를 사용하였는데, 다음 state에서 minimax search를 돌려서 \\\\(v_{+}(S_{t + 1})\\\\)을 구하고, 그 값으로 \\\\(v(S_t, w)\\\\)를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **value function을 학습하는 도중에 search를 쓴 큰 의미가 있는 알고리즘임**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/50768940-9fc73280-12c5-11e9-8681-c04d0d091c34.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{+}(S_t, w) = minimax_{s \\in leaves(S_t)} v(s, w), \\;\\;\\; v_{+}(S_{t + 1},w) = minimax_{s \\in leaves(S_{t + 1})} v(s, w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{+}(S_t , w) \\leftarrow v_{+}(S_{t + 1}, w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내 현재의 search 결과를 다음 step의 search 결과로 바꿔준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreeStrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD leaf와 TD root는 현재 step과 앞 step을 섞는 것이다.\n",
    "- TreeStrap에서는 굳이 현재와 미래를 섞을 필요가 있냐?는 이론임.\n",
    "- 같은 단계의 모든 node들이 본인 node 아래의 node로 update 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](https://user-images.githubusercontent.com/22078438/50772457-3baa6b80-12d1-11e9-9487-c2957f54aa26.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation-based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- root state부터 self-play로 시뮬레이션을 계속 한다.\n",
    "- simulated experience에 RL(model-free rl)을 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte-Carlo Tree Search, MCTS는 monte-carlo control을 search에 넣는다. 각 action의 value를 simulated experience로 부터 학습하고 좋을 것 같은 action value를 고른다. 하지만 이것은 exploitation만 되니까,\n",
    "- UCT 알고리즘을 통해서 exploration도 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth UCT Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imperfect information일때 사용함.\n",
    "- 특정 확률로 UCT를 하고, 그 반대의 확률로 average strategy라는 것을 만들어서 지금 까지 배웠던 상대들의 average 행동을 학습한다. 그것을 통해서 UCT를 수정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[팡요랩 Youtube](https://www.youtube.com/watch?v=C5_2v4pRc5c&t=54s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
