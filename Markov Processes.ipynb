{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Markov Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전제조건으로 agent가 환경에서 발생되는 모든 정보를 볼 수 있다고 가정한다.\n",
    "- 우리가 살고 있는 세상은 모든 정보를 알기는 힘들다. 하지만 특정 시점에서는 모든 환경을 다 볼 수 있기 때문에 완전히 특성을 갖추고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미래는 과거와 독립적이다. $$ \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_(t+1)|S_1, ... , S_t] $$\n",
    "- 현재 상태는 과거의 어떤 과정에 의해서 발생된 것이기 때문에 현재 상태는 과거의 중요 정보를 포함하고 있다.\n",
    "- 따라서 의사 결정을 할때는 현재 정보에 집중하면 된다. 현재 정보가 Markov property를 가지고 있기 때문\n",
    "- 이것을 통해서 RL은 현재 시점에서 미래 reward를 예측하고 그것에 따라서 의사결정 할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s와 다음 상태를 s'이라 했을 때 s -> s'으로 상태가 바뀔 확률은 조건부 확률을 통해서 표현가능하다. $$ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1}=s'|S_t=s] $$\n",
    "- State transition matrix(현재 상태 s에서 다음 상태 s'으로 가는 모든 가능한 경우) \\\\(\\mathcal{P}\\\\)를 matrix형태로 정의할 수 있다. $$ \\mathcal{P} = \\begin{bmatrix} \\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \\vdots & \\vdots \\\\ \\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov process는 기본적으로 random process라고 생각한다. 과거에 어떤 action을 취했는지 기억하지 않고 현재 상태에서 랜덤으로 선택한다.\n",
    "- 모든 상태를 고려하여 앞으로 변경될 확률을 생각한 것이 Markov process, Markov chain이라 한다. $$ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic](https://user-images.githubusercontent.com/22078438/48101102-42811780-e269-11e8-8f07-481b4657b560.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 왼쪽 그림에서 이동 가능한 상태에 대해서 \\\\(\\mathcal{P}\\\\)로 나타내면 오른쪽 matrix와 같이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 언급했던 Markov chain에 reward를 추가한 개념이다.\n",
    "- value를 판단하기 위해서 reward와 discounted factor가 사용된다.\n",
    "- reward는 현재 상태를 기준으로 다음 상태에 받게 될 expectation reward이고, discounted factor는 현재 받는 reward의 가치와 미래에 받을 reward의 가치는 다르기 때문에 적용시켜준다.\n",
    "- reward function, \\\\( \\mathcal{R}_s=\\mathbb{E}[R_(t+1)\\;|\\;S_t=s]\\\\)\n",
    "- discounted factor, \\\\( \\gamma \\in[0,1]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reward 관점에서 본다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(G_t\\\\)는 time-step t로 부터 total discounted reward\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=1}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "- 현재 시점에서 앞으로 내가 받을 보상의 모든 합을 계산한다.\n",
    "- time-step이 무한대로 진행한다고 위의 식에서 되어있지만 time-step이 커질수록 0으로 수렴하기 때문에 결국은 유한하다.\n",
    "- \\\\(\\gamma\\\\)가 0에 가까우면 'myopic' 평가, 근시안적 평가\n",
    "- \\\\(\\gamma\\\\)가 1에 가까우면 'far-sighted' 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면, 왜 discounted factor를 사용하나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산이 편하다.\n",
    "- 무한대로 진행되더라도 markov reward 계산이 가능하다.\n",
    "- discounted factor를 적용하지 않는 경우도 있다. 현재 상태를 유지하는 어떤 것을 하는 경우에 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s에서 앞으로 발생할 것으로 기대되는 expectation(E) 모든 reward의 합을 value라 한다. $$ v(s) = \\mathbb{E}[G_t\\;|\\;S_t=s]$$\n",
    "- value function은 현재 상태에서 미래의 모든 expectation reward를 표현한다.\n",
    "- 강화학습에서 찾고자 하는 것은 value function을 최대한 optimal하게 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value function은 두 가지 파트로 나뉠 수 있다. 이때 사용되는 것이 bellman equation이다.<br>\n",
    "    immediate reward, \\\\(R_{t+1}\\\\)<br>\n",
    "    discounted value of successor state \\\\(\\gamma v(S_{t+1})\\\\)<br>\n",
    "$$ \\begin{split} v(s) &= \\mathbb{E}[G_t\\;|\\;S_t=s] \\\\ &= \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... \\;|\\; S_t=s] \\\\ &= \\mathbb{E}[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + ...) \\;|\\; S_t = s] \\\\ &= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\;|\\; S_t=s]   \\\\ &= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) \\;|\\; S_t=s] \\end{split}$$\n",
    "- 따라서 bellman equation을 통해서 현재 시점의 value는 현재의 reward와 다음 시점의 value로 표현할 수 있다.\n",
    "- 결과적으로, $$ v(s) = \\mathcal{R}_s + \\gamma \\sum_{s' \\in S} \\mathcal{P}_{ss'}v(s') $$ 현재 상태 s에서 받을 reward + discounted factor * 다음 상태 s'으로 갈 확률 * s'에서 value function의 합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic1](https://user-images.githubusercontent.com/22078438/48103349-14540580-e272-11e8-96a9-ab334107723d.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 확인 했던 수식을 위와 같이 계산 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman equation 표현 using matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서서 하나의 state 단위로 수식을 보았는데, 여러 state단위로 해서 matrix로 표현\n",
    "- 간단하게 Bellman equation을 나타내면 $$ v = \\mathcal{R} + \\gamma \\mathcal{P}v $$\n",
    "- 각 state마다 표현하면 \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathcal{R}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{R}_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\gamma\n",
    "\\begin{bmatrix}\n",
    "\\mathcal{P}_{11} & \\cdots &  \\mathcal{P}_{1n} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{P}_{n1} & \\cdots &  \\mathcal{P}_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 구현 equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bellman equation은 linear equation이다. $$ \\begin{split} v &= \\mathcal{R} + \\gamma \\mathcal{P}v \\\\ (I - \\gamma \\mathcal{P})v &= \\mathcal{R} \\\\\n",
    "v &= (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 방식으로 직접 푸는 것은 사이즈가 작은 MRP에서 가능하다.\n",
    "- 따라서, 우리는 Dynamic programming, Monte-Carlo evaluation, Temporal-Difference learning을 사용할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
