{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Markov Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전제조건으로 agent가 환경에서 발생되는 모든 정보를 볼 수 있다고 가정한다.\n",
    "- 우리가 살고 있는 세상은 모든 정보를 알기는 힘들다. 하지만 특정 시점에서는 모든 환경을 다 볼 수 있기 때문에 완전히 특성을 갖추고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미래는 과거와 독립적이다. $$ \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_(t+1)|S_1, ... , S_t] $$\n",
    "- 현재 상태는 과거의 어떤 과정에 의해서 발생된 것이기 때문에 현재 상태는 과거의 중요 정보를 포함하고 있다.\n",
    "- 따라서 의사 결정을 할때는 현재 정보에 집중하면 된다. 현재 정보가 Markov property를 가지고 있기 때문\n",
    "- 이것을 통해서 RL은 현재 시점에서 미래 reward를 예측하고 그것에 따라서 의사결정 할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s와 다음 상태를 s'이라 했을 때 s -> s'으로 상태가 바뀔 확률은 조건부 확률을 통해서 표현가능하다. $$ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1}=s'|S_t=s] $$\n",
    "- State transition matrix(현재 상태 s에서 다음 상태 s'으로 가는 모든 가능한 경우) \\\\(\\mathcal{P}\\\\)를 matrix형태로 정의할 수 있다. $$ \\mathcal{P} = \\begin{bmatrix} \\mathcal{P}_{11} & \\cdots & \\mathcal{P}_{1n} \\\\ \\vdots & \\vdots \\\\ \\mathcal{P}_{n1} & \\cdots & \\mathcal{P}_{nn} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov process는 기본적으로 random process라고 생각한다. 과거에 어떤 action을 취했는지 기억하지 않고 현재 상태에서 랜덤으로 선택한다.\n",
    "- 모든 상태를 고려하여 앞으로 변경될 확률을 생각한 것이 Markov process, Markov chain이라 한다. $$ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s'|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic](https://user-images.githubusercontent.com/22078438/48101102-42811780-e269-11e8-8f07-481b4657b560.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 왼쪽 그림에서 이동 가능한 상태에 대해서 \\\\(\\mathcal{P}\\\\)로 나타내면 오른쪽 matrix와 같이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 언급했던 Markov chain에 reward를 추가한 개념이다.\n",
    "- value를 판단하기 위해서 reward와 discounted factor가 사용된다.\n",
    "- reward는 현재 상태를 기준으로 다음 상태에 받게 될 expectation reward이고, discounted factor는 현재 받는 reward의 가치와 미래에 받을 reward의 가치는 다르기 때문에 적용시켜준다.\n",
    "- reward function, \\\\( \\mathcal{R}_s=\\mathbb{E}[R_(t+1)|S_t=s]\\\\)\n",
    "- discounted factor, \\\\( \\gamma \\in[0,1]\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reward 관점에서 본다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(G_t\\\\)는 time-step t로 부터 total discounted reward\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=1}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "- 현재 시점에서 앞으로 내가 받을 보상의 모든 합을 계산한다.\n",
    "- time-step이 무한대로 진행한다고 위의 식에서 되어있지만 time-step이 커질수록 0으로 수렴하기 때문에 결국은 유한하다.\n",
    "- \\\\(\\gamma\\\\)가 0에 가까우면 'myopic' 평가, 근시안적 평가\n",
    "- \\\\(\\gamma\\\\)가 1에 가까우면 'far-sighted' 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면, 왜 discounted factor를 사용하나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산이 편하다.\n",
    "- 무한대로 진행되더라도 markov reward 계산이 가능하다.\n",
    "- discounted factor를 적용하지 않는 경우도 있다. 현재 상태를 유지하는 어떤 것을 하는 경우에 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 상태 s에서 앞으로 발생할 것으로 기대되는 expectation(E) 모든 reward의 합을 value라 한다. $$ v(s) = \\mathbb{E}[G_t|S_t=s]$$\n",
    "- value function은 현재 상태에서 미래의 모든 expectation reward를 표현한다.\n",
    "- 강화학습에서 찾고자 하는 것은 value function을 최대한 optimal하게 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
