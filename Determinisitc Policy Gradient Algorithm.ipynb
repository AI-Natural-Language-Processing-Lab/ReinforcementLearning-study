{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinisitc Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expected gradient of the action value function 형태이다.\n",
    "- Policy variance가 0으로 수렴할 경우, DPG는 Stochastic Policy Gradient와 동일해진다.\n",
    "  - 따라서, Policy gradient에 기법을 DPG에 적용가능하다.\n",
    "- 효율적인 exploration을 위해서, model-free, off-policy actor critic 알고리즘이 제안되었다.\n",
    "  - action-value function approximator를 사용해서 발생하는 bias를 막기 위해서 compatibility condition을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DPG는 SPG보다 성능이 좋다.\n",
    "  - high dimensional action spaces에서 성능 향상이 크다.\n",
    "      - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서 평균을 취한다.\n",
    "      - 결과적으로, action spaces의 차원이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이루어진다.\n",
    "  - 기존 방식보다 computation이 많지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Actor-critic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- actor와 critic이 번갈아가면서 동작하며 stochastic policy를 최적화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\\\(Q^{\\pi}(s, a)\\\\)를 근사한 \\\\(Q^w(s, a)\\\\)를 이용해서 stochastic policy gradient를 ascent하는 방향으로 policy parameter \\\\(\\theta\\\\)를 update함으로써 stochastic policy를 개선한다.<br><br>\n",
    "$$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA나 Q-learning같은 Temporal-difference(TD) learning을 이용해서 action-value function의 parameter \\\\(w\\\\)를 update함으로써 \\\\(Q^w(s, a)\\\\)가 \\\\(Q^{\\pi}(s, a)\\\\) 유사하도록 한다.\n",
    "- 실제 값 \\\\(Q^{\\pi}(s, a)\\\\)을 사용하지 않고 근사값 \\\\(Q^{w}(s, a)\\\\)를 사용하게 되면 bias가 발생한다. 하지만 compatible condition에 부합하는 근사값을 사용하면 bias가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distinct behavior policy \\\\(\\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) )\\\\)로 부터 sampling된 경로를 이용한 actor-critic\n",
    "- off-policy actor-critic은 \\\\(\\beta\\\\)에 의해 sampling 된 trajectory를 이용해서 policy \\\\(\\pi\\\\)를 근사하는 것이기 때문에 importance sampling이 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generalized policy iteration\n",
    "  - policy evaluation과 policy improvement를 한 번씩 번갈아 가면서 실행하는 policy iteration\n",
    "  - value function은 optimal에 수렴함.\n",
    "- Policy evalutation, action-value function \\\\(Q^{\\pi}(s, a)\\\\)나 \\\\(Q^{\\mu}(s, a)\\\\)를 estimate 하는 것이다.\n",
    "- Policy improvement,\n",
    "  - estimated action-value function에 따라 정책을 update한다.\n",
    "  - action-value function에 대한 greedy max를 사용하게 되는데, continuous action spaces가 되면 계산량이 많이 늘어난다.\n",
    "  - 따라서, policy gradient를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Actor Critic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA critic, on policy actor critic\n",
    "  - 단점\n",
    "    - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기 때문에 sub-optimal에 빠지기 쉽다.\n",
    "  - 목적\n",
    "    - 환경에서 충분한 noise를 제공하여 exploration 시킬 수 있다면 좋은 결과를 얻을 수 있음.\n",
    "- update formula\n",
    "  - \\\\(Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t}))\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning, off-policy actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stochastic behavior policy \\\\(\\beta(a|s)\\\\)에 의해 생성된 경로로 부터 deterministic target policy \\\\(\\mu_{\\theta}(s)\\\\)를 학습하는 off-policy actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-learning\n",
    "  - \\\\(Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t}))\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compatible function approximation 및 gradient temporal-difference learning을 이용한 actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
